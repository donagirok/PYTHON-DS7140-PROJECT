2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:13:04,151:INFO:PyCaret ClassificationExperiment
2025-02-28 12:13:04,151:INFO:Logging name: clf-default-name
2025-02-28 12:13:04,151:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 12:13:04,151:INFO:version 3.3.2
2025-02-28 12:13:04,151:INFO:Initializing setup()
2025-02-28 12:13:04,151:INFO:self.USI: cfb5
2025-02-28 12:13:04,152:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fix_imbalance', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'is_multiclass', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:13:04,152:INFO:Checking environment
2025-02-28 12:13:04,152:INFO:python_version: 3.10.16
2025-02-28 12:13:04,152:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:13:04,152:INFO:machine: AMD64
2025-02-28 12:13:04,152:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:13:04,152:INFO:Memory: svmem(total=34200334336, available=17714921472, percent=48.2, used=16485412864, free=17714921472)
2025-02-28 12:13:04,152:INFO:Physical Core: 24
2025-02-28 12:13:04,152:INFO:Logical Core: 32
2025-02-28 12:13:04,152:INFO:Checking libraries
2025-02-28 12:13:04,152:INFO:System:
2025-02-28 12:13:04,152:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:13:04,152:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:13:04,152:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:13:04,152:INFO:PyCaret required dependencies:
2025-02-28 12:13:07,640:INFO:                 pip: 25.0
2025-02-28 12:13:07,640:INFO:          setuptools: 75.8.0
2025-02-28 12:13:07,640:INFO:             pycaret: 3.3.2
2025-02-28 12:13:07,640:INFO:             IPython: 8.30.0
2025-02-28 12:13:07,640:INFO:          ipywidgets: 8.1.5
2025-02-28 12:13:07,640:INFO:                tqdm: 4.67.1
2025-02-28 12:13:07,640:INFO:               numpy: 1.26.4
2025-02-28 12:13:07,640:INFO:              pandas: 2.1.4
2025-02-28 12:13:07,640:INFO:              jinja2: 3.1.5
2025-02-28 12:13:07,640:INFO:               scipy: 1.11.4
2025-02-28 12:13:07,640:INFO:              joblib: 1.3.2
2025-02-28 12:13:07,640:INFO:             sklearn: 1.4.2
2025-02-28 12:13:07,640:INFO:                pyod: 2.0.3
2025-02-28 12:13:07,640:INFO:            imblearn: 0.13.0
2025-02-28 12:13:07,640:INFO:   category_encoders: 2.7.0
2025-02-28 12:13:07,640:INFO:            lightgbm: 4.5.0
2025-02-28 12:13:07,640:INFO:               numba: 0.61.0
2025-02-28 12:13:07,640:INFO:            requests: 2.32.3
2025-02-28 12:13:07,640:INFO:          matplotlib: 3.7.5
2025-02-28 12:13:07,640:INFO:          scikitplot: 0.3.7
2025-02-28 12:13:07,640:INFO:         yellowbrick: 1.5
2025-02-28 12:13:07,640:INFO:              plotly: 5.24.1
2025-02-28 12:13:07,640:INFO:    plotly-resampler: Not installed
2025-02-28 12:13:07,640:INFO:             kaleido: 0.2.1
2025-02-28 12:13:07,640:INFO:           schemdraw: 0.15
2025-02-28 12:13:07,640:INFO:         statsmodels: 0.14.4
2025-02-28 12:13:07,640:INFO:              sktime: 0.26.0
2025-02-28 12:13:07,640:INFO:               tbats: 1.1.3
2025-02-28 12:13:07,640:INFO:            pmdarima: 2.0.4
2025-02-28 12:13:07,640:INFO:              psutil: 5.9.0
2025-02-28 12:13:07,640:INFO:          markupsafe: 2.1.5
2025-02-28 12:13:07,640:INFO:             pickle5: Not installed
2025-02-28 12:13:07,640:INFO:         cloudpickle: 3.1.1
2025-02-28 12:13:07,640:INFO:         deprecation: 2.1.0
2025-02-28 12:13:07,640:INFO:              xxhash: 3.5.0
2025-02-28 12:13:07,640:INFO:           wurlitzer: Not installed
2025-02-28 12:13:07,640:INFO:PyCaret optional dependencies:
2025-02-28 12:13:15,610:INFO:                shap: 0.44.1
2025-02-28 12:13:15,610:INFO:           interpret: 0.6.9
2025-02-28 12:13:15,610:INFO:                umap: 0.5.7
2025-02-28 12:13:15,610:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:13:15,610:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:13:15,610:INFO:             autoviz: Not installed
2025-02-28 12:13:15,610:INFO:           fairlearn: 0.7.0
2025-02-28 12:13:15,610:INFO:          deepchecks: Not installed
2025-02-28 12:13:15,610:INFO:             xgboost: 2.1.4
2025-02-28 12:13:15,610:INFO:            catboost: 1.2.7
2025-02-28 12:13:15,610:INFO:              kmodes: 0.12.2
2025-02-28 12:13:15,610:INFO:             mlxtend: 0.23.4
2025-02-28 12:13:15,610:INFO:       statsforecast: 1.5.0
2025-02-28 12:13:15,610:INFO:        tune_sklearn: Not installed
2025-02-28 12:13:15,610:INFO:                 ray: Not installed
2025-02-28 12:13:15,610:INFO:            hyperopt: 0.2.7
2025-02-28 12:13:15,610:INFO:              optuna: 4.2.0
2025-02-28 12:13:15,610:INFO:               skopt: 0.10.2
2025-02-28 12:13:15,610:INFO:              mlflow: 2.20.1
2025-02-28 12:13:15,610:INFO:              gradio: 5.15.0
2025-02-28 12:13:15,610:INFO:             fastapi: 0.115.8
2025-02-28 12:13:15,610:INFO:             uvicorn: 0.34.0
2025-02-28 12:13:15,610:INFO:              m2cgen: 0.10.0
2025-02-28 12:13:15,610:INFO:           evidently: 0.4.40
2025-02-28 12:13:15,610:INFO:               fugue: 0.8.7
2025-02-28 12:13:15,610:INFO:           streamlit: Not installed
2025-02-28 12:13:15,610:INFO:             prophet: Not installed
2025-02-28 12:13:15,610:INFO:None
2025-02-28 12:13:15,610:INFO:Set up data.
2025-02-28 12:13:15,621:INFO:Set up folding strategy.
2025-02-28 12:13:15,621:INFO:Set up train/test split.
2025-02-28 12:13:15,630:INFO:Set up index.
2025-02-28 12:13:15,630:INFO:Assigning column types.
2025-02-28 12:13:15,633:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:13:15,652:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:13:15,655:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:15,672:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:15,673:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,024:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,025:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,036:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,038:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,038:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:13:35,058:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,070:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,071:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,091:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,103:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,104:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,105:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 12:13:35,137:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,138:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,170:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,171:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,174:INFO:Preparing preprocessing pipeline...
2025-02-28 12:13:35,175:INFO:Set up simple imputation.
2025-02-28 12:13:35,177:INFO:Set up encoding of ordinal features.
2025-02-28 12:13:35,179:INFO:Set up encoding of categorical features.
2025-02-28 12:13:35,261:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:13:35,271:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score',
                                             'Starting_Salary',
                                             'Career_Sa...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Student_ID'],
                                    transformer=TargetEncoder(cols=['Student_ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:13:35,271:INFO:Creating final display dataframe.
2025-02-28 12:13:35,468:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 20)
4        Transformed data shape        (5000, 31)
5   Transformed train set shape        (3500, 31)
6    Transformed test set shape        (1500, 31)
7              Numeric features                14
8          Categorical features                 5
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              cfb5
2025-02-28 12:13:35,505:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,506:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,546:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,547:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,548:INFO:setup() successfully completed in 31.4s...............
2025-02-28 12:14:03,115:INFO:Initializing compare_models()
2025-02-28 12:14:03,116:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 12:14:03,116:INFO:Checking exceptions
2025-02-28 12:14:03,119:INFO:Preparing display monitor
2025-02-28 12:14:03,133:INFO:Initializing Logistic Regression
2025-02-28 12:14:03,133:INFO:Total runtime is 0.0 minutes
2025-02-28 12:14:03,135:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:03,135:INFO:Initializing create_model()
2025-02-28 12:14:03,135:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:03,135:INFO:Checking exceptions
2025-02-28 12:14:03,135:INFO:Importing libraries
2025-02-28 12:14:03,135:INFO:Copying training dataset
2025-02-28 12:14:03,139:INFO:Defining folds
2025-02-28 12:14:03,139:INFO:Declaring metric variables
2025-02-28 12:14:03,141:INFO:Importing untrained model
2025-02-28 12:14:03,143:INFO:Logistic Regression Imported successfully
2025-02-28 12:14:03,148:INFO:Starting cross validation
2025-02-28 12:14:03,150:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:06,874:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,890:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,902:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,908:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,910:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,919:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,925:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,928:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,928:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,935:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,937:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,939:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,941:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,947:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,949:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,951:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,954:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,961:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,970:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,977:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,995:INFO:Calculating mean and std
2025-02-28 12:14:06,996:INFO:Creating metrics dataframe
2025-02-28 12:14:06,998:INFO:Uploading results into container
2025-02-28 12:14:06,999:INFO:Uploading model into container now
2025-02-28 12:14:06,999:INFO:_master_model_container: 1
2025-02-28 12:14:06,999:INFO:_display_container: 2
2025-02-28 12:14:06,999:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 12:14:06,999:INFO:create_model() successfully completed......................................
2025-02-28 12:14:07,116:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:07,116:INFO:Creating metrics dataframe
2025-02-28 12:14:07,119:INFO:Initializing K Neighbors Classifier
2025-02-28 12:14:07,119:INFO:Total runtime is 0.06642432610193888 minutes
2025-02-28 12:14:07,121:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:07,121:INFO:Initializing create_model()
2025-02-28 12:14:07,121:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:07,122:INFO:Checking exceptions
2025-02-28 12:14:07,122:INFO:Importing libraries
2025-02-28 12:14:07,122:INFO:Copying training dataset
2025-02-28 12:14:07,125:INFO:Defining folds
2025-02-28 12:14:07,125:INFO:Declaring metric variables
2025-02-28 12:14:07,128:INFO:Importing untrained model
2025-02-28 12:14:07,130:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:14:07,134:INFO:Starting cross validation
2025-02-28 12:14:07,135:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:10,005:INFO:Calculating mean and std
2025-02-28 12:14:10,006:INFO:Creating metrics dataframe
2025-02-28 12:14:10,007:INFO:Uploading results into container
2025-02-28 12:14:10,008:INFO:Uploading model into container now
2025-02-28 12:14:10,008:INFO:_master_model_container: 2
2025-02-28 12:14:10,008:INFO:_display_container: 2
2025-02-28 12:14:10,008:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:14:10,008:INFO:create_model() successfully completed......................................
2025-02-28 12:14:10,122:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:10,122:INFO:Creating metrics dataframe
2025-02-28 12:14:10,126:INFO:Initializing Naive Bayes
2025-02-28 12:14:10,126:INFO:Total runtime is 0.11655208269755045 minutes
2025-02-28 12:14:10,128:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:10,128:INFO:Initializing create_model()
2025-02-28 12:14:10,128:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:10,128:INFO:Checking exceptions
2025-02-28 12:14:10,129:INFO:Importing libraries
2025-02-28 12:14:10,129:INFO:Copying training dataset
2025-02-28 12:14:10,132:INFO:Defining folds
2025-02-28 12:14:10,132:INFO:Declaring metric variables
2025-02-28 12:14:10,135:INFO:Importing untrained model
2025-02-28 12:14:10,137:INFO:Naive Bayes Imported successfully
2025-02-28 12:14:10,141:INFO:Starting cross validation
2025-02-28 12:14:10,142:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:13,006:INFO:Calculating mean and std
2025-02-28 12:14:13,007:INFO:Creating metrics dataframe
2025-02-28 12:14:13,008:INFO:Uploading results into container
2025-02-28 12:14:13,009:INFO:Uploading model into container now
2025-02-28 12:14:13,009:INFO:_master_model_container: 3
2025-02-28 12:14:13,009:INFO:_display_container: 2
2025-02-28 12:14:13,009:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 12:14:13,010:INFO:create_model() successfully completed......................................
2025-02-28 12:14:13,123:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:13,123:INFO:Creating metrics dataframe
2025-02-28 12:14:13,126:INFO:Initializing Decision Tree Classifier
2025-02-28 12:14:13,127:INFO:Total runtime is 0.16656994024912516 minutes
2025-02-28 12:14:13,129:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:13,129:INFO:Initializing create_model()
2025-02-28 12:14:13,129:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:13,129:INFO:Checking exceptions
2025-02-28 12:14:13,129:INFO:Importing libraries
2025-02-28 12:14:13,129:INFO:Copying training dataset
2025-02-28 12:14:13,134:INFO:Defining folds
2025-02-28 12:14:13,134:INFO:Declaring metric variables
2025-02-28 12:14:13,137:INFO:Importing untrained model
2025-02-28 12:14:13,139:INFO:Decision Tree Classifier Imported successfully
2025-02-28 12:14:13,143:INFO:Starting cross validation
2025-02-28 12:14:13,145:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:13,239:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,241:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,246:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,251:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,251:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,254:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,255:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,255:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,162:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,164:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,178:INFO:Calculating mean and std
2025-02-28 12:14:15,179:INFO:Creating metrics dataframe
2025-02-28 12:14:15,180:INFO:Uploading results into container
2025-02-28 12:14:15,180:INFO:Uploading model into container now
2025-02-28 12:14:15,180:INFO:_master_model_container: 4
2025-02-28 12:14:15,181:INFO:_display_container: 2
2025-02-28 12:14:15,181:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:14:15,181:INFO:create_model() successfully completed......................................
2025-02-28 12:14:15,290:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:15,290:INFO:Creating metrics dataframe
2025-02-28 12:14:15,296:INFO:Initializing SVM - Linear Kernel
2025-02-28 12:14:15,296:INFO:Total runtime is 0.20270905494689942 minutes
2025-02-28 12:14:15,298:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:15,298:INFO:Initializing create_model()
2025-02-28 12:14:15,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:15,298:INFO:Checking exceptions
2025-02-28 12:14:15,298:INFO:Importing libraries
2025-02-28 12:14:15,298:INFO:Copying training dataset
2025-02-28 12:14:15,301:INFO:Defining folds
2025-02-28 12:14:15,301:INFO:Declaring metric variables
2025-02-28 12:14:15,304:INFO:Importing untrained model
2025-02-28 12:14:15,306:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 12:14:15,311:INFO:Starting cross validation
2025-02-28 12:14:15,312:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:15,467:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,472:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,475:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,476:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,477:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,478:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,478:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,480:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,480:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,481:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,489:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,489:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,491:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,491:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,501:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,501:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,502:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,504:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,504:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,505:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,508:INFO:Calculating mean and std
2025-02-28 12:14:15,508:INFO:Creating metrics dataframe
2025-02-28 12:14:15,510:INFO:Uploading results into container
2025-02-28 12:14:15,510:INFO:Uploading model into container now
2025-02-28 12:14:15,511:INFO:_master_model_container: 5
2025-02-28 12:14:15,511:INFO:_display_container: 2
2025-02-28 12:14:15,511:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:14:15,511:INFO:create_model() successfully completed......................................
2025-02-28 12:14:15,619:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:15,620:INFO:Creating metrics dataframe
2025-02-28 12:14:15,624:INFO:Initializing Ridge Classifier
2025-02-28 12:14:15,624:INFO:Total runtime is 0.20817364851633707 minutes
2025-02-28 12:14:15,627:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:15,627:INFO:Initializing create_model()
2025-02-28 12:14:15,627:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:15,627:INFO:Checking exceptions
2025-02-28 12:14:15,627:INFO:Importing libraries
2025-02-28 12:14:15,627:INFO:Copying training dataset
2025-02-28 12:14:15,630:INFO:Defining folds
2025-02-28 12:14:15,630:INFO:Declaring metric variables
2025-02-28 12:14:15,632:INFO:Importing untrained model
2025-02-28 12:14:15,635:INFO:Ridge Classifier Imported successfully
2025-02-28 12:14:15,639:INFO:Starting cross validation
2025-02-28 12:14:15,640:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:15,744:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,745:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,747:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,748:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,748:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,749:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,750:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,751:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,754:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,757:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,777:INFO:Calculating mean and std
2025-02-28 12:14:15,778:INFO:Creating metrics dataframe
2025-02-28 12:14:15,779:INFO:Uploading results into container
2025-02-28 12:14:15,779:INFO:Uploading model into container now
2025-02-28 12:14:15,780:INFO:_master_model_container: 6
2025-02-28 12:14:15,780:INFO:_display_container: 2
2025-02-28 12:14:15,780:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 12:14:15,780:INFO:create_model() successfully completed......................................
2025-02-28 12:14:15,889:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:15,890:INFO:Creating metrics dataframe
2025-02-28 12:14:15,894:INFO:Initializing Random Forest Classifier
2025-02-28 12:14:15,894:INFO:Total runtime is 0.2126800020535787 minutes
2025-02-28 12:14:15,896:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:15,896:INFO:Initializing create_model()
2025-02-28 12:14:15,896:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:15,897:INFO:Checking exceptions
2025-02-28 12:14:15,897:INFO:Importing libraries
2025-02-28 12:14:15,897:INFO:Copying training dataset
2025-02-28 12:14:15,900:INFO:Defining folds
2025-02-28 12:14:15,900:INFO:Declaring metric variables
2025-02-28 12:14:15,903:INFO:Importing untrained model
2025-02-28 12:14:15,906:INFO:Random Forest Classifier Imported successfully
2025-02-28 12:14:15,910:INFO:Starting cross validation
2025-02-28 12:14:15,911:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:16,256:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,257:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,284:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,300:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,361:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,370:INFO:Calculating mean and std
2025-02-28 12:14:16,371:INFO:Creating metrics dataframe
2025-02-28 12:14:16,372:INFO:Uploading results into container
2025-02-28 12:14:16,373:INFO:Uploading model into container now
2025-02-28 12:14:16,373:INFO:_master_model_container: 7
2025-02-28 12:14:16,373:INFO:_display_container: 2
2025-02-28 12:14:16,373:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 12:14:16,373:INFO:create_model() successfully completed......................................
2025-02-28 12:14:16,489:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:16,489:INFO:Creating metrics dataframe
2025-02-28 12:14:16,494:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 12:14:16,494:INFO:Total runtime is 0.2226726253827413 minutes
2025-02-28 12:14:16,496:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:16,496:INFO:Initializing create_model()
2025-02-28 12:14:16,496:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:16,496:INFO:Checking exceptions
2025-02-28 12:14:16,496:INFO:Importing libraries
2025-02-28 12:14:16,496:INFO:Copying training dataset
2025-02-28 12:14:16,500:INFO:Defining folds
2025-02-28 12:14:16,500:INFO:Declaring metric variables
2025-02-28 12:14:16,502:INFO:Importing untrained model
2025-02-28 12:14:16,504:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 12:14:16,508:INFO:Starting cross validation
2025-02-28 12:14:16,509:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,596:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,600:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,613:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,613:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,618:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,619:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,620:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,621:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,622:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,623:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,626:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,628:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,631:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,647:INFO:Calculating mean and std
2025-02-28 12:14:16,647:INFO:Creating metrics dataframe
2025-02-28 12:14:16,648:INFO:Uploading results into container
2025-02-28 12:14:16,648:INFO:Uploading model into container now
2025-02-28 12:14:16,649:INFO:_master_model_container: 8
2025-02-28 12:14:16,649:INFO:_display_container: 2
2025-02-28 12:14:16,649:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 12:14:16,649:INFO:create_model() successfully completed......................................
2025-02-28 12:14:16,764:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:16,764:INFO:Creating metrics dataframe
2025-02-28 12:14:16,769:INFO:Initializing Ada Boost Classifier
2025-02-28 12:14:16,770:INFO:Total runtime is 0.22727389335632325 minutes
2025-02-28 12:14:16,771:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:16,771:INFO:Initializing create_model()
2025-02-28 12:14:16,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:16,772:INFO:Checking exceptions
2025-02-28 12:14:16,772:INFO:Importing libraries
2025-02-28 12:14:16,772:INFO:Copying training dataset
2025-02-28 12:14:16,775:INFO:Defining folds
2025-02-28 12:14:16,776:INFO:Declaring metric variables
2025-02-28 12:14:16,778:INFO:Importing untrained model
2025-02-28 12:14:16,780:INFO:Ada Boost Classifier Imported successfully
2025-02-28 12:14:16,784:INFO:Starting cross validation
2025-02-28 12:14:16,785:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:16,854:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,856:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,857:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,863:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,863:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,864:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,865:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,876:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:17,033:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,036:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,037:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,040:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,041:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,041:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,042:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,043:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,044:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,046:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,048:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,051:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,062:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,064:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,066:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,066:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,068:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,068:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,083:INFO:Calculating mean and std
2025-02-28 12:14:17,084:INFO:Creating metrics dataframe
2025-02-28 12:14:17,085:INFO:Uploading results into container
2025-02-28 12:14:17,085:INFO:Uploading model into container now
2025-02-28 12:14:17,085:INFO:_master_model_container: 9
2025-02-28 12:14:17,086:INFO:_display_container: 2
2025-02-28 12:14:17,086:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 12:14:17,086:INFO:create_model() successfully completed......................................
2025-02-28 12:14:17,201:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:17,201:INFO:Creating metrics dataframe
2025-02-28 12:14:17,206:INFO:Initializing Gradient Boosting Classifier
2025-02-28 12:14:17,206:INFO:Total runtime is 0.23455353180567426 minutes
2025-02-28 12:14:17,208:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:17,208:INFO:Initializing create_model()
2025-02-28 12:14:17,208:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:17,208:INFO:Checking exceptions
2025-02-28 12:14:17,208:INFO:Importing libraries
2025-02-28 12:14:17,208:INFO:Copying training dataset
2025-02-28 12:14:17,212:INFO:Defining folds
2025-02-28 12:14:17,212:INFO:Declaring metric variables
2025-02-28 12:14:17,215:INFO:Importing untrained model
2025-02-28 12:14:17,217:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 12:14:17,222:INFO:Starting cross validation
2025-02-28 12:14:17,223:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:19,280:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,284:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,317:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,320:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,321:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,323:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,323:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,325:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,325:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,328:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,333:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,335:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,357:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,359:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,365:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,367:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,382:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,384:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,390:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,393:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,406:INFO:Calculating mean and std
2025-02-28 12:14:19,407:INFO:Creating metrics dataframe
2025-02-28 12:14:19,408:INFO:Uploading results into container
2025-02-28 12:14:19,408:INFO:Uploading model into container now
2025-02-28 12:14:19,408:INFO:_master_model_container: 10
2025-02-28 12:14:19,408:INFO:_display_container: 2
2025-02-28 12:14:19,409:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:14:19,409:INFO:create_model() successfully completed......................................
2025-02-28 12:14:19,519:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:19,519:INFO:Creating metrics dataframe
2025-02-28 12:14:19,523:INFO:Initializing Linear Discriminant Analysis
2025-02-28 12:14:19,524:INFO:Total runtime is 0.27318072716395064 minutes
2025-02-28 12:14:19,526:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:19,526:INFO:Initializing create_model()
2025-02-28 12:14:19,526:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:19,526:INFO:Checking exceptions
2025-02-28 12:14:19,526:INFO:Importing libraries
2025-02-28 12:14:19,526:INFO:Copying training dataset
2025-02-28 12:14:19,530:INFO:Defining folds
2025-02-28 12:14:19,530:INFO:Declaring metric variables
2025-02-28 12:14:19,533:INFO:Importing untrained model
2025-02-28 12:14:19,535:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 12:14:19,540:INFO:Starting cross validation
2025-02-28 12:14:19,541:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:19,636:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,637:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,638:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,638:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,639:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,639:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,640:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,641:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,646:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,646:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,648:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,649:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,653:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,654:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,655:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,656:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,658:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,659:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,661:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,662:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,674:INFO:Calculating mean and std
2025-02-28 12:14:19,675:INFO:Creating metrics dataframe
2025-02-28 12:14:19,676:INFO:Uploading results into container
2025-02-28 12:14:19,676:INFO:Uploading model into container now
2025-02-28 12:14:19,676:INFO:_master_model_container: 11
2025-02-28 12:14:19,676:INFO:_display_container: 2
2025-02-28 12:14:19,677:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 12:14:19,677:INFO:create_model() successfully completed......................................
2025-02-28 12:14:19,784:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:19,784:INFO:Creating metrics dataframe
2025-02-28 12:14:19,789:INFO:Initializing Extra Trees Classifier
2025-02-28 12:14:19,789:INFO:Total runtime is 0.27759943803151454 minutes
2025-02-28 12:14:19,791:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:19,792:INFO:Initializing create_model()
2025-02-28 12:14:19,792:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:19,792:INFO:Checking exceptions
2025-02-28 12:14:19,792:INFO:Importing libraries
2025-02-28 12:14:19,792:INFO:Copying training dataset
2025-02-28 12:14:19,795:INFO:Defining folds
2025-02-28 12:14:19,795:INFO:Declaring metric variables
2025-02-28 12:14:19,797:INFO:Importing untrained model
2025-02-28 12:14:19,799:INFO:Extra Trees Classifier Imported successfully
2025-02-28 12:14:19,804:INFO:Starting cross validation
2025-02-28 12:14:19,806:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:20,119:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,119:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,121:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,134:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,134:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,135:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,149:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,158:INFO:Calculating mean and std
2025-02-28 12:14:20,159:INFO:Creating metrics dataframe
2025-02-28 12:14:20,160:INFO:Uploading results into container
2025-02-28 12:14:20,160:INFO:Uploading model into container now
2025-02-28 12:14:20,160:INFO:_master_model_container: 12
2025-02-28 12:14:20,160:INFO:_display_container: 2
2025-02-28 12:14:20,161:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 12:14:20,161:INFO:create_model() successfully completed......................................
2025-02-28 12:14:20,270:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:20,270:INFO:Creating metrics dataframe
2025-02-28 12:14:20,276:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:14:20,276:INFO:Total runtime is 0.28570960362752285 minutes
2025-02-28 12:14:20,278:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:20,279:INFO:Initializing create_model()
2025-02-28 12:14:20,279:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:20,279:INFO:Checking exceptions
2025-02-28 12:14:20,279:INFO:Importing libraries
2025-02-28 12:14:20,279:INFO:Copying training dataset
2025-02-28 12:14:20,281:INFO:Defining folds
2025-02-28 12:14:20,281:INFO:Declaring metric variables
2025-02-28 12:14:20,284:INFO:Importing untrained model
2025-02-28 12:14:20,287:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:14:20,291:INFO:Starting cross validation
2025-02-28 12:14:20,292:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:20,910:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,925:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,926:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,932:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,938:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,943:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,965:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,977:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,981:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,986:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,998:INFO:Calculating mean and std
2025-02-28 12:14:20,999:INFO:Creating metrics dataframe
2025-02-28 12:14:21,000:INFO:Uploading results into container
2025-02-28 12:14:21,000:INFO:Uploading model into container now
2025-02-28 12:14:21,001:INFO:_master_model_container: 13
2025-02-28 12:14:21,001:INFO:_display_container: 2
2025-02-28 12:14:21,001:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 12:14:21,001:INFO:create_model() successfully completed......................................
2025-02-28 12:14:21,110:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:21,110:INFO:Creating metrics dataframe
2025-02-28 12:14:21,115:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:14:21,116:INFO:Total runtime is 0.2997180938720704 minutes
2025-02-28 12:14:21,118:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:21,118:INFO:Initializing create_model()
2025-02-28 12:14:21,118:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:21,118:INFO:Checking exceptions
2025-02-28 12:14:21,118:INFO:Importing libraries
2025-02-28 12:14:21,118:INFO:Copying training dataset
2025-02-28 12:14:21,122:INFO:Defining folds
2025-02-28 12:14:21,122:INFO:Declaring metric variables
2025-02-28 12:14:21,125:INFO:Importing untrained model
2025-02-28 12:14:21,127:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:14:21,131:INFO:Starting cross validation
2025-02-28 12:14:21,132:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:24,555:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,579:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,626:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,664:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,665:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,713:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,759:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,783:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,988:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:25,007:INFO:Calculating mean and std
2025-02-28 12:14:25,008:INFO:Creating metrics dataframe
2025-02-28 12:14:25,009:INFO:Uploading results into container
2025-02-28 12:14:25,009:INFO:Uploading model into container now
2025-02-28 12:14:25,010:INFO:_master_model_container: 14
2025-02-28 12:14:25,010:INFO:_display_container: 2
2025-02-28 12:14:25,010:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:14:25,010:INFO:create_model() successfully completed......................................
2025-02-28 12:14:25,131:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:25,131:INFO:Creating metrics dataframe
2025-02-28 12:14:25,137:INFO:Initializing CatBoost Classifier
2025-02-28 12:14:25,137:INFO:Total runtime is 0.36673409541447965 minutes
2025-02-28 12:14:25,139:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:25,139:INFO:Initializing create_model()
2025-02-28 12:14:25,139:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:25,139:INFO:Checking exceptions
2025-02-28 12:14:25,139:INFO:Importing libraries
2025-02-28 12:14:25,139:INFO:Copying training dataset
2025-02-28 12:14:25,143:INFO:Defining folds
2025-02-28 12:14:25,143:INFO:Declaring metric variables
2025-02-28 12:14:25,146:INFO:Importing untrained model
2025-02-28 12:14:25,148:INFO:CatBoost Classifier Imported successfully
2025-02-28 12:14:25,153:INFO:Starting cross validation
2025-02-28 12:14:25,154:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:32,425:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,430:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,432:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,442:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,448:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,454:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,454:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,456:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,457:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,461:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
1 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\catboost\core.py", line 5245, in fit
    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\catboost\core.py", line 2410, in _fit
    self._train(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\catboost\core.py", line 1790, in _train
    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)
  File "_catboost.pyx", line 5017, in _catboost._CatBoost._train
  File "_catboost.pyx", line 5066, in _catboost._CatBoost._train
_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:26: Can't create train tmp dir: tmp

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-02-28 12:14:32,461:INFO:Calculating mean and std
2025-02-28 12:14:32,462:INFO:Creating metrics dataframe
2025-02-28 12:14:32,462:INFO:Uploading results into container
2025-02-28 12:14:32,464:INFO:Uploading model into container now
2025-02-28 12:14:32,464:INFO:_master_model_container: 15
2025-02-28 12:14:32,464:INFO:_display_container: 2
2025-02-28 12:14:32,464:INFO:<catboost.core.CatBoostClassifier object at 0x0000024E59816C80>
2025-02-28 12:14:32,464:INFO:create_model() successfully completed......................................
2025-02-28 12:14:32,569:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:32,569:INFO:Creating metrics dataframe
2025-02-28 12:14:32,576:INFO:Initializing Dummy Classifier
2025-02-28 12:14:32,576:INFO:Total runtime is 0.49071048498153697 minutes
2025-02-28 12:14:32,578:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:32,579:INFO:Initializing create_model()
2025-02-28 12:14:32,579:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:32,579:INFO:Checking exceptions
2025-02-28 12:14:32,579:INFO:Importing libraries
2025-02-28 12:14:32,579:INFO:Copying training dataset
2025-02-28 12:14:32,581:INFO:Defining folds
2025-02-28 12:14:32,582:INFO:Declaring metric variables
2025-02-28 12:14:32,584:INFO:Importing untrained model
2025-02-28 12:14:32,587:INFO:Dummy Classifier Imported successfully
2025-02-28 12:14:32,591:INFO:Starting cross validation
2025-02-28 12:14:32,592:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:32,677:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,685:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,688:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,689:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,696:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,699:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,707:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,710:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,710:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,714:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,726:INFO:Calculating mean and std
2025-02-28 12:14:32,727:INFO:Creating metrics dataframe
2025-02-28 12:14:32,728:INFO:Uploading results into container
2025-02-28 12:14:32,728:INFO:Uploading model into container now
2025-02-28 12:14:32,728:INFO:_master_model_container: 16
2025-02-28 12:14:32,729:INFO:_display_container: 2
2025-02-28 12:14:32,729:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 12:14:32,729:INFO:create_model() successfully completed......................................
2025-02-28 12:14:32,835:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:32,835:INFO:Creating metrics dataframe
2025-02-28 12:14:32,842:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:14:32,847:INFO:Initializing create_model()
2025-02-28 12:14:32,847:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:32,847:INFO:Checking exceptions
2025-02-28 12:14:32,848:INFO:Importing libraries
2025-02-28 12:14:32,848:INFO:Copying training dataset
2025-02-28 12:14:32,854:INFO:Defining folds
2025-02-28 12:14:32,854:INFO:Declaring metric variables
2025-02-28 12:14:32,854:INFO:Importing untrained model
2025-02-28 12:14:32,854:INFO:Declaring custom model
2025-02-28 12:14:32,854:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:14:32,856:INFO:Cross validation set to False
2025-02-28 12:14:32,856:INFO:Fitting Model
2025-02-28 12:14:32,905:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:14:32,905:INFO:create_model() successfully completed......................................
2025-02-28 12:14:33,026:INFO:_master_model_container: 16
2025-02-28 12:14:33,026:INFO:_display_container: 2
2025-02-28 12:14:33,026:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:14:33,026:INFO:compare_models() successfully completed......................................
2025-02-28 12:17:20,901:INFO:Initializing plot_model()
2025-02-28 12:17:20,901:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, system=True)
2025-02-28 12:17:20,901:INFO:Checking exceptions
2025-02-28 12:17:20,905:INFO:Preloading libraries
2025-02-28 12:17:20,906:INFO:Copying training dataset
2025-02-28 12:17:20,906:INFO:Plot type: confusion_matrix
2025-02-28 12:17:21,114:INFO:Fitting Model
2025-02-28 12:17:21,115:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:17:21,116:INFO:Scoring test/hold-out set
2025-02-28 12:17:21,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\externals\loky\backend\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:
[WinError 2] The system cannot find the file specified
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(

2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\externals\loky\backend\context.py", line 257, in _count_physical_cores
2025-02-28 12:17:21,121:WARNING:    cpu_info = subprocess.run(
2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\subprocess.py", line 503, in run
2025-02-28 12:17:21,121:WARNING:    with Popen(*popenargs, **kwargs) as process:
2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\subprocess.py", line 971, in __init__
2025-02-28 12:17:21,121:WARNING:    self._execute_child(args, executable, preexec_fn, close_fds,
2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\subprocess.py", line 1456, in _execute_child
2025-02-28 12:17:21,121:WARNING:    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
2025-02-28 12:17:21,491:INFO:Visual Rendered Successfully
2025-02-28 12:17:21,611:INFO:plot_model() successfully completed......................................
2025-02-28 12:17:34,242:INFO:Initializing plot_model()
2025-02-28 12:17:34,242:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, system=True)
2025-02-28 12:17:34,243:INFO:Checking exceptions
2025-02-28 12:17:34,246:INFO:Preloading libraries
2025-02-28 12:17:34,246:INFO:Copying training dataset
2025-02-28 12:17:34,246:INFO:Plot type: auc
2025-02-28 12:17:34,444:INFO:Fitting Model
2025-02-28 12:17:34,444:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:17:34,444:INFO:Scoring test/hold-out set
2025-02-28 12:17:34,773:INFO:Visual Rendered Successfully
2025-02-28 12:17:34,881:INFO:plot_model() successfully completed......................................
2025-02-28 12:17:43,719:INFO:Initializing plot_model()
2025-02-28 12:17:43,720:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, system=True)
2025-02-28 12:17:43,720:INFO:Checking exceptions
2025-02-28 12:19:23,147:INFO:PyCaret RegressionExperiment
2025-02-28 12:19:23,147:INFO:Logging name: reg-default-name
2025-02-28 12:19:23,147:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 12:19:23,147:INFO:version 3.3.2
2025-02-28 12:19:23,147:INFO:Initializing setup()
2025-02-28 12:19:23,147:INFO:self.USI: eab2
2025-02-28 12:19:23,148:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'transform_target_param', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:19:23,148:INFO:Checking environment
2025-02-28 12:19:23,148:INFO:python_version: 3.10.16
2025-02-28 12:19:23,148:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:19:23,148:INFO:machine: AMD64
2025-02-28 12:19:23,148:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:19:23,148:INFO:Memory: svmem(total=34200334336, available=12409036800, percent=63.7, used=21791297536, free=12409036800)
2025-02-28 12:19:23,148:INFO:Physical Core: 24
2025-02-28 12:19:23,148:INFO:Logical Core: 32
2025-02-28 12:19:23,148:INFO:Checking libraries
2025-02-28 12:19:23,148:INFO:System:
2025-02-28 12:19:23,148:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:19:23,148:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:19:23,148:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:19:23,148:INFO:PyCaret required dependencies:
2025-02-28 12:19:23,148:INFO:                 pip: 25.0
2025-02-28 12:19:23,148:INFO:          setuptools: 75.8.0
2025-02-28 12:19:23,148:INFO:             pycaret: 3.3.2
2025-02-28 12:19:23,148:INFO:             IPython: 8.30.0
2025-02-28 12:19:23,148:INFO:          ipywidgets: 8.1.5
2025-02-28 12:19:23,148:INFO:                tqdm: 4.67.1
2025-02-28 12:19:23,149:INFO:               numpy: 1.26.4
2025-02-28 12:19:23,149:INFO:              pandas: 2.1.4
2025-02-28 12:19:23,149:INFO:              jinja2: 3.1.5
2025-02-28 12:19:23,149:INFO:               scipy: 1.11.4
2025-02-28 12:19:23,149:INFO:              joblib: 1.3.2
2025-02-28 12:19:23,149:INFO:             sklearn: 1.4.2
2025-02-28 12:19:23,149:INFO:                pyod: 2.0.3
2025-02-28 12:19:23,149:INFO:            imblearn: 0.13.0
2025-02-28 12:19:23,149:INFO:   category_encoders: 2.7.0
2025-02-28 12:19:23,149:INFO:            lightgbm: 4.5.0
2025-02-28 12:19:23,149:INFO:               numba: 0.61.0
2025-02-28 12:19:23,149:INFO:            requests: 2.32.3
2025-02-28 12:19:23,149:INFO:          matplotlib: 3.7.5
2025-02-28 12:19:23,149:INFO:          scikitplot: 0.3.7
2025-02-28 12:19:23,149:INFO:         yellowbrick: 1.5
2025-02-28 12:19:23,149:INFO:              plotly: 5.24.1
2025-02-28 12:19:23,149:INFO:    plotly-resampler: Not installed
2025-02-28 12:19:23,149:INFO:             kaleido: 0.2.1
2025-02-28 12:19:23,149:INFO:           schemdraw: 0.15
2025-02-28 12:19:23,149:INFO:         statsmodels: 0.14.4
2025-02-28 12:19:23,149:INFO:              sktime: 0.26.0
2025-02-28 12:19:23,149:INFO:               tbats: 1.1.3
2025-02-28 12:19:23,149:INFO:            pmdarima: 2.0.4
2025-02-28 12:19:23,149:INFO:              psutil: 5.9.0
2025-02-28 12:19:23,149:INFO:          markupsafe: 2.1.5
2025-02-28 12:19:23,149:INFO:             pickle5: Not installed
2025-02-28 12:19:23,149:INFO:         cloudpickle: 3.1.1
2025-02-28 12:19:23,149:INFO:         deprecation: 2.1.0
2025-02-28 12:19:23,149:INFO:              xxhash: 3.5.0
2025-02-28 12:19:23,149:INFO:           wurlitzer: Not installed
2025-02-28 12:19:23,149:INFO:PyCaret optional dependencies:
2025-02-28 12:19:23,149:INFO:                shap: 0.44.1
2025-02-28 12:19:23,149:INFO:           interpret: 0.6.9
2025-02-28 12:19:23,149:INFO:                umap: 0.5.7
2025-02-28 12:19:23,149:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:19:23,149:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:19:23,149:INFO:             autoviz: Not installed
2025-02-28 12:19:23,149:INFO:           fairlearn: 0.7.0
2025-02-28 12:19:23,149:INFO:          deepchecks: Not installed
2025-02-28 12:19:23,149:INFO:             xgboost: 2.1.4
2025-02-28 12:19:23,150:INFO:            catboost: 1.2.7
2025-02-28 12:19:23,150:INFO:              kmodes: 0.12.2
2025-02-28 12:19:23,150:INFO:             mlxtend: 0.23.4
2025-02-28 12:19:23,150:INFO:       statsforecast: 1.5.0
2025-02-28 12:19:23,150:INFO:        tune_sklearn: Not installed
2025-02-28 12:19:23,150:INFO:                 ray: Not installed
2025-02-28 12:19:23,150:INFO:            hyperopt: 0.2.7
2025-02-28 12:19:23,150:INFO:              optuna: 4.2.0
2025-02-28 12:19:23,150:INFO:               skopt: 0.10.2
2025-02-28 12:19:23,150:INFO:              mlflow: 2.20.1
2025-02-28 12:19:23,150:INFO:              gradio: 5.15.0
2025-02-28 12:19:23,150:INFO:             fastapi: 0.115.8
2025-02-28 12:19:23,150:INFO:             uvicorn: 0.34.0
2025-02-28 12:19:23,150:INFO:              m2cgen: 0.10.0
2025-02-28 12:19:23,150:INFO:           evidently: 0.4.40
2025-02-28 12:19:23,150:INFO:               fugue: 0.8.7
2025-02-28 12:19:23,150:INFO:           streamlit: Not installed
2025-02-28 12:19:23,150:INFO:             prophet: Not installed
2025-02-28 12:19:23,150:INFO:None
2025-02-28 12:19:23,150:INFO:Set up data.
2025-02-28 12:19:23,156:INFO:Set up folding strategy.
2025-02-28 12:19:23,157:INFO:Set up train/test split.
2025-02-28 12:19:23,160:INFO:Set up index.
2025-02-28 12:19:23,161:INFO:Assigning column types.
2025-02-28 12:19:23,163:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:19:23,163:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,165:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,167:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,193:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,213:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,213:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,214:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,215:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,217:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,219:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,245:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,264:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,265:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,266:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,266:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 12:19:23,268:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,270:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,296:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,315:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,315:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,317:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,319:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,321:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,346:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,365:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,366:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,367:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,367:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 12:19:23,372:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,396:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,415:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,415:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,416:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,420:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,445:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,465:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,465:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,466:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,467:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 12:19:23,497:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,516:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,517:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,518:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,548:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,567:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,567:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,568:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,569:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:19:23,599:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,618:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,619:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,649:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,668:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,670:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,670:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 12:19:23,719:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,720:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,768:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,769:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,772:INFO:Preparing preprocessing pipeline...
2025-02-28 12:19:23,772:INFO:Set up simple imputation.
2025-02-28 12:19:23,774:INFO:Set up encoding of ordinal features.
2025-02-28 12:19:23,775:INFO:Set up encoding of categorical features.
2025-02-28 12:19:23,846:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:19:23,854:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score', 'Job_Offers',
                                             'Career_Satisfa...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Student_ID'],
                                    transformer=TargetEncoder(cols=['Student_ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:19:23,854:INFO:Creating final display dataframe.
2025-02-28 12:19:24,054:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 20)
4        Transformed data shape        (5000, 31)
5   Transformed train set shape        (3500, 31)
6    Transformed test set shape        (1500, 31)
7              Numeric features                14
8          Categorical features                 5
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              eab2
2025-02-28 12:19:24,109:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:24,110:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:24,160:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:24,161:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:24,161:INFO:setup() successfully completed in 1.02s...............
2025-02-28 12:19:24,182:INFO:Initializing compare_models()
2025-02-28 12:19:24,182:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 12:19:24,182:INFO:Checking exceptions
2025-02-28 12:19:24,184:INFO:Preparing display monitor
2025-02-28 12:19:24,202:INFO:Initializing Linear Regression
2025-02-28 12:19:24,202:INFO:Total runtime is 0.0 minutes
2025-02-28 12:19:24,205:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:24,206:INFO:Initializing create_model()
2025-02-28 12:19:24,206:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:24,206:INFO:Checking exceptions
2025-02-28 12:19:24,206:INFO:Importing libraries
2025-02-28 12:19:24,206:INFO:Copying training dataset
2025-02-28 12:19:24,209:INFO:Defining folds
2025-02-28 12:19:24,209:INFO:Declaring metric variables
2025-02-28 12:19:24,212:INFO:Importing untrained model
2025-02-28 12:19:24,216:INFO:Linear Regression Imported successfully
2025-02-28 12:19:24,222:INFO:Starting cross validation
2025-02-28 12:19:24,223:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:24,376:INFO:Calculating mean and std
2025-02-28 12:19:24,376:INFO:Creating metrics dataframe
2025-02-28 12:19:24,377:INFO:Uploading results into container
2025-02-28 12:19:24,377:INFO:Uploading model into container now
2025-02-28 12:19:24,378:INFO:_master_model_container: 1
2025-02-28 12:19:24,378:INFO:_display_container: 2
2025-02-28 12:19:24,378:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 12:19:24,378:INFO:create_model() successfully completed......................................
2025-02-28 12:19:24,507:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:24,507:INFO:Creating metrics dataframe
2025-02-28 12:19:24,511:INFO:Initializing Lasso Regression
2025-02-28 12:19:24,511:INFO:Total runtime is 0.005150417486826579 minutes
2025-02-28 12:19:24,513:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:24,513:INFO:Initializing create_model()
2025-02-28 12:19:24,513:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:24,514:INFO:Checking exceptions
2025-02-28 12:19:24,514:INFO:Importing libraries
2025-02-28 12:19:24,514:INFO:Copying training dataset
2025-02-28 12:19:24,517:INFO:Defining folds
2025-02-28 12:19:24,517:INFO:Declaring metric variables
2025-02-28 12:19:24,519:INFO:Importing untrained model
2025-02-28 12:19:24,521:INFO:Lasso Regression Imported successfully
2025-02-28 12:19:24,526:INFO:Starting cross validation
2025-02-28 12:19:24,527:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:24,654:INFO:Calculating mean and std
2025-02-28 12:19:24,654:INFO:Creating metrics dataframe
2025-02-28 12:19:24,655:INFO:Uploading results into container
2025-02-28 12:19:24,655:INFO:Uploading model into container now
2025-02-28 12:19:24,656:INFO:_master_model_container: 2
2025-02-28 12:19:24,656:INFO:_display_container: 2
2025-02-28 12:19:24,656:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 12:19:24,656:INFO:create_model() successfully completed......................................
2025-02-28 12:19:24,784:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:24,784:INFO:Creating metrics dataframe
2025-02-28 12:19:24,788:INFO:Initializing Ridge Regression
2025-02-28 12:19:24,788:INFO:Total runtime is 0.009762648741404215 minutes
2025-02-28 12:19:24,790:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:24,790:INFO:Initializing create_model()
2025-02-28 12:19:24,790:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:24,790:INFO:Checking exceptions
2025-02-28 12:19:24,790:INFO:Importing libraries
2025-02-28 12:19:24,790:INFO:Copying training dataset
2025-02-28 12:19:24,793:INFO:Defining folds
2025-02-28 12:19:24,793:INFO:Declaring metric variables
2025-02-28 12:19:24,795:INFO:Importing untrained model
2025-02-28 12:19:24,797:INFO:Ridge Regression Imported successfully
2025-02-28 12:19:24,801:INFO:Starting cross validation
2025-02-28 12:19:24,803:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:24,937:INFO:Calculating mean and std
2025-02-28 12:19:24,938:INFO:Creating metrics dataframe
2025-02-28 12:19:24,939:INFO:Uploading results into container
2025-02-28 12:19:24,939:INFO:Uploading model into container now
2025-02-28 12:19:24,939:INFO:_master_model_container: 3
2025-02-28 12:19:24,939:INFO:_display_container: 2
2025-02-28 12:19:24,939:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 12:19:24,939:INFO:create_model() successfully completed......................................
2025-02-28 12:19:25,066:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:25,066:INFO:Creating metrics dataframe
2025-02-28 12:19:25,069:INFO:Initializing Elastic Net
2025-02-28 12:19:25,071:INFO:Total runtime is 0.014487226804097492 minutes
2025-02-28 12:19:25,073:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:25,073:INFO:Initializing create_model()
2025-02-28 12:19:25,073:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:25,073:INFO:Checking exceptions
2025-02-28 12:19:25,073:INFO:Importing libraries
2025-02-28 12:19:25,073:INFO:Copying training dataset
2025-02-28 12:19:25,076:INFO:Defining folds
2025-02-28 12:19:25,076:INFO:Declaring metric variables
2025-02-28 12:19:25,078:INFO:Importing untrained model
2025-02-28 12:19:25,081:INFO:Elastic Net Imported successfully
2025-02-28 12:19:25,086:INFO:Starting cross validation
2025-02-28 12:19:25,087:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:25,218:INFO:Calculating mean and std
2025-02-28 12:19:25,219:INFO:Creating metrics dataframe
2025-02-28 12:19:25,220:INFO:Uploading results into container
2025-02-28 12:19:25,220:INFO:Uploading model into container now
2025-02-28 12:19:25,220:INFO:_master_model_container: 4
2025-02-28 12:19:25,220:INFO:_display_container: 2
2025-02-28 12:19:25,221:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 12:19:25,221:INFO:create_model() successfully completed......................................
2025-02-28 12:19:25,345:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:25,345:INFO:Creating metrics dataframe
2025-02-28 12:19:25,349:INFO:Initializing Least Angle Regression
2025-02-28 12:19:25,349:INFO:Total runtime is 0.019125668207804362 minutes
2025-02-28 12:19:25,352:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:25,352:INFO:Initializing create_model()
2025-02-28 12:19:25,352:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:25,352:INFO:Checking exceptions
2025-02-28 12:19:25,352:INFO:Importing libraries
2025-02-28 12:19:25,352:INFO:Copying training dataset
2025-02-28 12:19:25,355:INFO:Defining folds
2025-02-28 12:19:25,355:INFO:Declaring metric variables
2025-02-28 12:19:25,358:INFO:Importing untrained model
2025-02-28 12:19:25,360:INFO:Least Angle Regression Imported successfully
2025-02-28 12:19:25,364:INFO:Starting cross validation
2025-02-28 12:19:25,365:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:27,424:INFO:Calculating mean and std
2025-02-28 12:19:27,425:INFO:Creating metrics dataframe
2025-02-28 12:19:27,426:INFO:Uploading results into container
2025-02-28 12:19:27,426:INFO:Uploading model into container now
2025-02-28 12:19:27,427:INFO:_master_model_container: 5
2025-02-28 12:19:27,427:INFO:_display_container: 2
2025-02-28 12:19:27,427:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 12:19:27,427:INFO:create_model() successfully completed......................................
2025-02-28 12:19:27,559:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:27,559:INFO:Creating metrics dataframe
2025-02-28 12:19:27,562:INFO:Initializing Lasso Least Angle Regression
2025-02-28 12:19:27,562:INFO:Total runtime is 0.0560053547223409 minutes
2025-02-28 12:19:27,565:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:27,565:INFO:Initializing create_model()
2025-02-28 12:19:27,565:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:27,566:INFO:Checking exceptions
2025-02-28 12:19:27,566:INFO:Importing libraries
2025-02-28 12:19:27,566:INFO:Copying training dataset
2025-02-28 12:19:27,569:INFO:Defining folds
2025-02-28 12:19:27,569:INFO:Declaring metric variables
2025-02-28 12:19:27,572:INFO:Importing untrained model
2025-02-28 12:19:27,575:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 12:19:27,579:INFO:Starting cross validation
2025-02-28 12:19:27,581:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:27,721:INFO:Calculating mean and std
2025-02-28 12:19:27,721:INFO:Creating metrics dataframe
2025-02-28 12:19:27,723:INFO:Uploading results into container
2025-02-28 12:19:27,723:INFO:Uploading model into container now
2025-02-28 12:19:27,723:INFO:_master_model_container: 6
2025-02-28 12:19:27,723:INFO:_display_container: 2
2025-02-28 12:19:27,724:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 12:19:27,724:INFO:create_model() successfully completed......................................
2025-02-28 12:19:27,850:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:27,850:INFO:Creating metrics dataframe
2025-02-28 12:19:27,855:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 12:19:27,855:INFO:Total runtime is 0.06088508764902751 minutes
2025-02-28 12:19:27,857:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:27,857:INFO:Initializing create_model()
2025-02-28 12:19:27,858:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:27,858:INFO:Checking exceptions
2025-02-28 12:19:27,858:INFO:Importing libraries
2025-02-28 12:19:27,858:INFO:Copying training dataset
2025-02-28 12:19:27,861:INFO:Defining folds
2025-02-28 12:19:27,861:INFO:Declaring metric variables
2025-02-28 12:19:27,863:INFO:Importing untrained model
2025-02-28 12:19:27,865:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 12:19:27,868:INFO:Starting cross validation
2025-02-28 12:19:27,870:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:27,940:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,943:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,950:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,958:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,960:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,961:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,962:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,984:INFO:Calculating mean and std
2025-02-28 12:19:27,984:INFO:Creating metrics dataframe
2025-02-28 12:19:27,985:INFO:Uploading results into container
2025-02-28 12:19:27,986:INFO:Uploading model into container now
2025-02-28 12:19:27,986:INFO:_master_model_container: 7
2025-02-28 12:19:27,986:INFO:_display_container: 2
2025-02-28 12:19:27,986:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 12:19:27,987:INFO:create_model() successfully completed......................................
2025-02-28 12:19:28,118:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:28,118:INFO:Creating metrics dataframe
2025-02-28 12:19:28,123:INFO:Initializing Bayesian Ridge
2025-02-28 12:19:28,123:INFO:Total runtime is 0.06535278161366781 minutes
2025-02-28 12:19:28,125:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:28,125:INFO:Initializing create_model()
2025-02-28 12:19:28,126:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:28,126:INFO:Checking exceptions
2025-02-28 12:19:28,126:INFO:Importing libraries
2025-02-28 12:19:28,126:INFO:Copying training dataset
2025-02-28 12:19:28,129:INFO:Defining folds
2025-02-28 12:19:28,129:INFO:Declaring metric variables
2025-02-28 12:19:28,131:INFO:Importing untrained model
2025-02-28 12:19:28,134:INFO:Bayesian Ridge Imported successfully
2025-02-28 12:19:28,138:INFO:Starting cross validation
2025-02-28 12:19:28,139:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:28,279:INFO:Calculating mean and std
2025-02-28 12:19:28,280:INFO:Creating metrics dataframe
2025-02-28 12:19:28,281:INFO:Uploading results into container
2025-02-28 12:19:28,281:INFO:Uploading model into container now
2025-02-28 12:19:28,281:INFO:_master_model_container: 8
2025-02-28 12:19:28,281:INFO:_display_container: 2
2025-02-28 12:19:28,281:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 12:19:28,281:INFO:create_model() successfully completed......................................
2025-02-28 12:19:28,406:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:28,406:INFO:Creating metrics dataframe
2025-02-28 12:19:28,410:INFO:Initializing Passive Aggressive Regressor
2025-02-28 12:19:28,410:INFO:Total runtime is 0.0701412041982015 minutes
2025-02-28 12:19:28,412:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:28,412:INFO:Initializing create_model()
2025-02-28 12:19:28,412:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:28,412:INFO:Checking exceptions
2025-02-28 12:19:28,412:INFO:Importing libraries
2025-02-28 12:19:28,412:INFO:Copying training dataset
2025-02-28 12:19:28,415:INFO:Defining folds
2025-02-28 12:19:28,416:INFO:Declaring metric variables
2025-02-28 12:19:28,418:INFO:Importing untrained model
2025-02-28 12:19:28,421:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 12:19:28,425:INFO:Starting cross validation
2025-02-28 12:19:28,426:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:28,561:INFO:Calculating mean and std
2025-02-28 12:19:28,561:INFO:Creating metrics dataframe
2025-02-28 12:19:28,562:INFO:Uploading results into container
2025-02-28 12:19:28,562:INFO:Uploading model into container now
2025-02-28 12:19:28,563:INFO:_master_model_container: 9
2025-02-28 12:19:28,563:INFO:_display_container: 2
2025-02-28 12:19:28,563:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:19:28,563:INFO:create_model() successfully completed......................................
2025-02-28 12:19:28,692:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:28,692:INFO:Creating metrics dataframe
2025-02-28 12:19:28,696:INFO:Initializing Huber Regressor
2025-02-28 12:19:28,696:INFO:Total runtime is 0.07489873568216959 minutes
2025-02-28 12:19:28,698:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:28,699:INFO:Initializing create_model()
2025-02-28 12:19:28,699:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:28,699:INFO:Checking exceptions
2025-02-28 12:19:28,699:INFO:Importing libraries
2025-02-28 12:19:28,699:INFO:Copying training dataset
2025-02-28 12:19:28,702:INFO:Defining folds
2025-02-28 12:19:28,702:INFO:Declaring metric variables
2025-02-28 12:19:28,704:INFO:Importing untrained model
2025-02-28 12:19:28,707:INFO:Huber Regressor Imported successfully
2025-02-28 12:19:28,713:INFO:Starting cross validation
2025-02-28 12:19:28,715:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:28,822:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:19:28,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:19:28,840:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:19:28,874:INFO:Calculating mean and std
2025-02-28 12:19:28,874:INFO:Creating metrics dataframe
2025-02-28 12:19:28,876:INFO:Uploading results into container
2025-02-28 12:19:28,876:INFO:Uploading model into container now
2025-02-28 12:19:28,876:INFO:_master_model_container: 10
2025-02-28 12:19:28,876:INFO:_display_container: 2
2025-02-28 12:19:28,876:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 12:19:28,876:INFO:create_model() successfully completed......................................
2025-02-28 12:19:29,001:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:29,001:INFO:Creating metrics dataframe
2025-02-28 12:19:29,005:INFO:Initializing K Neighbors Regressor
2025-02-28 12:19:29,006:INFO:Total runtime is 0.08007542292277017 minutes
2025-02-28 12:19:29,008:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:29,008:INFO:Initializing create_model()
2025-02-28 12:19:29,008:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:29,008:INFO:Checking exceptions
2025-02-28 12:19:29,008:INFO:Importing libraries
2025-02-28 12:19:29,008:INFO:Copying training dataset
2025-02-28 12:19:29,011:INFO:Defining folds
2025-02-28 12:19:29,012:INFO:Declaring metric variables
2025-02-28 12:19:29,014:INFO:Importing untrained model
2025-02-28 12:19:29,017:INFO:K Neighbors Regressor Imported successfully
2025-02-28 12:19:29,021:INFO:Starting cross validation
2025-02-28 12:19:29,022:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:29,156:INFO:Calculating mean and std
2025-02-28 12:19:29,157:INFO:Creating metrics dataframe
2025-02-28 12:19:29,158:INFO:Uploading results into container
2025-02-28 12:19:29,158:INFO:Uploading model into container now
2025-02-28 12:19:29,158:INFO:_master_model_container: 11
2025-02-28 12:19:29,158:INFO:_display_container: 2
2025-02-28 12:19:29,159:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 12:19:29,159:INFO:create_model() successfully completed......................................
2025-02-28 12:19:29,289:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:29,289:INFO:Creating metrics dataframe
2025-02-28 12:19:29,293:INFO:Initializing Decision Tree Regressor
2025-02-28 12:19:29,293:INFO:Total runtime is 0.08485944271087646 minutes
2025-02-28 12:19:29,295:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:29,296:INFO:Initializing create_model()
2025-02-28 12:19:29,296:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:29,296:INFO:Checking exceptions
2025-02-28 12:19:29,297:INFO:Importing libraries
2025-02-28 12:19:29,297:INFO:Copying training dataset
2025-02-28 12:19:29,300:INFO:Defining folds
2025-02-28 12:19:29,300:INFO:Declaring metric variables
2025-02-28 12:19:29,302:INFO:Importing untrained model
2025-02-28 12:19:29,304:INFO:Decision Tree Regressor Imported successfully
2025-02-28 12:19:29,309:INFO:Starting cross validation
2025-02-28 12:19:29,309:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:29,454:INFO:Calculating mean and std
2025-02-28 12:19:29,454:INFO:Creating metrics dataframe
2025-02-28 12:19:29,456:INFO:Uploading results into container
2025-02-28 12:19:29,456:INFO:Uploading model into container now
2025-02-28 12:19:29,456:INFO:_master_model_container: 12
2025-02-28 12:19:29,456:INFO:_display_container: 2
2025-02-28 12:19:29,456:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:19:29,456:INFO:create_model() successfully completed......................................
2025-02-28 12:19:29,584:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:29,585:INFO:Creating metrics dataframe
2025-02-28 12:19:29,589:INFO:Initializing Random Forest Regressor
2025-02-28 12:19:29,589:INFO:Total runtime is 0.08978945811589559 minutes
2025-02-28 12:19:29,591:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:29,592:INFO:Initializing create_model()
2025-02-28 12:19:29,592:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:29,592:INFO:Checking exceptions
2025-02-28 12:19:29,592:INFO:Importing libraries
2025-02-28 12:19:29,592:INFO:Copying training dataset
2025-02-28 12:19:29,595:INFO:Defining folds
2025-02-28 12:19:29,595:INFO:Declaring metric variables
2025-02-28 12:19:29,598:INFO:Importing untrained model
2025-02-28 12:19:29,601:INFO:Random Forest Regressor Imported successfully
2025-02-28 12:19:29,605:INFO:Starting cross validation
2025-02-28 12:19:29,607:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:30,343:INFO:Calculating mean and std
2025-02-28 12:19:30,344:INFO:Creating metrics dataframe
2025-02-28 12:19:30,345:INFO:Uploading results into container
2025-02-28 12:19:30,345:INFO:Uploading model into container now
2025-02-28 12:19:30,346:INFO:_master_model_container: 13
2025-02-28 12:19:30,346:INFO:_display_container: 2
2025-02-28 12:19:30,346:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 12:19:30,346:INFO:create_model() successfully completed......................................
2025-02-28 12:19:30,469:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:30,469:INFO:Creating metrics dataframe
2025-02-28 12:19:30,474:INFO:Initializing Extra Trees Regressor
2025-02-28 12:19:30,474:INFO:Total runtime is 0.1045389453570048 minutes
2025-02-28 12:19:30,476:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:30,476:INFO:Initializing create_model()
2025-02-28 12:19:30,477:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:30,477:INFO:Checking exceptions
2025-02-28 12:19:30,477:INFO:Importing libraries
2025-02-28 12:19:30,477:INFO:Copying training dataset
2025-02-28 12:19:30,480:INFO:Defining folds
2025-02-28 12:19:30,480:INFO:Declaring metric variables
2025-02-28 12:19:30,482:INFO:Importing untrained model
2025-02-28 12:19:30,484:INFO:Extra Trees Regressor Imported successfully
2025-02-28 12:19:30,489:INFO:Starting cross validation
2025-02-28 12:19:30,490:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:31,007:INFO:Calculating mean and std
2025-02-28 12:19:31,007:INFO:Creating metrics dataframe
2025-02-28 12:19:31,009:INFO:Uploading results into container
2025-02-28 12:19:31,009:INFO:Uploading model into container now
2025-02-28 12:19:31,009:INFO:_master_model_container: 14
2025-02-28 12:19:31,009:INFO:_display_container: 2
2025-02-28 12:19:31,010:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 12:19:31,010:INFO:create_model() successfully completed......................................
2025-02-28 12:19:31,141:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:31,141:INFO:Creating metrics dataframe
2025-02-28 12:19:31,147:INFO:Initializing AdaBoost Regressor
2025-02-28 12:19:31,147:INFO:Total runtime is 0.11575092077255249 minutes
2025-02-28 12:19:31,149:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:31,149:INFO:Initializing create_model()
2025-02-28 12:19:31,149:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:31,149:INFO:Checking exceptions
2025-02-28 12:19:31,149:INFO:Importing libraries
2025-02-28 12:19:31,149:INFO:Copying training dataset
2025-02-28 12:19:31,152:INFO:Defining folds
2025-02-28 12:19:31,153:INFO:Declaring metric variables
2025-02-28 12:19:31,155:INFO:Importing untrained model
2025-02-28 12:19:31,157:INFO:AdaBoost Regressor Imported successfully
2025-02-28 12:19:31,162:INFO:Starting cross validation
2025-02-28 12:19:31,164:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:31,555:INFO:Calculating mean and std
2025-02-28 12:19:31,556:INFO:Creating metrics dataframe
2025-02-28 12:19:31,557:INFO:Uploading results into container
2025-02-28 12:19:31,557:INFO:Uploading model into container now
2025-02-28 12:19:31,557:INFO:_master_model_container: 15
2025-02-28 12:19:31,558:INFO:_display_container: 2
2025-02-28 12:19:31,558:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 12:19:31,558:INFO:create_model() successfully completed......................................
2025-02-28 12:19:31,687:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:31,687:INFO:Creating metrics dataframe
2025-02-28 12:19:31,693:INFO:Initializing Gradient Boosting Regressor
2025-02-28 12:19:31,693:INFO:Total runtime is 0.12485628922780355 minutes
2025-02-28 12:19:31,695:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:31,695:INFO:Initializing create_model()
2025-02-28 12:19:31,695:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:31,695:INFO:Checking exceptions
2025-02-28 12:19:31,695:INFO:Importing libraries
2025-02-28 12:19:31,695:INFO:Copying training dataset
2025-02-28 12:19:31,699:INFO:Defining folds
2025-02-28 12:19:31,699:INFO:Declaring metric variables
2025-02-28 12:19:31,702:INFO:Importing untrained model
2025-02-28 12:19:31,705:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 12:19:31,712:INFO:Starting cross validation
2025-02-28 12:19:31,713:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:32,319:INFO:Calculating mean and std
2025-02-28 12:19:32,319:INFO:Creating metrics dataframe
2025-02-28 12:19:32,321:INFO:Uploading results into container
2025-02-28 12:19:32,321:INFO:Uploading model into container now
2025-02-28 12:19:32,321:INFO:_master_model_container: 16
2025-02-28 12:19:32,321:INFO:_display_container: 2
2025-02-28 12:19:32,321:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:19:32,321:INFO:create_model() successfully completed......................................
2025-02-28 12:19:32,447:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:32,447:INFO:Creating metrics dataframe
2025-02-28 12:19:32,453:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:19:32,453:INFO:Total runtime is 0.13752080202102662 minutes
2025-02-28 12:19:32,455:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:32,455:INFO:Initializing create_model()
2025-02-28 12:19:32,455:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:32,455:INFO:Checking exceptions
2025-02-28 12:19:32,455:INFO:Importing libraries
2025-02-28 12:19:32,455:INFO:Copying training dataset
2025-02-28 12:19:32,459:INFO:Defining folds
2025-02-28 12:19:32,459:INFO:Declaring metric variables
2025-02-28 12:19:32,461:INFO:Importing untrained model
2025-02-28 12:19:32,464:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:19:32,469:INFO:Starting cross validation
2025-02-28 12:19:32,470:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:33,052:INFO:Calculating mean and std
2025-02-28 12:19:33,052:INFO:Creating metrics dataframe
2025-02-28 12:19:33,054:INFO:Uploading results into container
2025-02-28 12:19:33,054:INFO:Uploading model into container now
2025-02-28 12:19:33,054:INFO:_master_model_container: 17
2025-02-28 12:19:33,054:INFO:_display_container: 2
2025-02-28 12:19:33,055:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:19:33,055:INFO:create_model() successfully completed......................................
2025-02-28 12:19:33,185:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:33,185:INFO:Creating metrics dataframe
2025-02-28 12:19:33,191:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:19:33,191:INFO:Total runtime is 0.14982364575068158 minutes
2025-02-28 12:19:33,193:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:33,193:INFO:Initializing create_model()
2025-02-28 12:19:33,193:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:33,194:INFO:Checking exceptions
2025-02-28 12:19:33,194:INFO:Importing libraries
2025-02-28 12:19:33,194:INFO:Copying training dataset
2025-02-28 12:19:33,197:INFO:Defining folds
2025-02-28 12:19:33,197:INFO:Declaring metric variables
2025-02-28 12:19:33,199:INFO:Importing untrained model
2025-02-28 12:19:33,202:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:19:33,207:INFO:Starting cross validation
2025-02-28 12:19:33,208:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:34,647:INFO:Calculating mean and std
2025-02-28 12:19:34,648:INFO:Creating metrics dataframe
2025-02-28 12:19:34,649:INFO:Uploading results into container
2025-02-28 12:19:34,650:INFO:Uploading model into container now
2025-02-28 12:19:34,650:INFO:_master_model_container: 18
2025-02-28 12:19:34,650:INFO:_display_container: 2
2025-02-28 12:19:34,651:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:19:34,651:INFO:create_model() successfully completed......................................
2025-02-28 12:19:34,792:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:34,792:INFO:Creating metrics dataframe
2025-02-28 12:19:34,798:INFO:Initializing CatBoost Regressor
2025-02-28 12:19:34,798:INFO:Total runtime is 0.1765997131665548 minutes
2025-02-28 12:19:34,800:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:34,800:INFO:Initializing create_model()
2025-02-28 12:19:34,800:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:34,801:INFO:Checking exceptions
2025-02-28 12:19:34,801:INFO:Importing libraries
2025-02-28 12:19:34,801:INFO:Copying training dataset
2025-02-28 12:19:34,803:INFO:Defining folds
2025-02-28 12:19:34,803:INFO:Declaring metric variables
2025-02-28 12:19:34,806:INFO:Importing untrained model
2025-02-28 12:19:34,808:INFO:CatBoost Regressor Imported successfully
2025-02-28 12:19:34,811:INFO:Starting cross validation
2025-02-28 12:19:34,813:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:37,832:INFO:Calculating mean and std
2025-02-28 12:19:37,833:INFO:Creating metrics dataframe
2025-02-28 12:19:37,834:INFO:Uploading results into container
2025-02-28 12:19:37,834:INFO:Uploading model into container now
2025-02-28 12:19:37,834:INFO:_master_model_container: 19
2025-02-28 12:19:37,834:INFO:_display_container: 2
2025-02-28 12:19:37,835:INFO:<catboost.core.CatBoostRegressor object at 0x0000024E5BBD15A0>
2025-02-28 12:19:37,835:INFO:create_model() successfully completed......................................
2025-02-28 12:19:37,958:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:37,958:INFO:Creating metrics dataframe
2025-02-28 12:19:37,963:INFO:Initializing Dummy Regressor
2025-02-28 12:19:37,963:INFO:Total runtime is 0.2293560028076172 minutes
2025-02-28 12:19:37,966:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:37,966:INFO:Initializing create_model()
2025-02-28 12:19:37,966:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:37,966:INFO:Checking exceptions
2025-02-28 12:19:37,966:INFO:Importing libraries
2025-02-28 12:19:37,966:INFO:Copying training dataset
2025-02-28 12:19:37,970:INFO:Defining folds
2025-02-28 12:19:37,970:INFO:Declaring metric variables
2025-02-28 12:19:37,973:INFO:Importing untrained model
2025-02-28 12:19:37,975:INFO:Dummy Regressor Imported successfully
2025-02-28 12:19:37,979:INFO:Starting cross validation
2025-02-28 12:19:37,980:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:38,114:INFO:Calculating mean and std
2025-02-28 12:19:38,115:INFO:Creating metrics dataframe
2025-02-28 12:19:38,116:INFO:Uploading results into container
2025-02-28 12:19:38,116:INFO:Uploading model into container now
2025-02-28 12:19:38,116:INFO:_master_model_container: 20
2025-02-28 12:19:38,116:INFO:_display_container: 2
2025-02-28 12:19:38,116:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:19:38,117:INFO:create_model() successfully completed......................................
2025-02-28 12:19:38,243:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:38,243:INFO:Creating metrics dataframe
2025-02-28 12:19:38,249:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:19:38,256:INFO:Initializing create_model()
2025-02-28 12:19:38,256:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:38,256:INFO:Checking exceptions
2025-02-28 12:19:38,257:INFO:Importing libraries
2025-02-28 12:19:38,257:INFO:Copying training dataset
2025-02-28 12:19:38,260:INFO:Defining folds
2025-02-28 12:19:38,260:INFO:Declaring metric variables
2025-02-28 12:19:38,260:INFO:Importing untrained model
2025-02-28 12:19:38,260:INFO:Declaring custom model
2025-02-28 12:19:38,260:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:19:38,261:INFO:Cross validation set to False
2025-02-28 12:19:38,261:INFO:Fitting Model
2025-02-28 12:19:38,500:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:19:38,501:INFO:create_model() successfully completed......................................
2025-02-28 12:19:38,656:INFO:_master_model_container: 20
2025-02-28 12:19:38,656:INFO:_display_container: 2
2025-02-28 12:19:38,656:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:19:38,656:INFO:compare_models() successfully completed......................................
2025-02-28 12:20:33,517:INFO:Initializing create_model()
2025-02-28 12:20:33,517:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:20:33,517:INFO:Checking exceptions
2025-02-28 12:20:33,525:INFO:Importing libraries
2025-02-28 12:20:33,525:INFO:Copying training dataset
2025-02-28 12:20:33,528:INFO:Defining folds
2025-02-28 12:20:33,528:INFO:Declaring metric variables
2025-02-28 12:20:33,530:INFO:Importing untrained model
2025-02-28 12:20:33,533:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:20:33,539:INFO:Starting cross validation
2025-02-28 12:20:33,540:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:20:34,172:INFO:Calculating mean and std
2025-02-28 12:20:34,172:INFO:Creating metrics dataframe
2025-02-28 12:20:34,175:INFO:Finalizing model
2025-02-28 12:20:34,286:INFO:Uploading results into container
2025-02-28 12:20:34,287:INFO:Uploading model into container now
2025-02-28 12:20:34,293:INFO:_master_model_container: 21
2025-02-28 12:20:34,293:INFO:_display_container: 3
2025-02-28 12:20:34,293:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:20:34,294:INFO:create_model() successfully completed......................................
2025-02-28 12:20:53,870:INFO:Initializing plot_model()
2025-02-28 12:20:53,870:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, system=True)
2025-02-28 12:20:53,870:INFO:Checking exceptions
2025-02-28 12:20:53,873:INFO:Preloading libraries
2025-02-28 12:20:53,878:INFO:Copying training dataset
2025-02-28 12:20:53,878:INFO:Plot type: residuals
2025-02-28 12:20:54,129:INFO:Fitting Model
2025-02-28 12:20:54,160:INFO:Scoring test/hold-out set
2025-02-28 12:20:54,434:INFO:Visual Rendered Successfully
2025-02-28 12:20:54,575:INFO:plot_model() successfully completed......................................
2025-02-28 12:21:02,113:INFO:Initializing plot_model()
2025-02-28 12:21:02,113:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, system=True)
2025-02-28 12:21:02,113:INFO:Checking exceptions
2025-02-28 12:21:02,116:INFO:Preloading libraries
2025-02-28 12:21:02,121:INFO:Copying training dataset
2025-02-28 12:21:02,121:INFO:Plot type: error
2025-02-28 12:21:02,342:INFO:Fitting Model
2025-02-28 12:21:02,342:INFO:Scoring test/hold-out set
2025-02-28 12:21:02,486:INFO:Visual Rendered Successfully
2025-02-28 12:21:02,617:INFO:plot_model() successfully completed......................................
2025-02-28 12:21:10,033:INFO:Initializing plot_model()
2025-02-28 12:21:10,033:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, system=True)
2025-02-28 12:21:10,033:INFO:Checking exceptions
2025-02-28 12:21:10,036:INFO:Preloading libraries
2025-02-28 12:21:10,041:INFO:Copying training dataset
2025-02-28 12:21:10,041:INFO:Plot type: feature
2025-02-28 12:21:10,041:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 12:21:10,189:INFO:Visual Rendered Successfully
2025-02-28 12:21:10,322:INFO:plot_model() successfully completed......................................
2025-02-28 12:23:59,991:INFO:PyCaret ClassificationExperiment
2025-02-28 12:23:59,991:INFO:Logging name: clf-default-name
2025-02-28 12:23:59,991:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 12:23:59,991:INFO:version 3.3.2
2025-02-28 12:23:59,991:INFO:Initializing setup()
2025-02-28 12:23:59,991:INFO:self.USI: c87e
2025-02-28 12:23:59,991:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fix_imbalance', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'is_multiclass', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:23:59,991:INFO:Checking environment
2025-02-28 12:23:59,991:INFO:python_version: 3.10.16
2025-02-28 12:23:59,991:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:23:59,991:INFO:machine: AMD64
2025-02-28 12:23:59,991:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:23:59,991:INFO:Memory: svmem(total=34200334336, available=8954920960, percent=73.8, used=25245413376, free=8954920960)
2025-02-28 12:23:59,991:INFO:Physical Core: 24
2025-02-28 12:23:59,992:INFO:Logical Core: 32
2025-02-28 12:23:59,992:INFO:Checking libraries
2025-02-28 12:23:59,992:INFO:System:
2025-02-28 12:23:59,992:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:23:59,992:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:23:59,992:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:23:59,992:INFO:PyCaret required dependencies:
2025-02-28 12:23:59,992:INFO:                 pip: 25.0
2025-02-28 12:23:59,992:INFO:          setuptools: 75.8.0
2025-02-28 12:23:59,992:INFO:             pycaret: 3.3.2
2025-02-28 12:23:59,992:INFO:             IPython: 8.30.0
2025-02-28 12:23:59,992:INFO:          ipywidgets: 8.1.5
2025-02-28 12:23:59,992:INFO:                tqdm: 4.67.1
2025-02-28 12:23:59,992:INFO:               numpy: 1.26.4
2025-02-28 12:23:59,992:INFO:              pandas: 2.1.4
2025-02-28 12:23:59,992:INFO:              jinja2: 3.1.5
2025-02-28 12:23:59,992:INFO:               scipy: 1.11.4
2025-02-28 12:23:59,992:INFO:              joblib: 1.3.2
2025-02-28 12:23:59,992:INFO:             sklearn: 1.4.2
2025-02-28 12:23:59,992:INFO:                pyod: 2.0.3
2025-02-28 12:23:59,992:INFO:            imblearn: 0.13.0
2025-02-28 12:23:59,992:INFO:   category_encoders: 2.7.0
2025-02-28 12:23:59,992:INFO:            lightgbm: 4.5.0
2025-02-28 12:23:59,992:INFO:               numba: 0.61.0
2025-02-28 12:23:59,992:INFO:            requests: 2.32.3
2025-02-28 12:23:59,992:INFO:          matplotlib: 3.7.5
2025-02-28 12:23:59,992:INFO:          scikitplot: 0.3.7
2025-02-28 12:23:59,992:INFO:         yellowbrick: 1.5
2025-02-28 12:23:59,993:INFO:              plotly: 5.24.1
2025-02-28 12:23:59,993:INFO:    plotly-resampler: Not installed
2025-02-28 12:23:59,993:INFO:             kaleido: 0.2.1
2025-02-28 12:23:59,993:INFO:           schemdraw: 0.15
2025-02-28 12:23:59,993:INFO:         statsmodels: 0.14.4
2025-02-28 12:23:59,993:INFO:              sktime: 0.26.0
2025-02-28 12:23:59,993:INFO:               tbats: 1.1.3
2025-02-28 12:23:59,993:INFO:            pmdarima: 2.0.4
2025-02-28 12:23:59,993:INFO:              psutil: 5.9.0
2025-02-28 12:23:59,993:INFO:          markupsafe: 2.1.5
2025-02-28 12:23:59,993:INFO:             pickle5: Not installed
2025-02-28 12:23:59,993:INFO:         cloudpickle: 3.1.1
2025-02-28 12:23:59,993:INFO:         deprecation: 2.1.0
2025-02-28 12:23:59,993:INFO:              xxhash: 3.5.0
2025-02-28 12:23:59,993:INFO:           wurlitzer: Not installed
2025-02-28 12:23:59,993:INFO:PyCaret optional dependencies:
2025-02-28 12:23:59,993:INFO:                shap: 0.44.1
2025-02-28 12:23:59,993:INFO:           interpret: 0.6.9
2025-02-28 12:23:59,993:INFO:                umap: 0.5.7
2025-02-28 12:23:59,993:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:23:59,993:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:23:59,993:INFO:             autoviz: Not installed
2025-02-28 12:23:59,993:INFO:           fairlearn: 0.7.0
2025-02-28 12:23:59,994:INFO:          deepchecks: Not installed
2025-02-28 12:23:59,994:INFO:             xgboost: 2.1.4
2025-02-28 12:23:59,994:INFO:            catboost: 1.2.7
2025-02-28 12:23:59,994:INFO:              kmodes: 0.12.2
2025-02-28 12:23:59,994:INFO:             mlxtend: 0.23.4
2025-02-28 12:23:59,994:INFO:       statsforecast: 1.5.0
2025-02-28 12:23:59,994:INFO:        tune_sklearn: Not installed
2025-02-28 12:23:59,994:INFO:                 ray: Not installed
2025-02-28 12:23:59,994:INFO:            hyperopt: 0.2.7
2025-02-28 12:23:59,994:INFO:              optuna: 4.2.0
2025-02-28 12:23:59,994:INFO:               skopt: 0.10.2
2025-02-28 12:23:59,994:INFO:              mlflow: 2.20.1
2025-02-28 12:23:59,994:INFO:              gradio: 5.15.0
2025-02-28 12:23:59,994:INFO:             fastapi: 0.115.8
2025-02-28 12:23:59,994:INFO:             uvicorn: 0.34.0
2025-02-28 12:23:59,994:INFO:              m2cgen: 0.10.0
2025-02-28 12:23:59,994:INFO:           evidently: 0.4.40
2025-02-28 12:23:59,994:INFO:               fugue: 0.8.7
2025-02-28 12:23:59,994:INFO:           streamlit: Not installed
2025-02-28 12:23:59,994:INFO:             prophet: Not installed
2025-02-28 12:23:59,994:INFO:None
2025-02-28 12:23:59,994:INFO:Set up data.
2025-02-28 12:24:00,000:INFO:Set up folding strategy.
2025-02-28 12:24:00,000:INFO:Set up train/test split.
2025-02-28 12:24:00,003:INFO:Set up index.
2025-02-28 12:24:00,003:INFO:Assigning column types.
2025-02-28 12:24:00,006:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:24:00,026:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,027:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,039:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,040:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,060:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,060:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,072:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,072:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,074:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:24:00,094:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,106:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,107:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,127:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,138:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,140:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,140:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 12:24:00,172:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,173:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,208:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,209:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,210:INFO:Preparing preprocessing pipeline...
2025-02-28 12:24:00,211:INFO:Set up simple imputation.
2025-02-28 12:24:00,212:INFO:Set up encoding of ordinal features.
2025-02-28 12:24:00,215:INFO:Set up encoding of categorical features.
2025-02-28 12:24:00,261:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:24:00,268:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score',
                                             'Starting_Salary',
                                             'Career_Sa...
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study',
                                             'Current_Job_Level'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study',
                                                                    'Current_Job_Level'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:24:00,268:INFO:Creating final display dataframe.
2025-02-28 12:24:00,413:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 19)
4        Transformed data shape        (5000, 30)
5   Transformed train set shape        (3500, 30)
6    Transformed test set shape        (1500, 30)
7              Numeric features                14
8          Categorical features                 4
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              c87e
2025-02-28 12:24:00,451:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,452:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,484:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,485:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,486:INFO:setup() successfully completed in 0.5s...............
2025-02-28 12:24:00,488:INFO:PyCaret RegressionExperiment
2025-02-28 12:24:00,488:INFO:Logging name: reg-default-name
2025-02-28 12:24:00,488:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 12:24:00,488:INFO:version 3.3.2
2025-02-28 12:24:00,488:INFO:Initializing setup()
2025-02-28 12:24:00,488:INFO:self.USI: b86f
2025-02-28 12:24:00,488:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'transform_target_param', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:24:00,488:INFO:Checking environment
2025-02-28 12:24:00,488:INFO:python_version: 3.10.16
2025-02-28 12:24:00,488:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:24:00,488:INFO:machine: AMD64
2025-02-28 12:24:00,488:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:24:00,488:INFO:Memory: svmem(total=34200334336, available=8937410560, percent=73.9, used=25262923776, free=8937410560)
2025-02-28 12:24:00,488:INFO:Physical Core: 24
2025-02-28 12:24:00,488:INFO:Logical Core: 32
2025-02-28 12:24:00,488:INFO:Checking libraries
2025-02-28 12:24:00,489:INFO:System:
2025-02-28 12:24:00,489:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:24:00,489:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:24:00,489:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:24:00,489:INFO:PyCaret required dependencies:
2025-02-28 12:24:00,489:INFO:                 pip: 25.0
2025-02-28 12:24:00,489:INFO:          setuptools: 75.8.0
2025-02-28 12:24:00,489:INFO:             pycaret: 3.3.2
2025-02-28 12:24:00,489:INFO:             IPython: 8.30.0
2025-02-28 12:24:00,489:INFO:          ipywidgets: 8.1.5
2025-02-28 12:24:00,489:INFO:                tqdm: 4.67.1
2025-02-28 12:24:00,489:INFO:               numpy: 1.26.4
2025-02-28 12:24:00,489:INFO:              pandas: 2.1.4
2025-02-28 12:24:00,489:INFO:              jinja2: 3.1.5
2025-02-28 12:24:00,489:INFO:               scipy: 1.11.4
2025-02-28 12:24:00,489:INFO:              joblib: 1.3.2
2025-02-28 12:24:00,489:INFO:             sklearn: 1.4.2
2025-02-28 12:24:00,489:INFO:                pyod: 2.0.3
2025-02-28 12:24:00,489:INFO:            imblearn: 0.13.0
2025-02-28 12:24:00,489:INFO:   category_encoders: 2.7.0
2025-02-28 12:24:00,489:INFO:            lightgbm: 4.5.0
2025-02-28 12:24:00,489:INFO:               numba: 0.61.0
2025-02-28 12:24:00,489:INFO:            requests: 2.32.3
2025-02-28 12:24:00,489:INFO:          matplotlib: 3.7.5
2025-02-28 12:24:00,489:INFO:          scikitplot: 0.3.7
2025-02-28 12:24:00,490:INFO:         yellowbrick: 1.5
2025-02-28 12:24:00,490:INFO:              plotly: 5.24.1
2025-02-28 12:24:00,490:INFO:    plotly-resampler: Not installed
2025-02-28 12:24:00,490:INFO:             kaleido: 0.2.1
2025-02-28 12:24:00,490:INFO:           schemdraw: 0.15
2025-02-28 12:24:00,490:INFO:         statsmodels: 0.14.4
2025-02-28 12:24:00,490:INFO:              sktime: 0.26.0
2025-02-28 12:24:00,490:INFO:               tbats: 1.1.3
2025-02-28 12:24:00,490:INFO:            pmdarima: 2.0.4
2025-02-28 12:24:00,490:INFO:              psutil: 5.9.0
2025-02-28 12:24:00,490:INFO:          markupsafe: 2.1.5
2025-02-28 12:24:00,490:INFO:             pickle5: Not installed
2025-02-28 12:24:00,490:INFO:         cloudpickle: 3.1.1
2025-02-28 12:24:00,490:INFO:         deprecation: 2.1.0
2025-02-28 12:24:00,490:INFO:              xxhash: 3.5.0
2025-02-28 12:24:00,490:INFO:           wurlitzer: Not installed
2025-02-28 12:24:00,490:INFO:PyCaret optional dependencies:
2025-02-28 12:24:00,490:INFO:                shap: 0.44.1
2025-02-28 12:24:00,490:INFO:           interpret: 0.6.9
2025-02-28 12:24:00,490:INFO:                umap: 0.5.7
2025-02-28 12:24:00,490:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:24:00,490:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:24:00,490:INFO:             autoviz: Not installed
2025-02-28 12:24:00,490:INFO:           fairlearn: 0.7.0
2025-02-28 12:24:00,490:INFO:          deepchecks: Not installed
2025-02-28 12:24:00,491:INFO:             xgboost: 2.1.4
2025-02-28 12:24:00,491:INFO:            catboost: 1.2.7
2025-02-28 12:24:00,491:INFO:              kmodes: 0.12.2
2025-02-28 12:24:00,491:INFO:             mlxtend: 0.23.4
2025-02-28 12:24:00,491:INFO:       statsforecast: 1.5.0
2025-02-28 12:24:00,491:INFO:        tune_sklearn: Not installed
2025-02-28 12:24:00,491:INFO:                 ray: Not installed
2025-02-28 12:24:00,491:INFO:            hyperopt: 0.2.7
2025-02-28 12:24:00,491:INFO:              optuna: 4.2.0
2025-02-28 12:24:00,491:INFO:               skopt: 0.10.2
2025-02-28 12:24:00,491:INFO:              mlflow: 2.20.1
2025-02-28 12:24:00,491:INFO:              gradio: 5.15.0
2025-02-28 12:24:00,491:INFO:             fastapi: 0.115.8
2025-02-28 12:24:00,491:INFO:             uvicorn: 0.34.0
2025-02-28 12:24:00,491:INFO:              m2cgen: 0.10.0
2025-02-28 12:24:00,491:INFO:           evidently: 0.4.40
2025-02-28 12:24:00,491:INFO:               fugue: 0.8.7
2025-02-28 12:24:00,491:INFO:           streamlit: Not installed
2025-02-28 12:24:00,491:INFO:             prophet: Not installed
2025-02-28 12:24:00,491:INFO:None
2025-02-28 12:24:00,491:INFO:Set up data.
2025-02-28 12:24:26,378:INFO:PyCaret ClassificationExperiment
2025-02-28 12:24:26,378:INFO:Logging name: clf-default-name
2025-02-28 12:24:26,378:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 12:24:26,378:INFO:version 3.3.2
2025-02-28 12:24:26,378:INFO:Initializing setup()
2025-02-28 12:24:26,378:INFO:self.USI: cd25
2025-02-28 12:24:26,378:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fix_imbalance', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'is_multiclass', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:24:26,378:INFO:Checking environment
2025-02-28 12:24:26,378:INFO:python_version: 3.10.16
2025-02-28 12:24:26,378:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:24:26,378:INFO:machine: AMD64
2025-02-28 12:24:26,379:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:24:26,379:INFO:Memory: svmem(total=34200334336, available=9020604416, percent=73.6, used=25179729920, free=9020604416)
2025-02-28 12:24:26,379:INFO:Physical Core: 24
2025-02-28 12:24:26,379:INFO:Logical Core: 32
2025-02-28 12:24:26,379:INFO:Checking libraries
2025-02-28 12:24:26,379:INFO:System:
2025-02-28 12:24:26,379:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:24:26,379:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:24:26,379:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:24:26,379:INFO:PyCaret required dependencies:
2025-02-28 12:24:26,379:INFO:                 pip: 25.0
2025-02-28 12:24:26,379:INFO:          setuptools: 75.8.0
2025-02-28 12:24:26,379:INFO:             pycaret: 3.3.2
2025-02-28 12:24:26,380:INFO:             IPython: 8.30.0
2025-02-28 12:24:26,380:INFO:          ipywidgets: 8.1.5
2025-02-28 12:24:26,380:INFO:                tqdm: 4.67.1
2025-02-28 12:24:26,380:INFO:               numpy: 1.26.4
2025-02-28 12:24:26,380:INFO:              pandas: 2.1.4
2025-02-28 12:24:26,380:INFO:              jinja2: 3.1.5
2025-02-28 12:24:26,380:INFO:               scipy: 1.11.4
2025-02-28 12:24:26,380:INFO:              joblib: 1.3.2
2025-02-28 12:24:26,381:INFO:             sklearn: 1.4.2
2025-02-28 12:24:26,382:INFO:                pyod: 2.0.3
2025-02-28 12:24:26,382:INFO:            imblearn: 0.13.0
2025-02-28 12:24:26,382:INFO:   category_encoders: 2.7.0
2025-02-28 12:24:26,382:INFO:            lightgbm: 4.5.0
2025-02-28 12:24:26,382:INFO:               numba: 0.61.0
2025-02-28 12:24:26,382:INFO:            requests: 2.32.3
2025-02-28 12:24:26,382:INFO:          matplotlib: 3.7.5
2025-02-28 12:24:26,382:INFO:          scikitplot: 0.3.7
2025-02-28 12:24:26,382:INFO:         yellowbrick: 1.5
2025-02-28 12:24:26,382:INFO:              plotly: 5.24.1
2025-02-28 12:24:26,382:INFO:    plotly-resampler: Not installed
2025-02-28 12:24:26,382:INFO:             kaleido: 0.2.1
2025-02-28 12:24:26,382:INFO:           schemdraw: 0.15
2025-02-28 12:24:26,382:INFO:         statsmodels: 0.14.4
2025-02-28 12:24:26,382:INFO:              sktime: 0.26.0
2025-02-28 12:24:26,382:INFO:               tbats: 1.1.3
2025-02-28 12:24:26,382:INFO:            pmdarima: 2.0.4
2025-02-28 12:24:26,382:INFO:              psutil: 5.9.0
2025-02-28 12:24:26,382:INFO:          markupsafe: 2.1.5
2025-02-28 12:24:26,383:INFO:             pickle5: Not installed
2025-02-28 12:24:26,383:INFO:         cloudpickle: 3.1.1
2025-02-28 12:24:26,383:INFO:         deprecation: 2.1.0
2025-02-28 12:24:26,383:INFO:              xxhash: 3.5.0
2025-02-28 12:24:26,383:INFO:           wurlitzer: Not installed
2025-02-28 12:24:26,383:INFO:PyCaret optional dependencies:
2025-02-28 12:24:26,383:INFO:                shap: 0.44.1
2025-02-28 12:24:26,383:INFO:           interpret: 0.6.9
2025-02-28 12:24:26,383:INFO:                umap: 0.5.7
2025-02-28 12:24:26,383:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:24:26,383:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:24:26,383:INFO:             autoviz: Not installed
2025-02-28 12:24:26,383:INFO:           fairlearn: 0.7.0
2025-02-28 12:24:26,383:INFO:          deepchecks: Not installed
2025-02-28 12:24:26,383:INFO:             xgboost: 2.1.4
2025-02-28 12:24:26,383:INFO:            catboost: 1.2.7
2025-02-28 12:24:26,383:INFO:              kmodes: 0.12.2
2025-02-28 12:24:26,383:INFO:             mlxtend: 0.23.4
2025-02-28 12:24:26,383:INFO:       statsforecast: 1.5.0
2025-02-28 12:24:26,383:INFO:        tune_sklearn: Not installed
2025-02-28 12:24:26,383:INFO:                 ray: Not installed
2025-02-28 12:24:26,383:INFO:            hyperopt: 0.2.7
2025-02-28 12:24:26,383:INFO:              optuna: 4.2.0
2025-02-28 12:24:26,383:INFO:               skopt: 0.10.2
2025-02-28 12:24:26,383:INFO:              mlflow: 2.20.1
2025-02-28 12:24:26,383:INFO:              gradio: 5.15.0
2025-02-28 12:24:26,384:INFO:             fastapi: 0.115.8
2025-02-28 12:24:26,384:INFO:             uvicorn: 0.34.0
2025-02-28 12:24:26,384:INFO:              m2cgen: 0.10.0
2025-02-28 12:24:26,384:INFO:           evidently: 0.4.40
2025-02-28 12:24:26,384:INFO:               fugue: 0.8.7
2025-02-28 12:24:26,384:INFO:           streamlit: Not installed
2025-02-28 12:24:26,384:INFO:             prophet: Not installed
2025-02-28 12:24:26,384:INFO:None
2025-02-28 12:24:26,384:INFO:Set up data.
2025-02-28 12:24:26,389:INFO:Set up folding strategy.
2025-02-28 12:24:26,389:INFO:Set up train/test split.
2025-02-28 12:24:26,393:INFO:Set up index.
2025-02-28 12:24:26,394:INFO:Assigning column types.
2025-02-28 12:24:26,397:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:24:26,416:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,416:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,429:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,430:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,451:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,451:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,464:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,465:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,465:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:24:26,485:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,496:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,498:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,519:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,531:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,532:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,533:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 12:24:26,564:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,565:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,597:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,598:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,599:INFO:Preparing preprocessing pipeline...
2025-02-28 12:24:26,600:INFO:Set up simple imputation.
2025-02-28 12:24:26,601:INFO:Set up encoding of ordinal features.
2025-02-28 12:24:26,603:INFO:Set up encoding of categorical features.
2025-02-28 12:24:26,644:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:24:26,652:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score',
                                             'Starting_Salary',
                                             'Career_Sa...
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study',
                                             'Current_Job_Level'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study',
                                                                    'Current_Job_Level'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:24:26,652:INFO:Creating final display dataframe.
2025-02-28 12:24:26,791:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 19)
4        Transformed data shape        (5000, 30)
5   Transformed train set shape        (3500, 30)
6    Transformed test set shape        (1500, 30)
7              Numeric features                14
8          Categorical features                 4
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              cd25
2025-02-28 12:24:26,827:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,829:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,860:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,861:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,862:INFO:setup() successfully completed in 0.49s...............
2025-02-28 12:24:26,888:INFO:Initializing compare_models()
2025-02-28 12:24:26,888:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 12:24:26,888:INFO:Checking exceptions
2025-02-28 12:24:26,891:INFO:Preparing display monitor
2025-02-28 12:24:26,904:INFO:Initializing Logistic Regression
2025-02-28 12:24:26,904:INFO:Total runtime is 0.0 minutes
2025-02-28 12:24:26,905:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:26,906:INFO:Initializing create_model()
2025-02-28 12:24:26,906:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:26,906:INFO:Checking exceptions
2025-02-28 12:24:26,906:INFO:Importing libraries
2025-02-28 12:24:26,906:INFO:Copying training dataset
2025-02-28 12:24:26,909:INFO:Defining folds
2025-02-28 12:24:26,909:INFO:Declaring metric variables
2025-02-28 12:24:26,911:INFO:Importing untrained model
2025-02-28 12:24:26,913:INFO:Logistic Regression Imported successfully
2025-02-28 12:24:26,917:INFO:Starting cross validation
2025-02-28 12:24:26,918:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:27,419:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,431:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,435:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,449:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,452:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,456:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,462:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,469:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,470:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,473:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,476:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,476:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,476:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,477:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,480:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,485:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,492:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,492:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,493:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,493:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,503:INFO:Calculating mean and std
2025-02-28 12:24:27,503:INFO:Creating metrics dataframe
2025-02-28 12:24:27,504:INFO:Uploading results into container
2025-02-28 12:24:27,504:INFO:Uploading model into container now
2025-02-28 12:24:27,504:INFO:_master_model_container: 1
2025-02-28 12:24:27,504:INFO:_display_container: 2
2025-02-28 12:24:27,505:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 12:24:27,505:INFO:create_model() successfully completed......................................
2025-02-28 12:24:27,655:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:27,655:INFO:Creating metrics dataframe
2025-02-28 12:24:27,658:INFO:Initializing K Neighbors Classifier
2025-02-28 12:24:27,658:INFO:Total runtime is 0.012566765149434408 minutes
2025-02-28 12:24:27,660:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:27,661:INFO:Initializing create_model()
2025-02-28 12:24:27,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:27,661:INFO:Checking exceptions
2025-02-28 12:24:27,661:INFO:Importing libraries
2025-02-28 12:24:27,661:INFO:Copying training dataset
2025-02-28 12:24:27,664:INFO:Defining folds
2025-02-28 12:24:27,664:INFO:Declaring metric variables
2025-02-28 12:24:27,667:INFO:Importing untrained model
2025-02-28 12:24:27,669:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:24:27,672:INFO:Starting cross validation
2025-02-28 12:24:27,673:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:27,815:INFO:Calculating mean and std
2025-02-28 12:24:27,815:INFO:Creating metrics dataframe
2025-02-28 12:24:27,816:INFO:Uploading results into container
2025-02-28 12:24:27,817:INFO:Uploading model into container now
2025-02-28 12:24:27,817:INFO:_master_model_container: 2
2025-02-28 12:24:27,817:INFO:_display_container: 2
2025-02-28 12:24:27,817:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:24:27,817:INFO:create_model() successfully completed......................................
2025-02-28 12:24:27,955:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:27,955:INFO:Creating metrics dataframe
2025-02-28 12:24:27,959:INFO:Initializing Naive Bayes
2025-02-28 12:24:27,959:INFO:Total runtime is 0.017585055033365885 minutes
2025-02-28 12:24:27,962:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:27,962:INFO:Initializing create_model()
2025-02-28 12:24:27,962:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:27,962:INFO:Checking exceptions
2025-02-28 12:24:27,962:INFO:Importing libraries
2025-02-28 12:24:27,962:INFO:Copying training dataset
2025-02-28 12:24:27,966:INFO:Defining folds
2025-02-28 12:24:27,966:INFO:Declaring metric variables
2025-02-28 12:24:27,968:INFO:Importing untrained model
2025-02-28 12:24:27,970:INFO:Naive Bayes Imported successfully
2025-02-28 12:24:27,975:INFO:Starting cross validation
2025-02-28 12:24:27,976:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:28,079:INFO:Calculating mean and std
2025-02-28 12:24:28,080:INFO:Creating metrics dataframe
2025-02-28 12:24:28,081:INFO:Uploading results into container
2025-02-28 12:24:28,081:INFO:Uploading model into container now
2025-02-28 12:24:28,081:INFO:_master_model_container: 3
2025-02-28 12:24:28,081:INFO:_display_container: 2
2025-02-28 12:24:28,081:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 12:24:28,082:INFO:create_model() successfully completed......................................
2025-02-28 12:24:28,218:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:28,218:INFO:Creating metrics dataframe
2025-02-28 12:24:28,222:INFO:Initializing Decision Tree Classifier
2025-02-28 12:24:28,222:INFO:Total runtime is 0.021964287757873534 minutes
2025-02-28 12:24:28,225:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:28,225:INFO:Initializing create_model()
2025-02-28 12:24:28,225:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:28,225:INFO:Checking exceptions
2025-02-28 12:24:28,225:INFO:Importing libraries
2025-02-28 12:24:28,225:INFO:Copying training dataset
2025-02-28 12:24:28,228:INFO:Defining folds
2025-02-28 12:24:28,228:INFO:Declaring metric variables
2025-02-28 12:24:28,230:INFO:Importing untrained model
2025-02-28 12:24:28,233:INFO:Decision Tree Classifier Imported successfully
2025-02-28 12:24:28,238:INFO:Starting cross validation
2025-02-28 12:24:28,239:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:28,373:INFO:Calculating mean and std
2025-02-28 12:24:28,373:INFO:Creating metrics dataframe
2025-02-28 12:24:28,375:INFO:Uploading results into container
2025-02-28 12:24:28,375:INFO:Uploading model into container now
2025-02-28 12:24:28,375:INFO:_master_model_container: 4
2025-02-28 12:24:28,375:INFO:_display_container: 2
2025-02-28 12:24:28,376:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:24:28,376:INFO:create_model() successfully completed......................................
2025-02-28 12:24:28,511:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:28,511:INFO:Creating metrics dataframe
2025-02-28 12:24:28,516:INFO:Initializing SVM - Linear Kernel
2025-02-28 12:24:28,516:INFO:Total runtime is 0.026861933867136638 minutes
2025-02-28 12:24:28,519:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:28,519:INFO:Initializing create_model()
2025-02-28 12:24:28,519:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:28,519:INFO:Checking exceptions
2025-02-28 12:24:28,519:INFO:Importing libraries
2025-02-28 12:24:28,519:INFO:Copying training dataset
2025-02-28 12:24:28,522:INFO:Defining folds
2025-02-28 12:24:28,523:INFO:Declaring metric variables
2025-02-28 12:24:28,525:INFO:Importing untrained model
2025-02-28 12:24:28,527:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 12:24:28,533:INFO:Starting cross validation
2025-02-28 12:24:28,534:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:28,666:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,668:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,669:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,671:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,680:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,681:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,681:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,683:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,683:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,683:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,692:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,693:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,694:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,694:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,707:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,707:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,709:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,709:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,709:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,718:INFO:Calculating mean and std
2025-02-28 12:24:28,719:INFO:Creating metrics dataframe
2025-02-28 12:24:28,720:INFO:Uploading results into container
2025-02-28 12:24:28,720:INFO:Uploading model into container now
2025-02-28 12:24:28,720:INFO:_master_model_container: 5
2025-02-28 12:24:28,720:INFO:_display_container: 2
2025-02-28 12:24:28,721:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:24:28,721:INFO:create_model() successfully completed......................................
2025-02-28 12:24:28,857:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:28,857:INFO:Creating metrics dataframe
2025-02-28 12:24:28,861:INFO:Initializing Ridge Classifier
2025-02-28 12:24:28,861:INFO:Total runtime is 0.03261667092641195 minutes
2025-02-28 12:24:28,863:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:28,863:INFO:Initializing create_model()
2025-02-28 12:24:28,864:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:28,864:INFO:Checking exceptions
2025-02-28 12:24:28,864:INFO:Importing libraries
2025-02-28 12:24:28,864:INFO:Copying training dataset
2025-02-28 12:24:28,867:INFO:Defining folds
2025-02-28 12:24:28,868:INFO:Declaring metric variables
2025-02-28 12:24:28,870:INFO:Importing untrained model
2025-02-28 12:24:28,872:INFO:Ridge Classifier Imported successfully
2025-02-28 12:24:28,876:INFO:Starting cross validation
2025-02-28 12:24:28,877:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:28,953:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,953:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,953:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,955:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,966:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,968:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,971:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,972:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,973:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,977:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,997:INFO:Calculating mean and std
2025-02-28 12:24:28,998:INFO:Creating metrics dataframe
2025-02-28 12:24:28,999:INFO:Uploading results into container
2025-02-28 12:24:28,999:INFO:Uploading model into container now
2025-02-28 12:24:29,000:INFO:_master_model_container: 6
2025-02-28 12:24:29,000:INFO:_display_container: 2
2025-02-28 12:24:29,000:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 12:24:29,000:INFO:create_model() successfully completed......................................
2025-02-28 12:24:29,137:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:29,137:INFO:Creating metrics dataframe
2025-02-28 12:24:29,142:INFO:Initializing Random Forest Classifier
2025-02-28 12:24:29,143:INFO:Total runtime is 0.037310230731964114 minutes
2025-02-28 12:24:29,144:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:29,145:INFO:Initializing create_model()
2025-02-28 12:24:29,145:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:29,145:INFO:Checking exceptions
2025-02-28 12:24:29,145:INFO:Importing libraries
2025-02-28 12:24:29,145:INFO:Copying training dataset
2025-02-28 12:24:29,148:INFO:Defining folds
2025-02-28 12:24:29,148:INFO:Declaring metric variables
2025-02-28 12:24:29,151:INFO:Importing untrained model
2025-02-28 12:24:29,154:INFO:Random Forest Classifier Imported successfully
2025-02-28 12:24:29,158:INFO:Starting cross validation
2025-02-28 12:24:29,159:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:29,651:INFO:Calculating mean and std
2025-02-28 12:24:29,652:INFO:Creating metrics dataframe
2025-02-28 12:24:29,653:INFO:Uploading results into container
2025-02-28 12:24:29,653:INFO:Uploading model into container now
2025-02-28 12:24:29,654:INFO:_master_model_container: 7
2025-02-28 12:24:29,654:INFO:_display_container: 2
2025-02-28 12:24:29,654:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 12:24:29,654:INFO:create_model() successfully completed......................................
2025-02-28 12:24:29,789:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:29,790:INFO:Creating metrics dataframe
2025-02-28 12:24:29,795:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 12:24:29,795:INFO:Total runtime is 0.048177178700764975 minutes
2025-02-28 12:24:29,798:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:29,798:INFO:Initializing create_model()
2025-02-28 12:24:29,798:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:29,798:INFO:Checking exceptions
2025-02-28 12:24:29,798:INFO:Importing libraries
2025-02-28 12:24:29,798:INFO:Copying training dataset
2025-02-28 12:24:29,802:INFO:Defining folds
2025-02-28 12:24:29,802:INFO:Declaring metric variables
2025-02-28 12:24:29,804:INFO:Importing untrained model
2025-02-28 12:24:29,806:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 12:24:29,811:INFO:Starting cross validation
2025-02-28 12:24:29,812:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:29,861:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,865:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,869:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,870:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,872:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,872:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,879:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,880:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,880:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,881:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,885:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,886:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,889:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,889:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,891:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,898:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,899:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,907:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,918:INFO:Calculating mean and std
2025-02-28 12:24:29,918:INFO:Creating metrics dataframe
2025-02-28 12:24:29,920:INFO:Uploading results into container
2025-02-28 12:24:29,920:INFO:Uploading model into container now
2025-02-28 12:24:29,920:INFO:_master_model_container: 8
2025-02-28 12:24:29,920:INFO:_display_container: 2
2025-02-28 12:24:29,920:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 12:24:29,920:INFO:create_model() successfully completed......................................
2025-02-28 12:24:30,054:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:30,054:INFO:Creating metrics dataframe
2025-02-28 12:24:30,060:INFO:Initializing Ada Boost Classifier
2025-02-28 12:24:30,060:INFO:Total runtime is 0.05258921384811401 minutes
2025-02-28 12:24:30,062:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:30,062:INFO:Initializing create_model()
2025-02-28 12:24:30,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:30,063:INFO:Checking exceptions
2025-02-28 12:24:30,063:INFO:Importing libraries
2025-02-28 12:24:30,063:INFO:Copying training dataset
2025-02-28 12:24:30,065:INFO:Defining folds
2025-02-28 12:24:30,066:INFO:Declaring metric variables
2025-02-28 12:24:30,068:INFO:Importing untrained model
2025-02-28 12:24:30,070:INFO:Ada Boost Classifier Imported successfully
2025-02-28 12:24:30,074:INFO:Starting cross validation
2025-02-28 12:24:30,076:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:30,125:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,128:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,131:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,131:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,132:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,133:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,139:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,140:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,142:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,304:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,305:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,305:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,309:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,312:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,314:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,316:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,318:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,321:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,329:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,338:INFO:Calculating mean and std
2025-02-28 12:24:30,339:INFO:Creating metrics dataframe
2025-02-28 12:24:30,340:INFO:Uploading results into container
2025-02-28 12:24:30,340:INFO:Uploading model into container now
2025-02-28 12:24:30,340:INFO:_master_model_container: 9
2025-02-28 12:24:30,340:INFO:_display_container: 2
2025-02-28 12:24:30,341:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 12:24:30,341:INFO:create_model() successfully completed......................................
2025-02-28 12:24:30,471:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:30,472:INFO:Creating metrics dataframe
2025-02-28 12:24:30,476:INFO:Initializing Gradient Boosting Classifier
2025-02-28 12:24:30,476:INFO:Total runtime is 0.059534160296122234 minutes
2025-02-28 12:24:30,479:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:30,479:INFO:Initializing create_model()
2025-02-28 12:24:30,479:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:30,479:INFO:Checking exceptions
2025-02-28 12:24:30,479:INFO:Importing libraries
2025-02-28 12:24:30,479:INFO:Copying training dataset
2025-02-28 12:24:30,483:INFO:Defining folds
2025-02-28 12:24:30,483:INFO:Declaring metric variables
2025-02-28 12:24:30,486:INFO:Importing untrained model
2025-02-28 12:24:30,488:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 12:24:30,492:INFO:Starting cross validation
2025-02-28 12:24:30,493:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:33,261:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,262:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,278:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,279:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,302:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,302:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,309:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,329:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,362:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,369:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,380:INFO:Calculating mean and std
2025-02-28 12:24:33,381:INFO:Creating metrics dataframe
2025-02-28 12:24:33,382:INFO:Uploading results into container
2025-02-28 12:24:33,382:INFO:Uploading model into container now
2025-02-28 12:24:33,382:INFO:_master_model_container: 10
2025-02-28 12:24:33,382:INFO:_display_container: 2
2025-02-28 12:24:33,383:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:24:33,383:INFO:create_model() successfully completed......................................
2025-02-28 12:24:33,526:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:33,526:INFO:Creating metrics dataframe
2025-02-28 12:24:33,531:INFO:Initializing Linear Discriminant Analysis
2025-02-28 12:24:33,532:INFO:Total runtime is 0.11046508153279622 minutes
2025-02-28 12:24:33,534:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:33,534:INFO:Initializing create_model()
2025-02-28 12:24:33,534:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:33,534:INFO:Checking exceptions
2025-02-28 12:24:33,534:INFO:Importing libraries
2025-02-28 12:24:33,534:INFO:Copying training dataset
2025-02-28 12:24:33,538:INFO:Defining folds
2025-02-28 12:24:33,538:INFO:Declaring metric variables
2025-02-28 12:24:33,541:INFO:Importing untrained model
2025-02-28 12:24:33,543:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 12:24:33,547:INFO:Starting cross validation
2025-02-28 12:24:33,548:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:33,616:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,625:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,630:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,634:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,637:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,640:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,643:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,643:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,681:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,691:INFO:Calculating mean and std
2025-02-28 12:24:33,692:INFO:Creating metrics dataframe
2025-02-28 12:24:33,692:INFO:Uploading results into container
2025-02-28 12:24:33,694:INFO:Uploading model into container now
2025-02-28 12:24:33,694:INFO:_master_model_container: 11
2025-02-28 12:24:33,694:INFO:_display_container: 2
2025-02-28 12:24:33,694:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 12:24:33,694:INFO:create_model() successfully completed......................................
2025-02-28 12:24:33,835:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:33,835:INFO:Creating metrics dataframe
2025-02-28 12:24:33,840:INFO:Initializing Extra Trees Classifier
2025-02-28 12:24:33,840:INFO:Total runtime is 0.1156006654103597 minutes
2025-02-28 12:24:33,842:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:33,842:INFO:Initializing create_model()
2025-02-28 12:24:33,842:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:33,842:INFO:Checking exceptions
2025-02-28 12:24:33,842:INFO:Importing libraries
2025-02-28 12:24:33,842:INFO:Copying training dataset
2025-02-28 12:24:33,845:INFO:Defining folds
2025-02-28 12:24:33,845:INFO:Declaring metric variables
2025-02-28 12:24:33,848:INFO:Importing untrained model
2025-02-28 12:24:33,850:INFO:Extra Trees Classifier Imported successfully
2025-02-28 12:24:33,854:INFO:Starting cross validation
2025-02-28 12:24:33,856:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:34,267:INFO:Calculating mean and std
2025-02-28 12:24:34,268:INFO:Creating metrics dataframe
2025-02-28 12:24:34,269:INFO:Uploading results into container
2025-02-28 12:24:34,269:INFO:Uploading model into container now
2025-02-28 12:24:34,270:INFO:_master_model_container: 12
2025-02-28 12:24:34,270:INFO:_display_container: 2
2025-02-28 12:24:34,270:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 12:24:34,270:INFO:create_model() successfully completed......................................
2025-02-28 12:24:34,406:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:34,406:INFO:Creating metrics dataframe
2025-02-28 12:24:34,411:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:24:34,411:INFO:Total runtime is 0.1251200238863627 minutes
2025-02-28 12:24:34,414:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:34,415:INFO:Initializing create_model()
2025-02-28 12:24:34,415:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:34,415:INFO:Checking exceptions
2025-02-28 12:24:34,415:INFO:Importing libraries
2025-02-28 12:24:34,415:INFO:Copying training dataset
2025-02-28 12:24:34,418:INFO:Defining folds
2025-02-28 12:24:34,418:INFO:Declaring metric variables
2025-02-28 12:24:34,420:INFO:Importing untrained model
2025-02-28 12:24:34,423:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:24:34,427:INFO:Starting cross validation
2025-02-28 12:24:34,428:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:35,157:INFO:Calculating mean and std
2025-02-28 12:24:35,157:INFO:Creating metrics dataframe
2025-02-28 12:24:35,159:INFO:Uploading results into container
2025-02-28 12:24:35,159:INFO:Uploading model into container now
2025-02-28 12:24:35,159:INFO:_master_model_container: 13
2025-02-28 12:24:35,160:INFO:_display_container: 2
2025-02-28 12:24:35,160:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 12:24:35,160:INFO:create_model() successfully completed......................................
2025-02-28 12:24:35,296:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:35,296:INFO:Creating metrics dataframe
2025-02-28 12:24:35,302:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:24:35,302:INFO:Total runtime is 0.13995962540308635 minutes
2025-02-28 12:24:35,304:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:35,304:INFO:Initializing create_model()
2025-02-28 12:24:35,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:35,304:INFO:Checking exceptions
2025-02-28 12:24:35,304:INFO:Importing libraries
2025-02-28 12:24:35,304:INFO:Copying training dataset
2025-02-28 12:24:35,307:INFO:Defining folds
2025-02-28 12:24:35,307:INFO:Declaring metric variables
2025-02-28 12:24:35,310:INFO:Importing untrained model
2025-02-28 12:24:35,312:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:24:35,317:INFO:Starting cross validation
2025-02-28 12:24:35,318:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:43,006:INFO:Calculating mean and std
2025-02-28 12:24:43,007:INFO:Creating metrics dataframe
2025-02-28 12:24:43,008:INFO:Uploading results into container
2025-02-28 12:24:43,009:INFO:Uploading model into container now
2025-02-28 12:24:43,009:INFO:_master_model_container: 14
2025-02-28 12:24:43,009:INFO:_display_container: 2
2025-02-28 12:24:43,009:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:24:43,010:INFO:create_model() successfully completed......................................
2025-02-28 12:24:43,163:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:43,163:INFO:Creating metrics dataframe
2025-02-28 12:24:43,169:INFO:Initializing CatBoost Classifier
2025-02-28 12:24:43,169:INFO:Total runtime is 0.2710877617200216 minutes
2025-02-28 12:24:43,171:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:43,171:INFO:Initializing create_model()
2025-02-28 12:24:43,171:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:43,171:INFO:Checking exceptions
2025-02-28 12:24:43,171:INFO:Importing libraries
2025-02-28 12:24:43,171:INFO:Copying training dataset
2025-02-28 12:24:43,175:INFO:Defining folds
2025-02-28 12:24:43,175:INFO:Declaring metric variables
2025-02-28 12:24:43,177:INFO:Importing untrained model
2025-02-28 12:24:43,179:INFO:CatBoost Classifier Imported successfully
2025-02-28 12:24:43,183:INFO:Starting cross validation
2025-02-28 12:24:43,185:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:50,792:INFO:Calculating mean and std
2025-02-28 12:24:50,793:INFO:Creating metrics dataframe
2025-02-28 12:24:50,794:INFO:Uploading results into container
2025-02-28 12:24:50,794:INFO:Uploading model into container now
2025-02-28 12:24:50,794:INFO:_master_model_container: 15
2025-02-28 12:24:50,794:INFO:_display_container: 2
2025-02-28 12:24:50,794:INFO:<catboost.core.CatBoostClassifier object at 0x0000024E516CBE80>
2025-02-28 12:24:50,794:INFO:create_model() successfully completed......................................
2025-02-28 12:24:50,935:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:50,935:INFO:Creating metrics dataframe
2025-02-28 12:24:50,941:INFO:Initializing Dummy Classifier
2025-02-28 12:24:50,941:INFO:Total runtime is 0.4006140947341919 minutes
2025-02-28 12:24:50,943:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:50,943:INFO:Initializing create_model()
2025-02-28 12:24:50,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:50,943:INFO:Checking exceptions
2025-02-28 12:24:50,944:INFO:Importing libraries
2025-02-28 12:24:50,944:INFO:Copying training dataset
2025-02-28 12:24:50,947:INFO:Defining folds
2025-02-28 12:24:50,947:INFO:Declaring metric variables
2025-02-28 12:24:50,949:INFO:Importing untrained model
2025-02-28 12:24:50,952:INFO:Dummy Classifier Imported successfully
2025-02-28 12:24:50,956:INFO:Starting cross validation
2025-02-28 12:24:50,957:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:51,033:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,033:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,046:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,047:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,051:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,052:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,056:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,062:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,071:INFO:Calculating mean and std
2025-02-28 12:24:51,072:INFO:Creating metrics dataframe
2025-02-28 12:24:51,073:INFO:Uploading results into container
2025-02-28 12:24:51,073:INFO:Uploading model into container now
2025-02-28 12:24:51,074:INFO:_master_model_container: 16
2025-02-28 12:24:51,074:INFO:_display_container: 2
2025-02-28 12:24:51,074:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 12:24:51,074:INFO:create_model() successfully completed......................................
2025-02-28 12:24:51,215:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:51,215:INFO:Creating metrics dataframe
2025-02-28 12:24:51,221:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:24:51,227:INFO:Initializing create_model()
2025-02-28 12:24:51,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:51,227:INFO:Checking exceptions
2025-02-28 12:24:51,228:INFO:Importing libraries
2025-02-28 12:24:51,228:INFO:Copying training dataset
2025-02-28 12:24:51,231:INFO:Defining folds
2025-02-28 12:24:51,231:INFO:Declaring metric variables
2025-02-28 12:24:51,231:INFO:Importing untrained model
2025-02-28 12:24:51,231:INFO:Declaring custom model
2025-02-28 12:24:51,232:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:24:51,232:INFO:Cross validation set to False
2025-02-28 12:24:51,233:INFO:Fitting Model
2025-02-28 12:24:51,268:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:24:51,268:INFO:create_model() successfully completed......................................
2025-02-28 12:24:51,424:INFO:_master_model_container: 16
2025-02-28 12:24:51,424:INFO:_display_container: 2
2025-02-28 12:24:51,424:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:24:51,424:INFO:compare_models() successfully completed......................................
2025-02-28 12:24:51,445:INFO:Initializing plot_model()
2025-02-28 12:24:51,445:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, system=True)
2025-02-28 12:24:51,445:INFO:Checking exceptions
2025-02-28 12:24:51,448:INFO:Preloading libraries
2025-02-28 12:24:51,449:INFO:Copying training dataset
2025-02-28 12:24:51,449:INFO:Plot type: confusion_matrix
2025-02-28 12:24:51,593:INFO:Fitting Model
2025-02-28 12:24:51,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:24:51,593:INFO:Scoring test/hold-out set
2025-02-28 12:24:51,939:INFO:Visual Rendered Successfully
2025-02-28 12:24:52,076:INFO:plot_model() successfully completed......................................
2025-02-28 12:24:52,092:INFO:Initializing plot_model()
2025-02-28 12:24:52,092:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, system=True)
2025-02-28 12:24:52,092:INFO:Checking exceptions
2025-02-28 12:24:52,096:INFO:Preloading libraries
2025-02-28 12:24:52,096:INFO:Copying training dataset
2025-02-28 12:24:52,096:INFO:Plot type: auc
2025-02-28 12:24:52,241:INFO:Fitting Model
2025-02-28 12:24:52,241:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:24:52,241:INFO:Scoring test/hold-out set
2025-02-28 12:24:52,555:INFO:Visual Rendered Successfully
2025-02-28 12:24:52,693:INFO:plot_model() successfully completed......................................
2025-02-28 12:24:52,710:INFO:Initializing plot_model()
2025-02-28 12:24:52,710:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, system=True)
2025-02-28 12:24:52,710:INFO:Checking exceptions
2025-02-28 12:25:17,277:INFO:PyCaret RegressionExperiment
2025-02-28 12:25:17,278:INFO:Logging name: reg-default-name
2025-02-28 12:25:17,278:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 12:25:17,278:INFO:version 3.3.2
2025-02-28 12:25:17,278:INFO:Initializing setup()
2025-02-28 12:25:17,278:INFO:self.USI: 8123
2025-02-28 12:25:17,278:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'transform_target_param', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:25:17,278:INFO:Checking environment
2025-02-28 12:25:17,278:INFO:python_version: 3.10.16
2025-02-28 12:25:17,278:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:25:17,278:INFO:machine: AMD64
2025-02-28 12:25:17,278:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:25:17,278:INFO:Memory: svmem(total=34200334336, available=7993126912, percent=76.6, used=26207207424, free=7993126912)
2025-02-28 12:25:17,278:INFO:Physical Core: 24
2025-02-28 12:25:17,278:INFO:Logical Core: 32
2025-02-28 12:25:17,279:INFO:Checking libraries
2025-02-28 12:25:17,279:INFO:System:
2025-02-28 12:25:17,279:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:25:17,279:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:25:17,279:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:25:17,279:INFO:PyCaret required dependencies:
2025-02-28 12:25:17,279:INFO:                 pip: 25.0
2025-02-28 12:25:17,279:INFO:          setuptools: 75.8.0
2025-02-28 12:25:17,279:INFO:             pycaret: 3.3.2
2025-02-28 12:25:17,279:INFO:             IPython: 8.30.0
2025-02-28 12:25:17,279:INFO:          ipywidgets: 8.1.5
2025-02-28 12:25:17,279:INFO:                tqdm: 4.67.1
2025-02-28 12:25:17,279:INFO:               numpy: 1.26.4
2025-02-28 12:25:17,279:INFO:              pandas: 2.1.4
2025-02-28 12:25:17,279:INFO:              jinja2: 3.1.5
2025-02-28 12:25:17,280:INFO:               scipy: 1.11.4
2025-02-28 12:25:17,280:INFO:              joblib: 1.3.2
2025-02-28 12:25:17,280:INFO:             sklearn: 1.4.2
2025-02-28 12:25:17,280:INFO:                pyod: 2.0.3
2025-02-28 12:25:17,280:INFO:            imblearn: 0.13.0
2025-02-28 12:25:17,280:INFO:   category_encoders: 2.7.0
2025-02-28 12:25:17,280:INFO:            lightgbm: 4.5.0
2025-02-28 12:25:17,280:INFO:               numba: 0.61.0
2025-02-28 12:25:17,280:INFO:            requests: 2.32.3
2025-02-28 12:25:17,280:INFO:          matplotlib: 3.7.5
2025-02-28 12:25:17,280:INFO:          scikitplot: 0.3.7
2025-02-28 12:25:17,280:INFO:         yellowbrick: 1.5
2025-02-28 12:25:17,280:INFO:              plotly: 5.24.1
2025-02-28 12:25:17,280:INFO:    plotly-resampler: Not installed
2025-02-28 12:25:17,280:INFO:             kaleido: 0.2.1
2025-02-28 12:25:17,280:INFO:           schemdraw: 0.15
2025-02-28 12:25:17,280:INFO:         statsmodels: 0.14.4
2025-02-28 12:25:17,280:INFO:              sktime: 0.26.0
2025-02-28 12:25:17,280:INFO:               tbats: 1.1.3
2025-02-28 12:25:17,280:INFO:            pmdarima: 2.0.4
2025-02-28 12:25:17,280:INFO:              psutil: 5.9.0
2025-02-28 12:25:17,280:INFO:          markupsafe: 2.1.5
2025-02-28 12:25:17,280:INFO:             pickle5: Not installed
2025-02-28 12:25:17,280:INFO:         cloudpickle: 3.1.1
2025-02-28 12:25:17,280:INFO:         deprecation: 2.1.0
2025-02-28 12:25:17,280:INFO:              xxhash: 3.5.0
2025-02-28 12:25:17,280:INFO:           wurlitzer: Not installed
2025-02-28 12:25:17,281:INFO:PyCaret optional dependencies:
2025-02-28 12:25:17,281:INFO:                shap: 0.44.1
2025-02-28 12:25:17,281:INFO:           interpret: 0.6.9
2025-02-28 12:25:17,281:INFO:                umap: 0.5.7
2025-02-28 12:25:17,281:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:25:17,281:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:25:17,281:INFO:             autoviz: Not installed
2025-02-28 12:25:17,281:INFO:           fairlearn: 0.7.0
2025-02-28 12:25:17,281:INFO:          deepchecks: Not installed
2025-02-28 12:25:17,281:INFO:             xgboost: 2.1.4
2025-02-28 12:25:17,281:INFO:            catboost: 1.2.7
2025-02-28 12:25:17,281:INFO:              kmodes: 0.12.2
2025-02-28 12:25:17,281:INFO:             mlxtend: 0.23.4
2025-02-28 12:25:17,281:INFO:       statsforecast: 1.5.0
2025-02-28 12:25:17,281:INFO:        tune_sklearn: Not installed
2025-02-28 12:25:17,281:INFO:                 ray: Not installed
2025-02-28 12:25:17,281:INFO:            hyperopt: 0.2.7
2025-02-28 12:25:17,281:INFO:              optuna: 4.2.0
2025-02-28 12:25:17,281:INFO:               skopt: 0.10.2
2025-02-28 12:25:17,281:INFO:              mlflow: 2.20.1
2025-02-28 12:25:17,281:INFO:              gradio: 5.15.0
2025-02-28 12:25:17,281:INFO:             fastapi: 0.115.8
2025-02-28 12:25:17,281:INFO:             uvicorn: 0.34.0
2025-02-28 12:25:17,281:INFO:              m2cgen: 0.10.0
2025-02-28 12:25:17,281:INFO:           evidently: 0.4.40
2025-02-28 12:25:17,281:INFO:               fugue: 0.8.7
2025-02-28 12:25:17,281:INFO:           streamlit: Not installed
2025-02-28 12:25:17,281:INFO:             prophet: Not installed
2025-02-28 12:25:17,282:INFO:None
2025-02-28 12:25:17,282:INFO:Set up data.
2025-02-28 12:25:17,287:INFO:Set up folding strategy.
2025-02-28 12:25:17,287:INFO:Set up train/test split.
2025-02-28 12:25:17,290:INFO:Set up index.
2025-02-28 12:25:17,290:INFO:Assigning column types.
2025-02-28 12:25:17,292:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:25:17,292:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,294:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,296:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,321:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,340:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,340:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,341:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,341:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,343:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,346:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,371:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,389:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,389:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,390:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,390:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 12:25:17,392:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,395:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,419:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,438:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,438:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,439:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,441:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,442:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,469:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,489:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,489:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,490:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,491:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 12:25:17,494:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,520:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,538:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,538:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,539:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,544:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,569:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,589:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,589:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,590:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,590:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 12:25:17,620:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,639:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,639:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,640:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,669:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,688:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,688:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,689:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,690:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:25:17,720:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,738:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,740:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,769:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,788:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,790:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,790:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 12:25:17,839:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,841:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,890:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,891:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,892:INFO:Preparing preprocessing pipeline...
2025-02-28 12:25:17,892:INFO:Set up simple imputation.
2025-02-28 12:25:17,894:INFO:Set up encoding of ordinal features.
2025-02-28 12:25:17,895:INFO:Set up encoding of categorical features.
2025-02-28 12:25:17,939:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:25:17,947:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score', 'Job_Offers',
                                             'Career_Satisfa...
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study',
                                             'Current_Job_Level'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study',
                                                                    'Current_Job_Level'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:25:17,947:INFO:Creating final display dataframe.
2025-02-28 12:25:18,087:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 19)
4        Transformed data shape        (5000, 30)
5   Transformed train set shape        (3500, 30)
6    Transformed test set shape        (1500, 30)
7              Numeric features                14
8          Categorical features                 4
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              8123
2025-02-28 12:25:18,144:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:18,145:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:18,192:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:18,193:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:18,194:INFO:setup() successfully completed in 0.92s...............
2025-02-28 12:25:18,204:INFO:Initializing compare_models()
2025-02-28 12:25:18,204:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 12:25:18,204:INFO:Checking exceptions
2025-02-28 12:25:18,206:INFO:Preparing display monitor
2025-02-28 12:25:18,219:INFO:Initializing Linear Regression
2025-02-28 12:25:18,220:INFO:Total runtime is 1.6661485036214194e-05 minutes
2025-02-28 12:25:18,222:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:18,222:INFO:Initializing create_model()
2025-02-28 12:25:18,222:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:18,222:INFO:Checking exceptions
2025-02-28 12:25:18,222:INFO:Importing libraries
2025-02-28 12:25:18,222:INFO:Copying training dataset
2025-02-28 12:25:18,224:INFO:Defining folds
2025-02-28 12:25:18,225:INFO:Declaring metric variables
2025-02-28 12:25:18,227:INFO:Importing untrained model
2025-02-28 12:25:18,229:INFO:Linear Regression Imported successfully
2025-02-28 12:25:18,233:INFO:Starting cross validation
2025-02-28 12:25:18,235:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:18,401:INFO:Calculating mean and std
2025-02-28 12:25:18,401:INFO:Creating metrics dataframe
2025-02-28 12:25:18,402:INFO:Uploading results into container
2025-02-28 12:25:18,402:INFO:Uploading model into container now
2025-02-28 12:25:18,402:INFO:_master_model_container: 1
2025-02-28 12:25:18,402:INFO:_display_container: 2
2025-02-28 12:25:18,403:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 12:25:18,403:INFO:create_model() successfully completed......................................
2025-02-28 12:25:18,546:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:18,546:INFO:Creating metrics dataframe
2025-02-28 12:25:18,549:INFO:Initializing Lasso Regression
2025-02-28 12:25:18,549:INFO:Total runtime is 0.005511474609375001 minutes
2025-02-28 12:25:18,552:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:18,552:INFO:Initializing create_model()
2025-02-28 12:25:18,552:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:18,552:INFO:Checking exceptions
2025-02-28 12:25:18,552:INFO:Importing libraries
2025-02-28 12:25:18,552:INFO:Copying training dataset
2025-02-28 12:25:18,556:INFO:Defining folds
2025-02-28 12:25:18,556:INFO:Declaring metric variables
2025-02-28 12:25:18,558:INFO:Importing untrained model
2025-02-28 12:25:18,560:INFO:Lasso Regression Imported successfully
2025-02-28 12:25:18,564:INFO:Starting cross validation
2025-02-28 12:25:18,565:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:18,680:INFO:Calculating mean and std
2025-02-28 12:25:18,680:INFO:Creating metrics dataframe
2025-02-28 12:25:18,681:INFO:Uploading results into container
2025-02-28 12:25:18,681:INFO:Uploading model into container now
2025-02-28 12:25:18,682:INFO:_master_model_container: 2
2025-02-28 12:25:18,682:INFO:_display_container: 2
2025-02-28 12:25:18,682:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 12:25:18,682:INFO:create_model() successfully completed......................................
2025-02-28 12:25:18,822:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:18,822:INFO:Creating metrics dataframe
2025-02-28 12:25:18,827:INFO:Initializing Ridge Regression
2025-02-28 12:25:18,827:INFO:Total runtime is 0.010136560599009196 minutes
2025-02-28 12:25:18,829:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:18,829:INFO:Initializing create_model()
2025-02-28 12:25:18,830:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:18,830:INFO:Checking exceptions
2025-02-28 12:25:18,830:INFO:Importing libraries
2025-02-28 12:25:18,830:INFO:Copying training dataset
2025-02-28 12:25:18,832:INFO:Defining folds
2025-02-28 12:25:18,833:INFO:Declaring metric variables
2025-02-28 12:25:18,834:INFO:Importing untrained model
2025-02-28 12:25:18,836:INFO:Ridge Regression Imported successfully
2025-02-28 12:25:18,840:INFO:Starting cross validation
2025-02-28 12:25:18,841:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:18,946:INFO:Calculating mean and std
2025-02-28 12:25:18,946:INFO:Creating metrics dataframe
2025-02-28 12:25:18,947:INFO:Uploading results into container
2025-02-28 12:25:18,948:INFO:Uploading model into container now
2025-02-28 12:25:18,948:INFO:_master_model_container: 3
2025-02-28 12:25:18,948:INFO:_display_container: 2
2025-02-28 12:25:18,948:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 12:25:18,948:INFO:create_model() successfully completed......................................
2025-02-28 12:25:19,095:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:19,095:INFO:Creating metrics dataframe
2025-02-28 12:25:19,100:INFO:Initializing Elastic Net
2025-02-28 12:25:19,100:INFO:Total runtime is 0.014685277144114176 minutes
2025-02-28 12:25:19,103:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:19,103:INFO:Initializing create_model()
2025-02-28 12:25:19,103:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:19,103:INFO:Checking exceptions
2025-02-28 12:25:19,103:INFO:Importing libraries
2025-02-28 12:25:19,103:INFO:Copying training dataset
2025-02-28 12:25:19,105:INFO:Defining folds
2025-02-28 12:25:19,106:INFO:Declaring metric variables
2025-02-28 12:25:19,108:INFO:Importing untrained model
2025-02-28 12:25:19,111:INFO:Elastic Net Imported successfully
2025-02-28 12:25:19,115:INFO:Starting cross validation
2025-02-28 12:25:19,116:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:19,226:INFO:Calculating mean and std
2025-02-28 12:25:19,226:INFO:Creating metrics dataframe
2025-02-28 12:25:19,228:INFO:Uploading results into container
2025-02-28 12:25:19,228:INFO:Uploading model into container now
2025-02-28 12:25:19,228:INFO:_master_model_container: 4
2025-02-28 12:25:19,228:INFO:_display_container: 2
2025-02-28 12:25:19,228:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 12:25:19,228:INFO:create_model() successfully completed......................................
2025-02-28 12:25:19,370:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:19,370:INFO:Creating metrics dataframe
2025-02-28 12:25:19,374:INFO:Initializing Least Angle Regression
2025-02-28 12:25:19,374:INFO:Total runtime is 0.019255654017130534 minutes
2025-02-28 12:25:19,376:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:19,377:INFO:Initializing create_model()
2025-02-28 12:25:19,377:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:19,377:INFO:Checking exceptions
2025-02-28 12:25:19,377:INFO:Importing libraries
2025-02-28 12:25:19,377:INFO:Copying training dataset
2025-02-28 12:25:19,379:INFO:Defining folds
2025-02-28 12:25:19,379:INFO:Declaring metric variables
2025-02-28 12:25:19,382:INFO:Importing untrained model
2025-02-28 12:25:19,385:INFO:Least Angle Regression Imported successfully
2025-02-28 12:25:19,389:INFO:Starting cross validation
2025-02-28 12:25:19,390:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:19,491:INFO:Calculating mean and std
2025-02-28 12:25:19,492:INFO:Creating metrics dataframe
2025-02-28 12:25:19,493:INFO:Uploading results into container
2025-02-28 12:25:19,493:INFO:Uploading model into container now
2025-02-28 12:25:19,493:INFO:_master_model_container: 5
2025-02-28 12:25:19,493:INFO:_display_container: 2
2025-02-28 12:25:19,493:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 12:25:19,493:INFO:create_model() successfully completed......................................
2025-02-28 12:25:19,634:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:19,634:INFO:Creating metrics dataframe
2025-02-28 12:25:19,638:INFO:Initializing Lasso Least Angle Regression
2025-02-28 12:25:19,638:INFO:Total runtime is 0.02365705172220866 minutes
2025-02-28 12:25:19,640:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:19,640:INFO:Initializing create_model()
2025-02-28 12:25:19,640:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:19,641:INFO:Checking exceptions
2025-02-28 12:25:19,641:INFO:Importing libraries
2025-02-28 12:25:19,641:INFO:Copying training dataset
2025-02-28 12:25:19,644:INFO:Defining folds
2025-02-28 12:25:19,644:INFO:Declaring metric variables
2025-02-28 12:25:19,646:INFO:Importing untrained model
2025-02-28 12:25:19,649:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 12:25:19,653:INFO:Starting cross validation
2025-02-28 12:25:19,654:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:19,757:INFO:Calculating mean and std
2025-02-28 12:25:19,758:INFO:Creating metrics dataframe
2025-02-28 12:25:19,759:INFO:Uploading results into container
2025-02-28 12:25:19,759:INFO:Uploading model into container now
2025-02-28 12:25:19,759:INFO:_master_model_container: 6
2025-02-28 12:25:19,759:INFO:_display_container: 2
2025-02-28 12:25:19,760:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 12:25:19,760:INFO:create_model() successfully completed......................................
2025-02-28 12:25:19,896:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:19,896:INFO:Creating metrics dataframe
2025-02-28 12:25:19,900:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 12:25:19,900:INFO:Total runtime is 0.028023290634155273 minutes
2025-02-28 12:25:19,903:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:19,903:INFO:Initializing create_model()
2025-02-28 12:25:19,903:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:19,903:INFO:Checking exceptions
2025-02-28 12:25:19,903:INFO:Importing libraries
2025-02-28 12:25:19,903:INFO:Copying training dataset
2025-02-28 12:25:19,906:INFO:Defining folds
2025-02-28 12:25:19,906:INFO:Declaring metric variables
2025-02-28 12:25:19,908:INFO:Importing untrained model
2025-02-28 12:25:19,910:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 12:25:19,915:INFO:Starting cross validation
2025-02-28 12:25:19,917:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:20,022:INFO:Calculating mean and std
2025-02-28 12:25:20,023:INFO:Creating metrics dataframe
2025-02-28 12:25:20,024:INFO:Uploading results into container
2025-02-28 12:25:20,025:INFO:Uploading model into container now
2025-02-28 12:25:20,025:INFO:_master_model_container: 7
2025-02-28 12:25:20,025:INFO:_display_container: 2
2025-02-28 12:25:20,025:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 12:25:20,025:INFO:create_model() successfully completed......................................
2025-02-28 12:25:20,161:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:20,162:INFO:Creating metrics dataframe
2025-02-28 12:25:20,166:INFO:Initializing Bayesian Ridge
2025-02-28 12:25:20,166:INFO:Total runtime is 0.03245712916056315 minutes
2025-02-28 12:25:20,168:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:20,168:INFO:Initializing create_model()
2025-02-28 12:25:20,168:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:20,168:INFO:Checking exceptions
2025-02-28 12:25:20,168:INFO:Importing libraries
2025-02-28 12:25:20,168:INFO:Copying training dataset
2025-02-28 12:25:20,172:INFO:Defining folds
2025-02-28 12:25:20,172:INFO:Declaring metric variables
2025-02-28 12:25:20,174:INFO:Importing untrained model
2025-02-28 12:25:20,177:INFO:Bayesian Ridge Imported successfully
2025-02-28 12:25:20,182:INFO:Starting cross validation
2025-02-28 12:25:20,183:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:20,288:INFO:Calculating mean and std
2025-02-28 12:25:20,289:INFO:Creating metrics dataframe
2025-02-28 12:25:20,290:INFO:Uploading results into container
2025-02-28 12:25:20,290:INFO:Uploading model into container now
2025-02-28 12:25:20,290:INFO:_master_model_container: 8
2025-02-28 12:25:20,290:INFO:_display_container: 2
2025-02-28 12:25:20,291:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 12:25:20,291:INFO:create_model() successfully completed......................................
2025-02-28 12:25:20,432:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:20,432:INFO:Creating metrics dataframe
2025-02-28 12:25:20,436:INFO:Initializing Passive Aggressive Regressor
2025-02-28 12:25:20,436:INFO:Total runtime is 0.03695876598358154 minutes
2025-02-28 12:25:20,439:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:20,439:INFO:Initializing create_model()
2025-02-28 12:25:20,439:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:20,439:INFO:Checking exceptions
2025-02-28 12:25:20,439:INFO:Importing libraries
2025-02-28 12:25:20,439:INFO:Copying training dataset
2025-02-28 12:25:20,442:INFO:Defining folds
2025-02-28 12:25:20,442:INFO:Declaring metric variables
2025-02-28 12:25:20,445:INFO:Importing untrained model
2025-02-28 12:25:20,447:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 12:25:20,451:INFO:Starting cross validation
2025-02-28 12:25:20,452:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:20,568:INFO:Calculating mean and std
2025-02-28 12:25:20,569:INFO:Creating metrics dataframe
2025-02-28 12:25:20,570:INFO:Uploading results into container
2025-02-28 12:25:20,570:INFO:Uploading model into container now
2025-02-28 12:25:20,570:INFO:_master_model_container: 9
2025-02-28 12:25:20,570:INFO:_display_container: 2
2025-02-28 12:25:20,571:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:25:20,571:INFO:create_model() successfully completed......................................
2025-02-28 12:25:20,710:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:20,710:INFO:Creating metrics dataframe
2025-02-28 12:25:20,714:INFO:Initializing Huber Regressor
2025-02-28 12:25:20,714:INFO:Total runtime is 0.0415929635365804 minutes
2025-02-28 12:25:20,716:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:20,716:INFO:Initializing create_model()
2025-02-28 12:25:20,716:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:20,716:INFO:Checking exceptions
2025-02-28 12:25:20,716:INFO:Importing libraries
2025-02-28 12:25:20,717:INFO:Copying training dataset
2025-02-28 12:25:20,720:INFO:Defining folds
2025-02-28 12:25:20,720:INFO:Declaring metric variables
2025-02-28 12:25:20,723:INFO:Importing untrained model
2025-02-28 12:25:20,725:INFO:Huber Regressor Imported successfully
2025-02-28 12:25:20,729:INFO:Starting cross validation
2025-02-28 12:25:20,730:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:20,803:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,803:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,809:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,809:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,820:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,826:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,833:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,840:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,845:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,862:INFO:Calculating mean and std
2025-02-28 12:25:20,862:INFO:Creating metrics dataframe
2025-02-28 12:25:20,864:INFO:Uploading results into container
2025-02-28 12:25:20,864:INFO:Uploading model into container now
2025-02-28 12:25:20,864:INFO:_master_model_container: 10
2025-02-28 12:25:20,864:INFO:_display_container: 2
2025-02-28 12:25:20,864:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 12:25:20,864:INFO:create_model() successfully completed......................................
2025-02-28 12:25:21,005:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:21,006:INFO:Creating metrics dataframe
2025-02-28 12:25:21,010:INFO:Initializing K Neighbors Regressor
2025-02-28 12:25:21,010:INFO:Total runtime is 0.046526400248209636 minutes
2025-02-28 12:25:21,012:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:21,013:INFO:Initializing create_model()
2025-02-28 12:25:21,013:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:21,013:INFO:Checking exceptions
2025-02-28 12:25:21,013:INFO:Importing libraries
2025-02-28 12:25:21,013:INFO:Copying training dataset
2025-02-28 12:25:21,017:INFO:Defining folds
2025-02-28 12:25:21,017:INFO:Declaring metric variables
2025-02-28 12:25:21,020:INFO:Importing untrained model
2025-02-28 12:25:21,022:INFO:K Neighbors Regressor Imported successfully
2025-02-28 12:25:21,026:INFO:Starting cross validation
2025-02-28 12:25:21,027:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:21,127:INFO:Calculating mean and std
2025-02-28 12:25:21,127:INFO:Creating metrics dataframe
2025-02-28 12:25:21,129:INFO:Uploading results into container
2025-02-28 12:25:21,129:INFO:Uploading model into container now
2025-02-28 12:25:21,129:INFO:_master_model_container: 11
2025-02-28 12:25:21,130:INFO:_display_container: 2
2025-02-28 12:25:21,130:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 12:25:21,130:INFO:create_model() successfully completed......................................
2025-02-28 12:25:21,268:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:21,268:INFO:Creating metrics dataframe
2025-02-28 12:25:21,272:INFO:Initializing Decision Tree Regressor
2025-02-28 12:25:21,272:INFO:Total runtime is 0.050894276301066084 minutes
2025-02-28 12:25:21,276:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:21,276:INFO:Initializing create_model()
2025-02-28 12:25:21,276:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:21,276:INFO:Checking exceptions
2025-02-28 12:25:21,276:INFO:Importing libraries
2025-02-28 12:25:21,276:INFO:Copying training dataset
2025-02-28 12:25:21,279:INFO:Defining folds
2025-02-28 12:25:21,279:INFO:Declaring metric variables
2025-02-28 12:25:21,282:INFO:Importing untrained model
2025-02-28 12:25:21,284:INFO:Decision Tree Regressor Imported successfully
2025-02-28 12:25:21,288:INFO:Starting cross validation
2025-02-28 12:25:21,289:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:21,408:INFO:Calculating mean and std
2025-02-28 12:25:21,409:INFO:Creating metrics dataframe
2025-02-28 12:25:21,410:INFO:Uploading results into container
2025-02-28 12:25:21,410:INFO:Uploading model into container now
2025-02-28 12:25:21,410:INFO:_master_model_container: 12
2025-02-28 12:25:21,410:INFO:_display_container: 2
2025-02-28 12:25:21,410:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:25:21,411:INFO:create_model() successfully completed......................................
2025-02-28 12:25:21,554:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:21,555:INFO:Creating metrics dataframe
2025-02-28 12:25:21,559:INFO:Initializing Random Forest Regressor
2025-02-28 12:25:21,559:INFO:Total runtime is 0.05567668676376343 minutes
2025-02-28 12:25:21,562:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:21,562:INFO:Initializing create_model()
2025-02-28 12:25:21,562:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:21,562:INFO:Checking exceptions
2025-02-28 12:25:21,562:INFO:Importing libraries
2025-02-28 12:25:21,562:INFO:Copying training dataset
2025-02-28 12:25:21,566:INFO:Defining folds
2025-02-28 12:25:21,567:INFO:Declaring metric variables
2025-02-28 12:25:21,569:INFO:Importing untrained model
2025-02-28 12:25:21,571:INFO:Random Forest Regressor Imported successfully
2025-02-28 12:25:21,575:INFO:Starting cross validation
2025-02-28 12:25:21,576:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:22,628:INFO:Calculating mean and std
2025-02-28 12:25:22,628:INFO:Creating metrics dataframe
2025-02-28 12:25:22,630:INFO:Uploading results into container
2025-02-28 12:25:22,630:INFO:Uploading model into container now
2025-02-28 12:25:22,630:INFO:_master_model_container: 13
2025-02-28 12:25:22,630:INFO:_display_container: 2
2025-02-28 12:25:22,631:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 12:25:22,631:INFO:create_model() successfully completed......................................
2025-02-28 12:25:22,773:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:22,773:INFO:Creating metrics dataframe
2025-02-28 12:25:22,779:INFO:Initializing Extra Trees Regressor
2025-02-28 12:25:22,779:INFO:Total runtime is 0.07600015799204508 minutes
2025-02-28 12:25:22,781:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:22,781:INFO:Initializing create_model()
2025-02-28 12:25:22,781:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:22,781:INFO:Checking exceptions
2025-02-28 12:25:22,781:INFO:Importing libraries
2025-02-28 12:25:22,781:INFO:Copying training dataset
2025-02-28 12:25:22,786:INFO:Defining folds
2025-02-28 12:25:22,786:INFO:Declaring metric variables
2025-02-28 12:25:22,789:INFO:Importing untrained model
2025-02-28 12:25:22,791:INFO:Extra Trees Regressor Imported successfully
2025-02-28 12:25:22,795:INFO:Starting cross validation
2025-02-28 12:25:22,796:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:23,577:INFO:Calculating mean and std
2025-02-28 12:25:23,578:INFO:Creating metrics dataframe
2025-02-28 12:25:23,579:INFO:Uploading results into container
2025-02-28 12:25:23,580:INFO:Uploading model into container now
2025-02-28 12:25:23,580:INFO:_master_model_container: 14
2025-02-28 12:25:23,580:INFO:_display_container: 2
2025-02-28 12:25:23,580:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 12:25:23,580:INFO:create_model() successfully completed......................................
2025-02-28 12:25:23,724:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:23,724:INFO:Creating metrics dataframe
2025-02-28 12:25:23,729:INFO:Initializing AdaBoost Regressor
2025-02-28 12:25:23,729:INFO:Total runtime is 0.09183123111724853 minutes
2025-02-28 12:25:23,731:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:23,732:INFO:Initializing create_model()
2025-02-28 12:25:23,732:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:23,732:INFO:Checking exceptions
2025-02-28 12:25:23,732:INFO:Importing libraries
2025-02-28 12:25:23,732:INFO:Copying training dataset
2025-02-28 12:25:23,735:INFO:Defining folds
2025-02-28 12:25:23,735:INFO:Declaring metric variables
2025-02-28 12:25:23,738:INFO:Importing untrained model
2025-02-28 12:25:23,740:INFO:AdaBoost Regressor Imported successfully
2025-02-28 12:25:23,745:INFO:Starting cross validation
2025-02-28 12:25:23,746:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:23,936:INFO:Calculating mean and std
2025-02-28 12:25:23,937:INFO:Creating metrics dataframe
2025-02-28 12:25:23,938:INFO:Uploading results into container
2025-02-28 12:25:23,938:INFO:Uploading model into container now
2025-02-28 12:25:23,938:INFO:_master_model_container: 15
2025-02-28 12:25:23,938:INFO:_display_container: 2
2025-02-28 12:25:23,938:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 12:25:23,939:INFO:create_model() successfully completed......................................
2025-02-28 12:25:24,077:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:24,077:INFO:Creating metrics dataframe
2025-02-28 12:25:24,083:INFO:Initializing Gradient Boosting Regressor
2025-02-28 12:25:24,083:INFO:Total runtime is 0.0977335770924886 minutes
2025-02-28 12:25:24,085:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:24,085:INFO:Initializing create_model()
2025-02-28 12:25:24,085:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:24,085:INFO:Checking exceptions
2025-02-28 12:25:24,085:INFO:Importing libraries
2025-02-28 12:25:24,085:INFO:Copying training dataset
2025-02-28 12:25:24,089:INFO:Defining folds
2025-02-28 12:25:24,089:INFO:Declaring metric variables
2025-02-28 12:25:24,091:INFO:Importing untrained model
2025-02-28 12:25:24,093:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 12:25:24,099:INFO:Starting cross validation
2025-02-28 12:25:24,100:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:24,613:INFO:Calculating mean and std
2025-02-28 12:25:24,613:INFO:Creating metrics dataframe
2025-02-28 12:25:24,614:INFO:Uploading results into container
2025-02-28 12:25:24,615:INFO:Uploading model into container now
2025-02-28 12:25:24,615:INFO:_master_model_container: 16
2025-02-28 12:25:24,615:INFO:_display_container: 2
2025-02-28 12:25:24,615:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:25:24,615:INFO:create_model() successfully completed......................................
2025-02-28 12:25:24,754:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:24,754:INFO:Creating metrics dataframe
2025-02-28 12:25:24,759:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:25:24,759:INFO:Total runtime is 0.1090067187945048 minutes
2025-02-28 12:25:24,762:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:24,762:INFO:Initializing create_model()
2025-02-28 12:25:24,762:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:24,762:INFO:Checking exceptions
2025-02-28 12:25:24,762:INFO:Importing libraries
2025-02-28 12:25:24,762:INFO:Copying training dataset
2025-02-28 12:25:24,766:INFO:Defining folds
2025-02-28 12:25:24,766:INFO:Declaring metric variables
2025-02-28 12:25:24,769:INFO:Importing untrained model
2025-02-28 12:25:24,771:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:25:24,776:INFO:Starting cross validation
2025-02-28 12:25:24,777:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:25,096:INFO:Calculating mean and std
2025-02-28 12:25:25,096:INFO:Creating metrics dataframe
2025-02-28 12:25:25,098:INFO:Uploading results into container
2025-02-28 12:25:25,098:INFO:Uploading model into container now
2025-02-28 12:25:25,098:INFO:_master_model_container: 17
2025-02-28 12:25:25,098:INFO:_display_container: 2
2025-02-28 12:25:25,099:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:25:25,099:INFO:create_model() successfully completed......................................
2025-02-28 12:25:25,241:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:25,241:INFO:Creating metrics dataframe
2025-02-28 12:25:25,248:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:25:25,248:INFO:Total runtime is 0.11714901129404703 minutes
2025-02-28 12:25:25,250:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:25,250:INFO:Initializing create_model()
2025-02-28 12:25:25,250:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:25,250:INFO:Checking exceptions
2025-02-28 12:25:25,250:INFO:Importing libraries
2025-02-28 12:25:25,250:INFO:Copying training dataset
2025-02-28 12:25:25,253:INFO:Defining folds
2025-02-28 12:25:25,253:INFO:Declaring metric variables
2025-02-28 12:25:25,255:INFO:Importing untrained model
2025-02-28 12:25:25,258:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:25:25,262:INFO:Starting cross validation
2025-02-28 12:25:25,263:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:26,725:INFO:Calculating mean and std
2025-02-28 12:25:26,726:INFO:Creating metrics dataframe
2025-02-28 12:25:26,727:INFO:Uploading results into container
2025-02-28 12:25:26,727:INFO:Uploading model into container now
2025-02-28 12:25:26,728:INFO:_master_model_container: 18
2025-02-28 12:25:26,728:INFO:_display_container: 2
2025-02-28 12:25:26,728:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:25:26,728:INFO:create_model() successfully completed......................................
2025-02-28 12:25:26,888:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:26,888:INFO:Creating metrics dataframe
2025-02-28 12:25:26,894:INFO:Initializing CatBoost Regressor
2025-02-28 12:25:26,894:INFO:Total runtime is 0.1445919076601664 minutes
2025-02-28 12:25:26,896:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:26,896:INFO:Initializing create_model()
2025-02-28 12:25:26,896:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:26,896:INFO:Checking exceptions
2025-02-28 12:25:26,896:INFO:Importing libraries
2025-02-28 12:25:26,896:INFO:Copying training dataset
2025-02-28 12:25:26,900:INFO:Defining folds
2025-02-28 12:25:26,900:INFO:Declaring metric variables
2025-02-28 12:25:26,902:INFO:Importing untrained model
2025-02-28 12:25:26,905:INFO:CatBoost Regressor Imported successfully
2025-02-28 12:25:26,909:INFO:Starting cross validation
2025-02-28 12:25:26,910:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:29,724:INFO:Calculating mean and std
2025-02-28 12:25:29,724:INFO:Creating metrics dataframe
2025-02-28 12:25:29,727:INFO:Uploading results into container
2025-02-28 12:25:29,727:INFO:Uploading model into container now
2025-02-28 12:25:29,727:INFO:_master_model_container: 19
2025-02-28 12:25:29,727:INFO:_display_container: 2
2025-02-28 12:25:29,727:INFO:<catboost.core.CatBoostRegressor object at 0x0000024E5B9DF370>
2025-02-28 12:25:29,727:INFO:create_model() successfully completed......................................
2025-02-28 12:25:29,868:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:29,869:INFO:Creating metrics dataframe
2025-02-28 12:25:29,874:INFO:Initializing Dummy Regressor
2025-02-28 12:25:29,875:INFO:Total runtime is 0.1942762017250061 minutes
2025-02-28 12:25:29,877:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:29,877:INFO:Initializing create_model()
2025-02-28 12:25:29,877:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:29,877:INFO:Checking exceptions
2025-02-28 12:25:29,877:INFO:Importing libraries
2025-02-28 12:25:29,877:INFO:Copying training dataset
2025-02-28 12:25:29,880:INFO:Defining folds
2025-02-28 12:25:29,880:INFO:Declaring metric variables
2025-02-28 12:25:29,883:INFO:Importing untrained model
2025-02-28 12:25:29,885:INFO:Dummy Regressor Imported successfully
2025-02-28 12:25:29,889:INFO:Starting cross validation
2025-02-28 12:25:29,890:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:29,991:INFO:Calculating mean and std
2025-02-28 12:25:29,992:INFO:Creating metrics dataframe
2025-02-28 12:25:29,993:INFO:Uploading results into container
2025-02-28 12:25:29,993:INFO:Uploading model into container now
2025-02-28 12:25:29,993:INFO:_master_model_container: 20
2025-02-28 12:25:29,993:INFO:_display_container: 2
2025-02-28 12:25:29,994:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:25:29,994:INFO:create_model() successfully completed......................................
2025-02-28 12:25:30,134:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:30,134:INFO:Creating metrics dataframe
2025-02-28 12:25:30,140:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:25:30,145:INFO:Initializing create_model()
2025-02-28 12:25:30,145:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:30,145:INFO:Checking exceptions
2025-02-28 12:25:30,146:INFO:Importing libraries
2025-02-28 12:25:30,146:INFO:Copying training dataset
2025-02-28 12:25:30,149:INFO:Defining folds
2025-02-28 12:25:30,149:INFO:Declaring metric variables
2025-02-28 12:25:30,149:INFO:Importing untrained model
2025-02-28 12:25:30,149:INFO:Declaring custom model
2025-02-28 12:25:30,149:INFO:Dummy Regressor Imported successfully
2025-02-28 12:25:30,150:INFO:Cross validation set to False
2025-02-28 12:25:30,150:INFO:Fitting Model
2025-02-28 12:25:30,184:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:25:30,184:INFO:create_model() successfully completed......................................
2025-02-28 12:25:30,344:INFO:_master_model_container: 20
2025-02-28 12:25:30,344:INFO:_display_container: 2
2025-02-28 12:25:30,344:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:25:30,344:INFO:compare_models() successfully completed......................................
2025-02-28 12:25:30,371:INFO:Initializing create_model()
2025-02-28 12:25:30,371:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:30,371:INFO:Checking exceptions
2025-02-28 12:25:30,379:INFO:Importing libraries
2025-02-28 12:25:30,379:INFO:Copying training dataset
2025-02-28 12:25:30,383:INFO:Defining folds
2025-02-28 12:25:30,383:INFO:Declaring metric variables
2025-02-28 12:25:30,385:INFO:Importing untrained model
2025-02-28 12:25:30,388:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:25:30,392:INFO:Starting cross validation
2025-02-28 12:25:30,394:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:30,658:INFO:Calculating mean and std
2025-02-28 12:25:30,658:INFO:Creating metrics dataframe
2025-02-28 12:25:30,661:INFO:Finalizing model
2025-02-28 12:25:30,786:INFO:Uploading results into container
2025-02-28 12:25:30,787:INFO:Uploading model into container now
2025-02-28 12:25:30,792:INFO:_master_model_container: 21
2025-02-28 12:25:30,794:INFO:_display_container: 3
2025-02-28 12:25:30,794:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:25:30,794:INFO:create_model() successfully completed......................................
2025-02-28 12:25:31,036:INFO:Initializing plot_model()
2025-02-28 12:25:31,036:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, system=True)
2025-02-28 12:25:31,036:INFO:Checking exceptions
2025-02-28 12:25:31,039:INFO:Preloading libraries
2025-02-28 12:25:31,047:INFO:Copying training dataset
2025-02-28 12:25:31,047:INFO:Plot type: residuals
2025-02-28 12:25:31,221:INFO:Fitting Model
2025-02-28 12:25:31,253:INFO:Scoring test/hold-out set
2025-02-28 12:25:31,471:INFO:Visual Rendered Successfully
2025-02-28 12:25:31,619:INFO:plot_model() successfully completed......................................
2025-02-28 12:25:31,674:INFO:Initializing plot_model()
2025-02-28 12:25:31,674:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, system=True)
2025-02-28 12:25:31,674:INFO:Checking exceptions
2025-02-28 12:25:31,678:INFO:Preloading libraries
2025-02-28 12:25:31,685:INFO:Copying training dataset
2025-02-28 12:25:31,685:INFO:Plot type: error
2025-02-28 12:25:31,846:INFO:Fitting Model
2025-02-28 12:25:31,846:INFO:Scoring test/hold-out set
2025-02-28 12:25:31,992:INFO:Visual Rendered Successfully
2025-02-28 12:25:32,133:INFO:plot_model() successfully completed......................................
2025-02-28 12:25:32,171:INFO:Initializing plot_model()
2025-02-28 12:25:32,171:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, system=True)
2025-02-28 12:25:32,171:INFO:Checking exceptions
2025-02-28 12:25:32,174:INFO:Preloading libraries
2025-02-28 12:25:32,180:INFO:Copying training dataset
2025-02-28 12:25:32,180:INFO:Plot type: feature
2025-02-28 12:25:32,181:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 12:25:32,312:INFO:Visual Rendered Successfully
2025-02-28 12:25:32,456:INFO:plot_model() successfully completed......................................
2025-02-28 12:42:41,103:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:42:41,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:42:41,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:42:41,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:42:42,488:INFO:PyCaret ClassificationExperiment
2025-02-28 12:42:42,488:INFO:Logging name: clf-default-name
2025-02-28 12:42:42,488:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 12:42:42,488:INFO:version 3.3.2
2025-02-28 12:42:42,488:INFO:Initializing setup()
2025-02-28 12:42:42,488:INFO:self.USI: e0e8
2025-02-28 12:42:42,488:INFO:self._variable_keys: {'gpu_param', 'target_param', 'exp_id', 'data', 'USI', 'gpu_n_jobs_param', 'fix_imbalance', '_ml_usecase', 'y_train', 'pipeline', '_available_plots', 'seed', 'X_train', 'fold_generator', 'is_multiclass', 'logging_param', 'X', 'fold_shuffle_param', 'html_param', 'idx', 'exp_name_log', 'memory', 'y', 'fold_groups_param', 'log_plots_param', 'y_test', 'X_test', 'n_jobs_param'}
2025-02-28 12:42:42,488:INFO:Checking environment
2025-02-28 12:42:42,488:INFO:python_version: 3.10.16
2025-02-28 12:42:42,488:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:42:42,488:INFO:machine: AMD64
2025-02-28 12:42:42,488:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:42:42,488:INFO:Memory: svmem(total=34200334336, available=18888056832, percent=44.8, used=15312277504, free=18888056832)
2025-02-28 12:42:42,488:INFO:Physical Core: 24
2025-02-28 12:42:42,488:INFO:Logical Core: 32
2025-02-28 12:42:42,488:INFO:Checking libraries
2025-02-28 12:42:42,488:INFO:System:
2025-02-28 12:42:42,488:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:42:42,488:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:42:42,488:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:42:42,488:INFO:PyCaret required dependencies:
2025-02-28 12:42:43,017:INFO:                 pip: 25.0
2025-02-28 12:42:43,017:INFO:          setuptools: 75.8.0
2025-02-28 12:42:43,017:INFO:             pycaret: 3.3.2
2025-02-28 12:42:43,017:INFO:             IPython: 8.30.0
2025-02-28 12:42:43,017:INFO:          ipywidgets: 8.1.5
2025-02-28 12:42:43,017:INFO:                tqdm: 4.67.1
2025-02-28 12:42:43,017:INFO:               numpy: 1.26.4
2025-02-28 12:42:43,017:INFO:              pandas: 2.1.4
2025-02-28 12:42:43,017:INFO:              jinja2: 3.1.5
2025-02-28 12:42:43,017:INFO:               scipy: 1.11.4
2025-02-28 12:42:43,017:INFO:              joblib: 1.3.2
2025-02-28 12:42:43,017:INFO:             sklearn: 1.4.2
2025-02-28 12:42:43,017:INFO:                pyod: 2.0.3
2025-02-28 12:42:43,017:INFO:            imblearn: 0.13.0
2025-02-28 12:42:43,017:INFO:   category_encoders: 2.7.0
2025-02-28 12:42:43,017:INFO:            lightgbm: 4.5.0
2025-02-28 12:42:43,017:INFO:               numba: 0.61.0
2025-02-28 12:42:43,017:INFO:            requests: 2.32.3
2025-02-28 12:42:43,017:INFO:          matplotlib: 3.7.5
2025-02-28 12:42:43,017:INFO:          scikitplot: 0.3.7
2025-02-28 12:42:43,017:INFO:         yellowbrick: 1.5
2025-02-28 12:42:43,017:INFO:              plotly: 5.24.1
2025-02-28 12:42:43,017:INFO:    plotly-resampler: Not installed
2025-02-28 12:42:43,017:INFO:             kaleido: 0.2.1
2025-02-28 12:42:43,017:INFO:           schemdraw: 0.15
2025-02-28 12:42:43,017:INFO:         statsmodels: 0.14.4
2025-02-28 12:42:43,017:INFO:              sktime: 0.26.0
2025-02-28 12:42:43,017:INFO:               tbats: 1.1.3
2025-02-28 12:42:43,018:INFO:            pmdarima: 2.0.4
2025-02-28 12:42:43,018:INFO:              psutil: 5.9.0
2025-02-28 12:42:43,018:INFO:          markupsafe: 2.1.5
2025-02-28 12:42:43,018:INFO:             pickle5: Not installed
2025-02-28 12:42:43,018:INFO:         cloudpickle: 3.1.1
2025-02-28 12:42:43,018:INFO:         deprecation: 2.1.0
2025-02-28 12:42:43,018:INFO:              xxhash: 3.5.0
2025-02-28 12:42:43,018:INFO:           wurlitzer: Not installed
2025-02-28 12:42:43,018:INFO:PyCaret optional dependencies:
2025-02-28 12:42:44,785:INFO:                shap: 0.44.1
2025-02-28 12:42:44,785:INFO:           interpret: 0.6.9
2025-02-28 12:42:44,785:INFO:                umap: 0.5.7
2025-02-28 12:42:44,785:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:42:44,785:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:42:44,785:INFO:             autoviz: Not installed
2025-02-28 12:42:44,785:INFO:           fairlearn: 0.7.0
2025-02-28 12:42:44,785:INFO:          deepchecks: Not installed
2025-02-28 12:42:44,785:INFO:             xgboost: 2.1.4
2025-02-28 12:42:44,785:INFO:            catboost: 1.2.7
2025-02-28 12:42:44,785:INFO:              kmodes: 0.12.2
2025-02-28 12:42:44,785:INFO:             mlxtend: 0.23.4
2025-02-28 12:42:44,785:INFO:       statsforecast: 1.5.0
2025-02-28 12:42:44,785:INFO:        tune_sklearn: Not installed
2025-02-28 12:42:44,785:INFO:                 ray: Not installed
2025-02-28 12:42:44,785:INFO:            hyperopt: 0.2.7
2025-02-28 12:42:44,785:INFO:              optuna: 4.2.0
2025-02-28 12:42:44,785:INFO:               skopt: 0.10.2
2025-02-28 12:42:44,785:INFO:              mlflow: 2.20.1
2025-02-28 12:42:44,785:INFO:              gradio: 5.15.0
2025-02-28 12:42:44,785:INFO:             fastapi: 0.115.8
2025-02-28 12:42:44,785:INFO:             uvicorn: 0.34.0
2025-02-28 12:42:44,785:INFO:              m2cgen: 0.10.0
2025-02-28 12:42:44,785:INFO:           evidently: 0.4.40
2025-02-28 12:42:44,785:INFO:               fugue: 0.8.7
2025-02-28 12:42:44,785:INFO:           streamlit: Not installed
2025-02-28 12:42:44,785:INFO:             prophet: Not installed
2025-02-28 12:42:44,785:INFO:None
2025-02-28 12:42:44,785:INFO:Set up data.
2025-02-28 12:42:44,788:INFO:Set up folding strategy.
2025-02-28 12:42:44,788:INFO:Set up train/test split.
2025-02-28 12:42:44,792:INFO:Set up index.
2025-02-28 12:42:44,793:INFO:Assigning column types.
2025-02-28 12:42:44,795:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:42:44,814:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,815:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,833:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,834:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:44,865:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,866:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,879:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,880:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:44,880:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:42:44,901:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,914:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,915:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:44,937:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,948:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,949:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:44,950:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 12:42:44,982:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,984:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:45,016:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:45,018:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:45,019:INFO:Preparing preprocessing pipeline...
2025-02-28 12:42:45,019:INFO:Set up simple imputation.
2025-02-28 12:42:45,021:INFO:Set up encoding of categorical features.
2025-02-28 12:42:45,062:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:42:45,065:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:42:45,065:INFO:Creating final display dataframe.
2025-02-28 12:42:45,168:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              e0e8
2025-02-28 12:42:45,206:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:45,208:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:45,241:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:45,243:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:45,243:INFO:setup() successfully completed in 2.76s...............
2025-02-28 12:42:45,267:INFO:Initializing compare_models()
2025-02-28 12:42:45,267:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 12:42:45,267:INFO:Checking exceptions
2025-02-28 12:42:45,271:INFO:Preparing display monitor
2025-02-28 12:42:45,287:INFO:Initializing Logistic Regression
2025-02-28 12:42:45,287:INFO:Total runtime is 0.0 minutes
2025-02-28 12:42:45,291:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:45,292:INFO:Initializing create_model()
2025-02-28 12:42:45,292:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:45,292:INFO:Checking exceptions
2025-02-28 12:42:45,292:INFO:Importing libraries
2025-02-28 12:42:45,292:INFO:Copying training dataset
2025-02-28 12:42:45,294:INFO:Defining folds
2025-02-28 12:42:45,295:INFO:Declaring metric variables
2025-02-28 12:42:45,297:INFO:Importing untrained model
2025-02-28 12:42:45,299:INFO:Logistic Regression Imported successfully
2025-02-28 12:42:45,303:INFO:Starting cross validation
2025-02-28 12:42:45,304:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:48,749:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,752:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,758:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,767:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,771:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,772:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,773:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,787:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,788:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,791:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,802:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,804:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,812:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,815:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,821:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,837:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,840:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,854:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,861:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,873:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,894:INFO:Calculating mean and std
2025-02-28 12:42:48,895:INFO:Creating metrics dataframe
2025-02-28 12:42:48,895:INFO:Uploading results into container
2025-02-28 12:42:48,897:INFO:Uploading model into container now
2025-02-28 12:42:48,897:INFO:_master_model_container: 1
2025-02-28 12:42:48,897:INFO:_display_container: 2
2025-02-28 12:42:48,898:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 12:42:48,898:INFO:create_model() successfully completed......................................
2025-02-28 12:42:48,973:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:48,973:INFO:Creating metrics dataframe
2025-02-28 12:42:48,977:INFO:Initializing K Neighbors Classifier
2025-02-28 12:42:48,977:INFO:Total runtime is 0.06151277621587117 minutes
2025-02-28 12:42:48,979:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:48,980:INFO:Initializing create_model()
2025-02-28 12:42:48,980:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:48,980:INFO:Checking exceptions
2025-02-28 12:42:48,980:INFO:Importing libraries
2025-02-28 12:42:48,980:INFO:Copying training dataset
2025-02-28 12:42:48,982:INFO:Defining folds
2025-02-28 12:42:48,982:INFO:Declaring metric variables
2025-02-28 12:42:48,984:INFO:Importing untrained model
2025-02-28 12:42:48,986:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:42:48,990:INFO:Starting cross validation
2025-02-28 12:42:48,992:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:51,761:INFO:Calculating mean and std
2025-02-28 12:42:51,762:INFO:Creating metrics dataframe
2025-02-28 12:42:51,764:INFO:Uploading results into container
2025-02-28 12:42:51,765:INFO:Uploading model into container now
2025-02-28 12:42:51,765:INFO:_master_model_container: 2
2025-02-28 12:42:51,765:INFO:_display_container: 2
2025-02-28 12:42:51,765:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:42:51,766:INFO:create_model() successfully completed......................................
2025-02-28 12:42:51,855:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:51,855:INFO:Creating metrics dataframe
2025-02-28 12:42:51,859:INFO:Initializing Naive Bayes
2025-02-28 12:42:51,859:INFO:Total runtime is 0.10954153935114543 minutes
2025-02-28 12:42:51,861:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:51,861:INFO:Initializing create_model()
2025-02-28 12:42:51,861:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:51,861:INFO:Checking exceptions
2025-02-28 12:42:51,862:INFO:Importing libraries
2025-02-28 12:42:51,862:INFO:Copying training dataset
2025-02-28 12:42:51,864:INFO:Defining folds
2025-02-28 12:42:51,864:INFO:Declaring metric variables
2025-02-28 12:42:51,866:INFO:Importing untrained model
2025-02-28 12:42:51,869:INFO:Naive Bayes Imported successfully
2025-02-28 12:42:51,872:INFO:Starting cross validation
2025-02-28 12:42:51,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:54,660:INFO:Calculating mean and std
2025-02-28 12:42:54,661:INFO:Creating metrics dataframe
2025-02-28 12:42:54,663:INFO:Uploading results into container
2025-02-28 12:42:54,664:INFO:Uploading model into container now
2025-02-28 12:42:54,664:INFO:_master_model_container: 3
2025-02-28 12:42:54,664:INFO:_display_container: 2
2025-02-28 12:42:54,665:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 12:42:54,665:INFO:create_model() successfully completed......................................
2025-02-28 12:42:54,745:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:54,745:INFO:Creating metrics dataframe
2025-02-28 12:42:54,748:INFO:Initializing Decision Tree Classifier
2025-02-28 12:42:54,748:INFO:Total runtime is 0.15769506692886354 minutes
2025-02-28 12:42:54,750:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:54,751:INFO:Initializing create_model()
2025-02-28 12:42:54,751:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:54,751:INFO:Checking exceptions
2025-02-28 12:42:54,751:INFO:Importing libraries
2025-02-28 12:42:54,751:INFO:Copying training dataset
2025-02-28 12:42:54,753:INFO:Defining folds
2025-02-28 12:42:54,753:INFO:Declaring metric variables
2025-02-28 12:42:54,755:INFO:Importing untrained model
2025-02-28 12:42:54,758:INFO:Decision Tree Classifier Imported successfully
2025-02-28 12:42:54,763:INFO:Starting cross validation
2025-02-28 12:42:54,763:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:56,785:INFO:Calculating mean and std
2025-02-28 12:42:56,785:INFO:Creating metrics dataframe
2025-02-28 12:42:56,787:INFO:Uploading results into container
2025-02-28 12:42:56,788:INFO:Uploading model into container now
2025-02-28 12:42:56,788:INFO:_master_model_container: 4
2025-02-28 12:42:56,788:INFO:_display_container: 2
2025-02-28 12:42:56,788:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:42:56,788:INFO:create_model() successfully completed......................................
2025-02-28 12:42:56,870:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:56,870:INFO:Creating metrics dataframe
2025-02-28 12:42:56,874:INFO:Initializing SVM - Linear Kernel
2025-02-28 12:42:56,874:INFO:Total runtime is 0.1931254545847575 minutes
2025-02-28 12:42:56,877:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:56,877:INFO:Initializing create_model()
2025-02-28 12:42:56,877:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:56,877:INFO:Checking exceptions
2025-02-28 12:42:56,877:INFO:Importing libraries
2025-02-28 12:42:56,877:INFO:Copying training dataset
2025-02-28 12:42:56,880:INFO:Defining folds
2025-02-28 12:42:56,880:INFO:Declaring metric variables
2025-02-28 12:42:56,882:INFO:Importing untrained model
2025-02-28 12:42:56,885:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 12:42:56,889:INFO:Starting cross validation
2025-02-28 12:42:56,890:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:57,007:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,008:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,010:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,010:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,010:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,011:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,015:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,015:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,021:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,023:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,024:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,025:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,035:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,035:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,037:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,037:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,053:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,053:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,066:INFO:Calculating mean and std
2025-02-28 12:42:57,067:INFO:Creating metrics dataframe
2025-02-28 12:42:57,068:INFO:Uploading results into container
2025-02-28 12:42:57,068:INFO:Uploading model into container now
2025-02-28 12:42:57,069:INFO:_master_model_container: 5
2025-02-28 12:42:57,069:INFO:_display_container: 2
2025-02-28 12:42:57,069:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:42:57,069:INFO:create_model() successfully completed......................................
2025-02-28 12:42:57,145:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:57,146:INFO:Creating metrics dataframe
2025-02-28 12:42:57,149:INFO:Initializing Ridge Classifier
2025-02-28 12:42:57,150:INFO:Total runtime is 0.19771718978881836 minutes
2025-02-28 12:42:57,152:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:57,152:INFO:Initializing create_model()
2025-02-28 12:42:57,152:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:57,152:INFO:Checking exceptions
2025-02-28 12:42:57,152:INFO:Importing libraries
2025-02-28 12:42:57,152:INFO:Copying training dataset
2025-02-28 12:42:57,154:INFO:Defining folds
2025-02-28 12:42:57,155:INFO:Declaring metric variables
2025-02-28 12:42:57,157:INFO:Importing untrained model
2025-02-28 12:42:57,158:INFO:Ridge Classifier Imported successfully
2025-02-28 12:42:57,163:INFO:Starting cross validation
2025-02-28 12:42:57,164:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:57,211:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,212:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,216:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,216:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,219:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,220:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,222:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,226:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,229:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,231:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,238:INFO:Calculating mean and std
2025-02-28 12:42:57,238:INFO:Creating metrics dataframe
2025-02-28 12:42:57,240:INFO:Uploading results into container
2025-02-28 12:42:57,240:INFO:Uploading model into container now
2025-02-28 12:42:57,240:INFO:_master_model_container: 6
2025-02-28 12:42:57,240:INFO:_display_container: 2
2025-02-28 12:42:57,240:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 12:42:57,241:INFO:create_model() successfully completed......................................
2025-02-28 12:42:57,320:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:57,320:INFO:Creating metrics dataframe
2025-02-28 12:42:57,325:INFO:Initializing Random Forest Classifier
2025-02-28 12:42:57,325:INFO:Total runtime is 0.20063705841700236 minutes
2025-02-28 12:42:57,326:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:57,326:INFO:Initializing create_model()
2025-02-28 12:42:57,326:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:57,327:INFO:Checking exceptions
2025-02-28 12:42:57,327:INFO:Importing libraries
2025-02-28 12:42:57,327:INFO:Copying training dataset
2025-02-28 12:42:57,329:INFO:Defining folds
2025-02-28 12:42:57,329:INFO:Declaring metric variables
2025-02-28 12:42:57,331:INFO:Importing untrained model
2025-02-28 12:42:57,333:INFO:Random Forest Classifier Imported successfully
2025-02-28 12:42:57,337:INFO:Starting cross validation
2025-02-28 12:42:57,337:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:57,721:INFO:Calculating mean and std
2025-02-28 12:42:57,722:INFO:Creating metrics dataframe
2025-02-28 12:42:57,723:INFO:Uploading results into container
2025-02-28 12:42:57,723:INFO:Uploading model into container now
2025-02-28 12:42:57,723:INFO:_master_model_container: 7
2025-02-28 12:42:57,723:INFO:_display_container: 2
2025-02-28 12:42:57,724:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 12:42:57,724:INFO:create_model() successfully completed......................................
2025-02-28 12:42:57,799:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:57,799:INFO:Creating metrics dataframe
2025-02-28 12:42:57,804:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 12:42:57,804:INFO:Total runtime is 0.2086216648419698 minutes
2025-02-28 12:42:57,805:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:57,805:INFO:Initializing create_model()
2025-02-28 12:42:57,807:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:57,807:INFO:Checking exceptions
2025-02-28 12:42:57,807:INFO:Importing libraries
2025-02-28 12:42:57,807:INFO:Copying training dataset
2025-02-28 12:42:57,809:INFO:Defining folds
2025-02-28 12:42:57,809:INFO:Declaring metric variables
2025-02-28 12:42:57,811:INFO:Importing untrained model
2025-02-28 12:42:57,813:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 12:42:57,817:INFO:Starting cross validation
2025-02-28 12:42:57,819:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:57,852:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,854:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,858:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,859:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,861:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,863:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,864:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,865:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,865:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,868:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,870:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,873:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,873:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,874:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,874:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,876:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,876:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

at the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,877:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,877:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,878:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,880:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,883:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,885:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,885:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,889:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,907:INFO:Calculating mean and std
2025-02-28 12:42:57,908:INFO:Creating metrics dataframe
2025-02-28 12:42:57,909:INFO:Uploading results into container
2025-02-28 12:42:57,909:INFO:Uploading model into container now
2025-02-28 12:42:57,909:INFO:_master_model_container: 8
2025-02-28 12:42:57,909:INFO:_display_container: 2
2025-02-28 12:42:57,910:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 12:42:57,910:INFO:create_model() successfully completed......................................
2025-02-28 12:42:57,986:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:57,986:INFO:Creating metrics dataframe
2025-02-28 12:42:57,990:INFO:Initializing Ada Boost Classifier
2025-02-28 12:42:57,991:INFO:Total runtime is 0.2117407520612081 minutes
2025-02-28 12:42:57,993:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:57,993:INFO:Initializing create_model()
2025-02-28 12:42:57,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:57,993:INFO:Checking exceptions
2025-02-28 12:42:57,993:INFO:Importing libraries
2025-02-28 12:42:57,993:INFO:Copying training dataset
2025-02-28 12:42:57,995:INFO:Defining folds
2025-02-28 12:42:57,995:INFO:Declaring metric variables
2025-02-28 12:42:57,998:INFO:Importing untrained model
2025-02-28 12:42:58,000:INFO:Ada Boost Classifier Imported successfully
2025-02-28 12:42:58,004:INFO:Starting cross validation
2025-02-28 12:42:58,005:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:58,037:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,039:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,040:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,042:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,043:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,045:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,046:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,054:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,055:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,055:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,175:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,183:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,187:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,195:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,200:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,200:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,201:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,202:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,207:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,208:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,218:INFO:Calculating mean and std
2025-02-28 12:42:58,218:INFO:Creating metrics dataframe
2025-02-28 12:42:58,219:INFO:Uploading results into container
2025-02-28 12:42:58,220:INFO:Uploading model into container now
2025-02-28 12:42:58,220:INFO:_master_model_container: 9
2025-02-28 12:42:58,220:INFO:_display_container: 2
2025-02-28 12:42:58,220:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 12:42:58,221:INFO:create_model() successfully completed......................................
2025-02-28 12:42:58,294:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:58,294:INFO:Creating metrics dataframe
2025-02-28 12:42:58,300:INFO:Initializing Gradient Boosting Classifier
2025-02-28 12:42:58,300:INFO:Total runtime is 0.21688273350397747 minutes
2025-02-28 12:42:58,302:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:58,302:INFO:Initializing create_model()
2025-02-28 12:42:58,302:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:58,302:INFO:Checking exceptions
2025-02-28 12:42:58,302:INFO:Importing libraries
2025-02-28 12:42:58,302:INFO:Copying training dataset
2025-02-28 12:42:58,305:INFO:Defining folds
2025-02-28 12:42:58,305:INFO:Declaring metric variables
2025-02-28 12:42:58,307:INFO:Importing untrained model
2025-02-28 12:42:58,309:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 12:42:58,313:INFO:Starting cross validation
2025-02-28 12:42:58,314:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:00,356:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,434:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,444:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,459:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,472:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,485:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,512:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,527:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,530:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,531:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,549:INFO:Calculating mean and std
2025-02-28 12:43:00,549:INFO:Creating metrics dataframe
2025-02-28 12:43:00,551:INFO:Uploading results into container
2025-02-28 12:43:00,551:INFO:Uploading model into container now
2025-02-28 12:43:00,551:INFO:_master_model_container: 10
2025-02-28 12:43:00,551:INFO:_display_container: 2
2025-02-28 12:43:00,552:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:43:00,552:INFO:create_model() successfully completed......................................
2025-02-28 12:43:00,625:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:00,627:INFO:Creating metrics dataframe
2025-02-28 12:43:00,632:INFO:Initializing Linear Discriminant Analysis
2025-02-28 12:43:00,632:INFO:Total runtime is 0.25574666261672974 minutes
2025-02-28 12:43:00,633:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:00,633:INFO:Initializing create_model()
2025-02-28 12:43:00,633:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:00,634:INFO:Checking exceptions
2025-02-28 12:43:00,634:INFO:Importing libraries
2025-02-28 12:43:00,634:INFO:Copying training dataset
2025-02-28 12:43:00,637:INFO:Defining folds
2025-02-28 12:43:00,637:INFO:Declaring metric variables
2025-02-28 12:43:00,638:INFO:Importing untrained model
2025-02-28 12:43:00,641:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 12:43:00,645:INFO:Starting cross validation
2025-02-28 12:43:00,646:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:00,694:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,695:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,698:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,699:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,704:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,705:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,706:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,708:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,710:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,711:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,722:INFO:Calculating mean and std
2025-02-28 12:43:00,723:INFO:Creating metrics dataframe
2025-02-28 12:43:00,724:INFO:Uploading results into container
2025-02-28 12:43:00,724:INFO:Uploading model into container now
2025-02-28 12:43:00,724:INFO:_master_model_container: 11
2025-02-28 12:43:00,724:INFO:_display_container: 2
2025-02-28 12:43:00,725:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 12:43:00,725:INFO:create_model() successfully completed......................................
2025-02-28 12:43:00,798:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:00,798:INFO:Creating metrics dataframe
2025-02-28 12:43:00,803:INFO:Initializing Extra Trees Classifier
2025-02-28 12:43:00,803:INFO:Total runtime is 0.25859919786453245 minutes
2025-02-28 12:43:00,805:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:00,805:INFO:Initializing create_model()
2025-02-28 12:43:00,805:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:00,805:INFO:Checking exceptions
2025-02-28 12:43:00,805:INFO:Importing libraries
2025-02-28 12:43:00,805:INFO:Copying training dataset
2025-02-28 12:43:00,808:INFO:Defining folds
2025-02-28 12:43:00,808:INFO:Declaring metric variables
2025-02-28 12:43:00,810:INFO:Importing untrained model
2025-02-28 12:43:00,812:INFO:Extra Trees Classifier Imported successfully
2025-02-28 12:43:00,816:INFO:Starting cross validation
2025-02-28 12:43:00,817:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:01,175:INFO:Calculating mean and std
2025-02-28 12:43:01,175:INFO:Creating metrics dataframe
2025-02-28 12:43:01,177:INFO:Uploading results into container
2025-02-28 12:43:01,177:INFO:Uploading model into container now
2025-02-28 12:43:01,178:INFO:_master_model_container: 12
2025-02-28 12:43:01,178:INFO:_display_container: 2
2025-02-28 12:43:01,178:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 12:43:01,178:INFO:create_model() successfully completed......................................
2025-02-28 12:43:01,254:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:01,254:INFO:Creating metrics dataframe
2025-02-28 12:43:01,259:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:43:01,259:INFO:Total runtime is 0.26621123552322384 minutes
2025-02-28 12:43:01,261:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:01,262:INFO:Initializing create_model()
2025-02-28 12:43:01,262:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:01,262:INFO:Checking exceptions
2025-02-28 12:43:01,262:INFO:Importing libraries
2025-02-28 12:43:01,262:INFO:Copying training dataset
2025-02-28 12:43:01,264:INFO:Defining folds
2025-02-28 12:43:01,264:INFO:Declaring metric variables
2025-02-28 12:43:01,266:INFO:Importing untrained model
2025-02-28 12:43:01,269:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:43:01,273:INFO:Starting cross validation
2025-02-28 12:43:01,274:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:02,001:INFO:Calculating mean and std
2025-02-28 12:43:02,002:INFO:Creating metrics dataframe
2025-02-28 12:43:02,003:INFO:Uploading results into container
2025-02-28 12:43:02,003:INFO:Uploading model into container now
2025-02-28 12:43:02,004:INFO:_master_model_container: 13
2025-02-28 12:43:02,004:INFO:_display_container: 2
2025-02-28 12:43:02,004:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 12:43:02,004:INFO:create_model() successfully completed......................................
2025-02-28 12:43:02,079:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:02,079:INFO:Creating metrics dataframe
2025-02-28 12:43:02,084:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:43:02,084:INFO:Total runtime is 0.27995494604110716 minutes
2025-02-28 12:43:02,087:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:02,087:INFO:Initializing create_model()
2025-02-28 12:43:02,087:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:02,087:INFO:Checking exceptions
2025-02-28 12:43:02,087:INFO:Importing libraries
2025-02-28 12:43:02,087:INFO:Copying training dataset
2025-02-28 12:43:02,090:INFO:Defining folds
2025-02-28 12:43:02,090:INFO:Declaring metric variables
2025-02-28 12:43:02,092:INFO:Importing untrained model
2025-02-28 12:43:02,094:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:43:02,097:INFO:Starting cross validation
2025-02-28 12:43:02,098:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:10,135:INFO:Calculating mean and std
2025-02-28 12:43:10,136:INFO:Creating metrics dataframe
2025-02-28 12:43:10,137:INFO:Uploading results into container
2025-02-28 12:43:10,137:INFO:Uploading model into container now
2025-02-28 12:43:10,138:INFO:_master_model_container: 14
2025-02-28 12:43:10,138:INFO:_display_container: 2
2025-02-28 12:43:10,138:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:43:10,138:INFO:create_model() successfully completed......................................
2025-02-28 12:43:10,232:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:10,232:INFO:Creating metrics dataframe
2025-02-28 12:43:10,237:INFO:Initializing CatBoost Classifier
2025-02-28 12:43:10,237:INFO:Total runtime is 0.4158382574717203 minutes
2025-02-28 12:43:10,239:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:10,239:INFO:Initializing create_model()
2025-02-28 12:43:10,239:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:10,239:INFO:Checking exceptions
2025-02-28 12:43:10,239:INFO:Importing libraries
2025-02-28 12:43:10,239:INFO:Copying training dataset
2025-02-28 12:43:10,242:INFO:Defining folds
2025-02-28 12:43:10,242:INFO:Declaring metric variables
2025-02-28 12:43:10,245:INFO:Importing untrained model
2025-02-28 12:43:10,247:INFO:CatBoost Classifier Imported successfully
2025-02-28 12:43:10,250:INFO:Starting cross validation
2025-02-28 12:43:10,251:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:17,197:INFO:Calculating mean and std
2025-02-28 12:43:17,197:INFO:Creating metrics dataframe
2025-02-28 12:43:17,199:INFO:Uploading results into container
2025-02-28 12:43:17,199:INFO:Uploading model into container now
2025-02-28 12:43:17,199:INFO:_master_model_container: 15
2025-02-28 12:43:17,200:INFO:_display_container: 2
2025-02-28 12:43:17,200:INFO:<catboost.core.CatBoostClassifier object at 0x000002188D05E200>
2025-02-28 12:43:17,200:INFO:create_model() successfully completed......................................
2025-02-28 12:43:17,277:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:17,277:INFO:Creating metrics dataframe
2025-02-28 12:43:17,282:INFO:Initializing Dummy Classifier
2025-02-28 12:43:17,283:INFO:Total runtime is 0.5332764546076456 minutes
2025-02-28 12:43:17,285:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:17,285:INFO:Initializing create_model()
2025-02-28 12:43:17,285:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:17,285:INFO:Checking exceptions
2025-02-28 12:43:17,285:INFO:Importing libraries
2025-02-28 12:43:17,285:INFO:Copying training dataset
2025-02-28 12:43:17,288:INFO:Defining folds
2025-02-28 12:43:17,288:INFO:Declaring metric variables
2025-02-28 12:43:17,290:INFO:Importing untrained model
2025-02-28 12:43:17,292:INFO:Dummy Classifier Imported successfully
2025-02-28 12:43:17,296:INFO:Starting cross validation
2025-02-28 12:43:17,297:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:17,344:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,345:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,349:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,350:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,350:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,351:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,353:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,354:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,363:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,365:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,368:INFO:Calculating mean and std
2025-02-28 12:43:17,368:INFO:Creating metrics dataframe
2025-02-28 12:43:17,370:INFO:Uploading results into container
2025-02-28 12:43:17,370:INFO:Uploading model into container now
2025-02-28 12:43:17,370:INFO:_master_model_container: 16
2025-02-28 12:43:17,370:INFO:_display_container: 2
2025-02-28 12:43:17,371:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 12:43:17,371:INFO:create_model() successfully completed......................................
2025-02-28 12:43:17,448:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:17,448:INFO:Creating metrics dataframe
2025-02-28 12:43:17,454:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:43:17,460:INFO:Initializing create_model()
2025-02-28 12:43:17,460:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:17,460:INFO:Checking exceptions
2025-02-28 12:43:17,461:INFO:Importing libraries
2025-02-28 12:43:17,461:INFO:Copying training dataset
2025-02-28 12:43:17,463:INFO:Defining folds
2025-02-28 12:43:17,463:INFO:Declaring metric variables
2025-02-28 12:43:17,463:INFO:Importing untrained model
2025-02-28 12:43:17,463:INFO:Declaring custom model
2025-02-28 12:43:17,464:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:43:17,464:INFO:Cross validation set to False
2025-02-28 12:43:17,464:INFO:Fitting Model
2025-02-28 12:43:17,493:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 12:43:17,493:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000185 seconds.
2025-02-28 12:43:17,494:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-28 12:43:17,494:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 12:43:17,494:INFO:[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 20
2025-02-28 12:43:17,494:INFO:[LightGBM] [Info] Start training from score -1.773639
2025-02-28 12:43:17,494:INFO:[LightGBM] [Info] Start training from score -1.804411
2025-02-28 12:43:17,495:INFO:[LightGBM] [Info] Start training from score -1.765257
2025-02-28 12:43:17,495:INFO:[LightGBM] [Info] Start training from score -1.800944
2025-02-28 12:43:17,495:INFO:[LightGBM] [Info] Start training from score -1.827239
2025-02-28 12:43:17,495:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 12:43:17,857:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:43:17,857:INFO:create_model() successfully completed......................................
2025-02-28 12:43:17,993:INFO:_master_model_container: 16
2025-02-28 12:43:17,993:INFO:_display_container: 2
2025-02-28 12:43:17,994:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:43:17,994:INFO:compare_models() successfully completed......................................
2025-02-28 12:43:18,014:INFO:Initializing plot_model()
2025-02-28 12:43:18,014:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, system=True)
2025-02-28 12:43:18,014:INFO:Checking exceptions
2025-02-28 12:43:18,016:INFO:Preloading libraries
2025-02-28 12:43:18,061:INFO:Copying training dataset
2025-02-28 12:43:18,061:INFO:Plot type: confusion_matrix
2025-02-28 12:43:18,183:INFO:Fitting Model
2025-02-28 12:43:18,184:INFO:Scoring test/hold-out set
2025-02-28 12:43:18,319:INFO:Visual Rendered Successfully
2025-02-28 12:43:18,397:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:18,418:INFO:Initializing plot_model()
2025-02-28 12:43:18,418:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, system=True)
2025-02-28 12:43:18,418:INFO:Checking exceptions
2025-02-28 12:43:18,421:INFO:Preloading libraries
2025-02-28 12:43:18,464:INFO:Copying training dataset
2025-02-28 12:43:18,464:INFO:Plot type: auc
2025-02-28 12:43:18,589:INFO:Fitting Model
2025-02-28 12:43:18,589:INFO:Scoring test/hold-out set
2025-02-28 12:43:18,733:INFO:Visual Rendered Successfully
2025-02-28 12:43:18,812:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:18,845:INFO:PyCaret RegressionExperiment
2025-02-28 12:43:18,845:INFO:Logging name: reg-default-name
2025-02-28 12:43:18,845:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 12:43:18,845:INFO:version 3.3.2
2025-02-28 12:43:18,845:INFO:Initializing setup()
2025-02-28 12:43:18,845:INFO:self.USI: 9222
2025-02-28 12:43:18,845:INFO:self._variable_keys: {'transform_target_param', 'gpu_param', 'target_param', 'exp_id', 'data', 'USI', 'gpu_n_jobs_param', '_ml_usecase', 'y_train', 'pipeline', '_available_plots', 'seed', 'X_train', 'fold_generator', 'logging_param', 'X', 'fold_shuffle_param', 'html_param', 'idx', 'exp_name_log', 'memory', 'y', 'fold_groups_param', 'log_plots_param', 'y_test', 'X_test', 'n_jobs_param'}
2025-02-28 12:43:18,845:INFO:Checking environment
2025-02-28 12:43:18,846:INFO:python_version: 3.10.16
2025-02-28 12:43:18,846:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:43:18,846:INFO:machine: AMD64
2025-02-28 12:43:18,846:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:43:18,846:INFO:Memory: svmem(total=34200334336, available=11794411520, percent=65.5, used=22405922816, free=11794411520)
2025-02-28 12:43:18,846:INFO:Physical Core: 24
2025-02-28 12:43:18,846:INFO:Logical Core: 32
2025-02-28 12:43:18,846:INFO:Checking libraries
2025-02-28 12:43:18,846:INFO:System:
2025-02-28 12:43:18,846:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:43:18,846:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:43:18,846:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:43:18,846:INFO:PyCaret required dependencies:
2025-02-28 12:43:18,846:INFO:                 pip: 25.0
2025-02-28 12:43:18,846:INFO:          setuptools: 75.8.0
2025-02-28 12:43:18,846:INFO:             pycaret: 3.3.2
2025-02-28 12:43:18,846:INFO:             IPython: 8.30.0
2025-02-28 12:43:18,846:INFO:          ipywidgets: 8.1.5
2025-02-28 12:43:18,846:INFO:                tqdm: 4.67.1
2025-02-28 12:43:18,846:INFO:               numpy: 1.26.4
2025-02-28 12:43:18,846:INFO:              pandas: 2.1.4
2025-02-28 12:43:18,847:INFO:              jinja2: 3.1.5
2025-02-28 12:43:18,847:INFO:               scipy: 1.11.4
2025-02-28 12:43:18,847:INFO:              joblib: 1.3.2
2025-02-28 12:43:18,847:INFO:             sklearn: 1.4.2
2025-02-28 12:43:18,847:INFO:                pyod: 2.0.3
2025-02-28 12:43:18,847:INFO:            imblearn: 0.13.0
2025-02-28 12:43:18,847:INFO:   category_encoders: 2.7.0
2025-02-28 12:43:18,847:INFO:            lightgbm: 4.5.0
2025-02-28 12:43:18,847:INFO:               numba: 0.61.0
2025-02-28 12:43:18,847:INFO:            requests: 2.32.3
2025-02-28 12:43:18,847:INFO:          matplotlib: 3.7.5
2025-02-28 12:43:18,847:INFO:          scikitplot: 0.3.7
2025-02-28 12:43:18,847:INFO:         yellowbrick: 1.5
2025-02-28 12:43:18,847:INFO:              plotly: 5.24.1
2025-02-28 12:43:18,847:INFO:    plotly-resampler: Not installed
2025-02-28 12:43:18,847:INFO:             kaleido: 0.2.1
2025-02-28 12:43:18,847:INFO:           schemdraw: 0.15
2025-02-28 12:43:18,847:INFO:         statsmodels: 0.14.4
2025-02-28 12:43:18,847:INFO:              sktime: 0.26.0
2025-02-28 12:43:18,847:INFO:               tbats: 1.1.3
2025-02-28 12:43:18,847:INFO:            pmdarima: 2.0.4
2025-02-28 12:43:18,847:INFO:              psutil: 5.9.0
2025-02-28 12:43:18,847:INFO:          markupsafe: 2.1.5
2025-02-28 12:43:18,847:INFO:             pickle5: Not installed
2025-02-28 12:43:18,847:INFO:         cloudpickle: 3.1.1
2025-02-28 12:43:18,847:INFO:         deprecation: 2.1.0
2025-02-28 12:43:18,847:INFO:              xxhash: 3.5.0
2025-02-28 12:43:18,847:INFO:           wurlitzer: Not installed
2025-02-28 12:43:18,847:INFO:PyCaret optional dependencies:
2025-02-28 12:43:18,847:INFO:                shap: 0.44.1
2025-02-28 12:43:18,848:INFO:           interpret: 0.6.9
2025-02-28 12:43:18,848:INFO:                umap: 0.5.7
2025-02-28 12:43:18,848:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:43:18,848:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:43:18,848:INFO:             autoviz: Not installed
2025-02-28 12:43:18,848:INFO:           fairlearn: 0.7.0
2025-02-28 12:43:18,848:INFO:          deepchecks: Not installed
2025-02-28 12:43:18,848:INFO:             xgboost: 2.1.4
2025-02-28 12:43:18,848:INFO:            catboost: 1.2.7
2025-02-28 12:43:18,848:INFO:              kmodes: 0.12.2
2025-02-28 12:43:18,848:INFO:             mlxtend: 0.23.4
2025-02-28 12:43:18,848:INFO:       statsforecast: 1.5.0
2025-02-28 12:43:18,848:INFO:        tune_sklearn: Not installed
2025-02-28 12:43:18,848:INFO:                 ray: Not installed
2025-02-28 12:43:18,848:INFO:            hyperopt: 0.2.7
2025-02-28 12:43:18,848:INFO:              optuna: 4.2.0
2025-02-28 12:43:18,848:INFO:               skopt: 0.10.2
2025-02-28 12:43:18,848:INFO:              mlflow: 2.20.1
2025-02-28 12:43:18,848:INFO:              gradio: 5.15.0
2025-02-28 12:43:18,848:INFO:             fastapi: 0.115.8
2025-02-28 12:43:18,848:INFO:             uvicorn: 0.34.0
2025-02-28 12:43:18,848:INFO:              m2cgen: 0.10.0
2025-02-28 12:43:18,848:INFO:           evidently: 0.4.40
2025-02-28 12:43:18,848:INFO:               fugue: 0.8.7
2025-02-28 12:43:18,848:INFO:           streamlit: Not installed
2025-02-28 12:43:18,848:INFO:             prophet: Not installed
2025-02-28 12:43:18,848:INFO:None
2025-02-28 12:43:18,848:INFO:Set up data.
2025-02-28 12:43:18,852:INFO:Set up folding strategy.
2025-02-28 12:43:18,852:INFO:Set up train/test split.
2025-02-28 12:43:18,854:INFO:Set up index.
2025-02-28 12:43:18,854:INFO:Assigning column types.
2025-02-28 12:43:18,855:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:43:18,857:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,859:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,861:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,887:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,907:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,907:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:18,909:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:18,909:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,911:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,913:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,939:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,959:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,959:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:18,961:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:18,961:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 12:43:18,963:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,965:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,991:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,011:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,011:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,013:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,015:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,017:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,043:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,062:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,063:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,064:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,064:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 12:43:19,069:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,095:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,114:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,115:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,115:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,121:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,146:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,166:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,166:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,167:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,167:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 12:43:19,197:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,217:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,217:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,219:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,249:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,268:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,269:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,270:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,271:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:43:19,301:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,321:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,322:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,353:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,374:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,375:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,376:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 12:43:19,427:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,428:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,478:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,480:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,480:INFO:Preparing preprocessing pipeline...
2025-02-28 12:43:19,481:INFO:Set up simple imputation.
2025-02-28 12:43:19,482:INFO:Set up encoding of categorical features.
2025-02-28 12:43:19,511:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:43:19,514:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:43:19,514:INFO:Creating final display dataframe.
2025-02-28 12:43:19,612:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              9222
2025-02-28 12:43:19,667:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,668:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,718:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,719:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,720:INFO:setup() successfully completed in 0.88s...............
2025-02-28 12:43:19,745:INFO:Initializing compare_models()
2025-02-28 12:43:19,745:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 12:43:19,745:INFO:Checking exceptions
2025-02-28 12:43:19,747:INFO:Preparing display monitor
2025-02-28 12:43:19,765:INFO:Initializing Linear Regression
2025-02-28 12:43:19,765:INFO:Total runtime is 0.0 minutes
2025-02-28 12:43:19,769:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:19,769:INFO:Initializing create_model()
2025-02-28 12:43:19,769:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:19,769:INFO:Checking exceptions
2025-02-28 12:43:19,769:INFO:Importing libraries
2025-02-28 12:43:19,769:INFO:Copying training dataset
2025-02-28 12:43:19,772:INFO:Defining folds
2025-02-28 12:43:19,772:INFO:Declaring metric variables
2025-02-28 12:43:19,775:INFO:Importing untrained model
2025-02-28 12:43:19,778:INFO:Linear Regression Imported successfully
2025-02-28 12:43:19,783:INFO:Starting cross validation
2025-02-28 12:43:19,784:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:19,875:INFO:Calculating mean and std
2025-02-28 12:43:19,875:INFO:Creating metrics dataframe
2025-02-28 12:43:19,875:INFO:Uploading results into container
2025-02-28 12:43:19,877:INFO:Uploading model into container now
2025-02-28 12:43:19,877:INFO:_master_model_container: 1
2025-02-28 12:43:19,877:INFO:_display_container: 2
2025-02-28 12:43:19,877:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 12:43:19,877:INFO:create_model() successfully completed......................................
2025-02-28 12:43:19,953:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:19,953:INFO:Creating metrics dataframe
2025-02-28 12:43:19,957:INFO:Initializing Lasso Regression
2025-02-28 12:43:19,957:INFO:Total runtime is 0.003201345602671305 minutes
2025-02-28 12:43:19,959:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:19,960:INFO:Initializing create_model()
2025-02-28 12:43:19,960:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:19,960:INFO:Checking exceptions
2025-02-28 12:43:19,960:INFO:Importing libraries
2025-02-28 12:43:19,960:INFO:Copying training dataset
2025-02-28 12:43:19,963:INFO:Defining folds
2025-02-28 12:43:19,963:INFO:Declaring metric variables
2025-02-28 12:43:19,965:INFO:Importing untrained model
2025-02-28 12:43:19,967:INFO:Lasso Regression Imported successfully
2025-02-28 12:43:19,970:INFO:Starting cross validation
2025-02-28 12:43:19,971:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,063:INFO:Calculating mean and std
2025-02-28 12:43:20,063:INFO:Creating metrics dataframe
2025-02-28 12:43:20,064:INFO:Uploading results into container
2025-02-28 12:43:20,064:INFO:Uploading model into container now
2025-02-28 12:43:20,065:INFO:_master_model_container: 2
2025-02-28 12:43:20,065:INFO:_display_container: 2
2025-02-28 12:43:20,065:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 12:43:20,065:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,144:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,144:INFO:Creating metrics dataframe
2025-02-28 12:43:20,148:INFO:Initializing Ridge Regression
2025-02-28 12:43:20,148:INFO:Total runtime is 0.006386613845825196 minutes
2025-02-28 12:43:20,150:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,150:INFO:Initializing create_model()
2025-02-28 12:43:20,150:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,150:INFO:Checking exceptions
2025-02-28 12:43:20,150:INFO:Importing libraries
2025-02-28 12:43:20,150:INFO:Copying training dataset
2025-02-28 12:43:20,152:INFO:Defining folds
2025-02-28 12:43:20,152:INFO:Declaring metric variables
2025-02-28 12:43:20,155:INFO:Importing untrained model
2025-02-28 12:43:20,157:INFO:Ridge Regression Imported successfully
2025-02-28 12:43:20,161:INFO:Starting cross validation
2025-02-28 12:43:20,162:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,249:INFO:Calculating mean and std
2025-02-28 12:43:20,249:INFO:Creating metrics dataframe
2025-02-28 12:43:20,250:INFO:Uploading results into container
2025-02-28 12:43:20,250:INFO:Uploading model into container now
2025-02-28 12:43:20,251:INFO:_master_model_container: 3
2025-02-28 12:43:20,251:INFO:_display_container: 2
2025-02-28 12:43:20,251:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 12:43:20,251:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,329:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,329:INFO:Creating metrics dataframe
2025-02-28 12:43:20,333:INFO:Initializing Elastic Net
2025-02-28 12:43:20,333:INFO:Total runtime is 0.009476137161254884 minutes
2025-02-28 12:43:20,335:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,335:INFO:Initializing create_model()
2025-02-28 12:43:20,335:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,335:INFO:Checking exceptions
2025-02-28 12:43:20,335:INFO:Importing libraries
2025-02-28 12:43:20,335:INFO:Copying training dataset
2025-02-28 12:43:20,338:INFO:Defining folds
2025-02-28 12:43:20,338:INFO:Declaring metric variables
2025-02-28 12:43:20,340:INFO:Importing untrained model
2025-02-28 12:43:20,343:INFO:Elastic Net Imported successfully
2025-02-28 12:43:20,347:INFO:Starting cross validation
2025-02-28 12:43:20,348:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,435:INFO:Calculating mean and std
2025-02-28 12:43:20,435:INFO:Creating metrics dataframe
2025-02-28 12:43:20,436:INFO:Uploading results into container
2025-02-28 12:43:20,436:INFO:Uploading model into container now
2025-02-28 12:43:20,437:INFO:_master_model_container: 4
2025-02-28 12:43:20,437:INFO:_display_container: 2
2025-02-28 12:43:20,437:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 12:43:20,437:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,514:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,514:INFO:Creating metrics dataframe
2025-02-28 12:43:20,518:INFO:Initializing Least Angle Regression
2025-02-28 12:43:20,518:INFO:Total runtime is 0.012556175390879314 minutes
2025-02-28 12:43:20,520:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,520:INFO:Initializing create_model()
2025-02-28 12:43:20,521:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,521:INFO:Checking exceptions
2025-02-28 12:43:20,521:INFO:Importing libraries
2025-02-28 12:43:20,521:INFO:Copying training dataset
2025-02-28 12:43:20,523:INFO:Defining folds
2025-02-28 12:43:20,523:INFO:Declaring metric variables
2025-02-28 12:43:20,525:INFO:Importing untrained model
2025-02-28 12:43:20,528:INFO:Least Angle Regression Imported successfully
2025-02-28 12:43:20,532:INFO:Starting cross validation
2025-02-28 12:43:20,533:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,620:INFO:Calculating mean and std
2025-02-28 12:43:20,620:INFO:Creating metrics dataframe
2025-02-28 12:43:20,621:INFO:Uploading results into container
2025-02-28 12:43:20,621:INFO:Uploading model into container now
2025-02-28 12:43:20,621:INFO:_master_model_container: 5
2025-02-28 12:43:20,622:INFO:_display_container: 2
2025-02-28 12:43:20,622:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 12:43:20,622:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,696:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,696:INFO:Creating metrics dataframe
2025-02-28 12:43:20,700:INFO:Initializing Lasso Least Angle Regression
2025-02-28 12:43:20,700:INFO:Total runtime is 0.015590274333953859 minutes
2025-02-28 12:43:20,702:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,702:INFO:Initializing create_model()
2025-02-28 12:43:20,703:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,703:INFO:Checking exceptions
2025-02-28 12:43:20,703:INFO:Importing libraries
2025-02-28 12:43:20,703:INFO:Copying training dataset
2025-02-28 12:43:20,705:INFO:Defining folds
2025-02-28 12:43:20,705:INFO:Declaring metric variables
2025-02-28 12:43:20,708:INFO:Importing untrained model
2025-02-28 12:43:20,710:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 12:43:20,714:INFO:Starting cross validation
2025-02-28 12:43:20,715:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,791:INFO:Calculating mean and std
2025-02-28 12:43:20,791:INFO:Creating metrics dataframe
2025-02-28 12:43:20,793:INFO:Uploading results into container
2025-02-28 12:43:20,793:INFO:Uploading model into container now
2025-02-28 12:43:20,793:INFO:_master_model_container: 6
2025-02-28 12:43:20,793:INFO:_display_container: 2
2025-02-28 12:43:20,794:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 12:43:20,794:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,884:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,884:INFO:Creating metrics dataframe
2025-02-28 12:43:20,888:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 12:43:20,888:INFO:Total runtime is 0.018726305166880293 minutes
2025-02-28 12:43:20,890:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,890:INFO:Initializing create_model()
2025-02-28 12:43:20,890:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,890:INFO:Checking exceptions
2025-02-28 12:43:20,890:INFO:Importing libraries
2025-02-28 12:43:20,890:INFO:Copying training dataset
2025-02-28 12:43:20,893:INFO:Defining folds
2025-02-28 12:43:20,893:INFO:Declaring metric variables
2025-02-28 12:43:20,895:INFO:Importing untrained model
2025-02-28 12:43:20,897:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 12:43:20,902:INFO:Starting cross validation
2025-02-28 12:43:20,903:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,992:INFO:Calculating mean and std
2025-02-28 12:43:20,992:INFO:Creating metrics dataframe
2025-02-28 12:43:20,993:INFO:Uploading results into container
2025-02-28 12:43:20,993:INFO:Uploading model into container now
2025-02-28 12:43:20,993:INFO:_master_model_container: 7
2025-02-28 12:43:20,993:INFO:_display_container: 2
2025-02-28 12:43:20,994:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 12:43:20,994:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,071:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,071:INFO:Creating metrics dataframe
2025-02-28 12:43:21,076:INFO:Initializing Bayesian Ridge
2025-02-28 12:43:21,077:INFO:Total runtime is 0.021868137518564864 minutes
2025-02-28 12:43:21,079:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,079:INFO:Initializing create_model()
2025-02-28 12:43:21,079:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,079:INFO:Checking exceptions
2025-02-28 12:43:21,079:INFO:Importing libraries
2025-02-28 12:43:21,079:INFO:Copying training dataset
2025-02-28 12:43:21,082:INFO:Defining folds
2025-02-28 12:43:21,082:INFO:Declaring metric variables
2025-02-28 12:43:21,084:INFO:Importing untrained model
2025-02-28 12:43:21,087:INFO:Bayesian Ridge Imported successfully
2025-02-28 12:43:21,091:INFO:Starting cross validation
2025-02-28 12:43:21,092:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,181:INFO:Calculating mean and std
2025-02-28 12:43:21,181:INFO:Creating metrics dataframe
2025-02-28 12:43:21,182:INFO:Uploading results into container
2025-02-28 12:43:21,183:INFO:Uploading model into container now
2025-02-28 12:43:21,183:INFO:_master_model_container: 8
2025-02-28 12:43:21,183:INFO:_display_container: 2
2025-02-28 12:43:21,183:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 12:43:21,184:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,263:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,263:INFO:Creating metrics dataframe
2025-02-28 12:43:21,267:INFO:Initializing Passive Aggressive Regressor
2025-02-28 12:43:21,267:INFO:Total runtime is 0.025045239925384526 minutes
2025-02-28 12:43:21,269:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,269:INFO:Initializing create_model()
2025-02-28 12:43:21,269:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,269:INFO:Checking exceptions
2025-02-28 12:43:21,269:INFO:Importing libraries
2025-02-28 12:43:21,269:INFO:Copying training dataset
2025-02-28 12:43:21,272:INFO:Defining folds
2025-02-28 12:43:21,272:INFO:Declaring metric variables
2025-02-28 12:43:21,274:INFO:Importing untrained model
2025-02-28 12:43:21,276:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 12:43:21,280:INFO:Starting cross validation
2025-02-28 12:43:21,281:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,367:INFO:Calculating mean and std
2025-02-28 12:43:21,368:INFO:Creating metrics dataframe
2025-02-28 12:43:21,369:INFO:Uploading results into container
2025-02-28 12:43:21,369:INFO:Uploading model into container now
2025-02-28 12:43:21,369:INFO:_master_model_container: 9
2025-02-28 12:43:21,369:INFO:_display_container: 2
2025-02-28 12:43:21,370:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:43:21,370:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,447:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,447:INFO:Creating metrics dataframe
2025-02-28 12:43:21,452:INFO:Initializing Huber Regressor
2025-02-28 12:43:21,452:INFO:Total runtime is 0.0281157930692037 minutes
2025-02-28 12:43:21,454:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,455:INFO:Initializing create_model()
2025-02-28 12:43:21,455:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,455:INFO:Checking exceptions
2025-02-28 12:43:21,455:INFO:Importing libraries
2025-02-28 12:43:21,455:INFO:Copying training dataset
2025-02-28 12:43:21,457:INFO:Defining folds
2025-02-28 12:43:21,457:INFO:Declaring metric variables
2025-02-28 12:43:21,459:INFO:Importing untrained model
2025-02-28 12:43:21,461:INFO:Huber Regressor Imported successfully
2025-02-28 12:43:21,465:INFO:Starting cross validation
2025-02-28 12:43:21,467:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,529:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,534:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,535:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,535:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,538:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,538:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,541:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,543:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,552:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,555:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,568:INFO:Calculating mean and std
2025-02-28 12:43:21,569:INFO:Creating metrics dataframe
2025-02-28 12:43:21,570:INFO:Uploading results into container
2025-02-28 12:43:21,570:INFO:Uploading model into container now
2025-02-28 12:43:21,570:INFO:_master_model_container: 10
2025-02-28 12:43:21,571:INFO:_display_container: 2
2025-02-28 12:43:21,571:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 12:43:21,571:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,652:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,652:INFO:Creating metrics dataframe
2025-02-28 12:43:21,657:INFO:Initializing K Neighbors Regressor
2025-02-28 12:43:21,657:INFO:Total runtime is 0.03153393665949504 minutes
2025-02-28 12:43:21,659:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,659:INFO:Initializing create_model()
2025-02-28 12:43:21,659:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,659:INFO:Checking exceptions
2025-02-28 12:43:21,659:INFO:Importing libraries
2025-02-28 12:43:21,659:INFO:Copying training dataset
2025-02-28 12:43:21,662:INFO:Defining folds
2025-02-28 12:43:21,662:INFO:Declaring metric variables
2025-02-28 12:43:21,664:INFO:Importing untrained model
2025-02-28 12:43:21,665:INFO:K Neighbors Regressor Imported successfully
2025-02-28 12:43:21,670:INFO:Starting cross validation
2025-02-28 12:43:21,671:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,758:INFO:Calculating mean and std
2025-02-28 12:43:21,759:INFO:Creating metrics dataframe
2025-02-28 12:43:21,761:INFO:Uploading results into container
2025-02-28 12:43:21,761:INFO:Uploading model into container now
2025-02-28 12:43:21,761:INFO:_master_model_container: 11
2025-02-28 12:43:21,761:INFO:_display_container: 2
2025-02-28 12:43:21,762:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 12:43:21,762:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,841:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,841:INFO:Creating metrics dataframe
2025-02-28 12:43:21,845:INFO:Initializing Decision Tree Regressor
2025-02-28 12:43:21,845:INFO:Total runtime is 0.03467075030008952 minutes
2025-02-28 12:43:21,847:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,847:INFO:Initializing create_model()
2025-02-28 12:43:21,848:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,848:INFO:Checking exceptions
2025-02-28 12:43:21,848:INFO:Importing libraries
2025-02-28 12:43:21,848:INFO:Copying training dataset
2025-02-28 12:43:21,850:INFO:Defining folds
2025-02-28 12:43:21,850:INFO:Declaring metric variables
2025-02-28 12:43:21,853:INFO:Importing untrained model
2025-02-28 12:43:21,855:INFO:Decision Tree Regressor Imported successfully
2025-02-28 12:43:21,858:INFO:Starting cross validation
2025-02-28 12:43:21,859:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,976:INFO:Calculating mean and std
2025-02-28 12:43:21,976:INFO:Creating metrics dataframe
2025-02-28 12:43:21,978:INFO:Uploading results into container
2025-02-28 12:43:21,978:INFO:Uploading model into container now
2025-02-28 12:43:21,978:INFO:_master_model_container: 12
2025-02-28 12:43:21,978:INFO:_display_container: 2
2025-02-28 12:43:21,978:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:43:21,978:INFO:create_model() successfully completed......................................
2025-02-28 12:43:22,057:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:22,057:INFO:Creating metrics dataframe
2025-02-28 12:43:22,062:INFO:Initializing Random Forest Regressor
2025-02-28 12:43:22,062:INFO:Total runtime is 0.03829707304636638 minutes
2025-02-28 12:43:22,064:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:22,064:INFO:Initializing create_model()
2025-02-28 12:43:22,065:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:22,065:INFO:Checking exceptions
2025-02-28 12:43:22,065:INFO:Importing libraries
2025-02-28 12:43:22,065:INFO:Copying training dataset
2025-02-28 12:43:22,067:INFO:Defining folds
2025-02-28 12:43:22,067:INFO:Declaring metric variables
2025-02-28 12:43:22,069:INFO:Importing untrained model
2025-02-28 12:43:22,072:INFO:Random Forest Regressor Imported successfully
2025-02-28 12:43:22,075:INFO:Starting cross validation
2025-02-28 12:43:22,076:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:22,927:INFO:Calculating mean and std
2025-02-28 12:43:22,928:INFO:Creating metrics dataframe
2025-02-28 12:43:22,929:INFO:Uploading results into container
2025-02-28 12:43:22,929:INFO:Uploading model into container now
2025-02-28 12:43:22,929:INFO:_master_model_container: 13
2025-02-28 12:43:22,930:INFO:_display_container: 2
2025-02-28 12:43:22,930:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 12:43:22,930:INFO:create_model() successfully completed......................................
2025-02-28 12:43:23,007:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:23,008:INFO:Creating metrics dataframe
2025-02-28 12:43:23,013:INFO:Initializing Extra Trees Regressor
2025-02-28 12:43:23,013:INFO:Total runtime is 0.05413374106089275 minutes
2025-02-28 12:43:23,015:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:23,015:INFO:Initializing create_model()
2025-02-28 12:43:23,015:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:23,015:INFO:Checking exceptions
2025-02-28 12:43:23,015:INFO:Importing libraries
2025-02-28 12:43:23,015:INFO:Copying training dataset
2025-02-28 12:43:23,017:INFO:Defining folds
2025-02-28 12:43:23,017:INFO:Declaring metric variables
2025-02-28 12:43:23,019:INFO:Importing untrained model
2025-02-28 12:43:23,021:INFO:Extra Trees Regressor Imported successfully
2025-02-28 12:43:23,025:INFO:Starting cross validation
2025-02-28 12:43:23,027:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:23,580:INFO:Calculating mean and std
2025-02-28 12:43:23,581:INFO:Creating metrics dataframe
2025-02-28 12:43:23,582:INFO:Uploading results into container
2025-02-28 12:43:23,582:INFO:Uploading model into container now
2025-02-28 12:43:23,582:INFO:_master_model_container: 14
2025-02-28 12:43:23,583:INFO:_display_container: 2
2025-02-28 12:43:23,583:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 12:43:23,583:INFO:create_model() successfully completed......................................
2025-02-28 12:43:23,667:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:23,667:INFO:Creating metrics dataframe
2025-02-28 12:43:23,673:INFO:Initializing AdaBoost Regressor
2025-02-28 12:43:23,673:INFO:Total runtime is 0.06514684359232585 minutes
2025-02-28 12:43:23,676:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:23,676:INFO:Initializing create_model()
2025-02-28 12:43:23,676:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:23,677:INFO:Checking exceptions
2025-02-28 12:43:23,677:INFO:Importing libraries
2025-02-28 12:43:23,677:INFO:Copying training dataset
2025-02-28 12:43:23,679:INFO:Defining folds
2025-02-28 12:43:23,679:INFO:Declaring metric variables
2025-02-28 12:43:23,681:INFO:Importing untrained model
2025-02-28 12:43:23,683:INFO:AdaBoost Regressor Imported successfully
2025-02-28 12:43:23,688:INFO:Starting cross validation
2025-02-28 12:43:23,688:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:23,861:INFO:Calculating mean and std
2025-02-28 12:43:23,861:INFO:Creating metrics dataframe
2025-02-28 12:43:23,862:INFO:Uploading results into container
2025-02-28 12:43:23,863:INFO:Uploading model into container now
2025-02-28 12:43:23,863:INFO:_master_model_container: 15
2025-02-28 12:43:23,863:INFO:_display_container: 2
2025-02-28 12:43:23,863:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 12:43:23,863:INFO:create_model() successfully completed......................................
2025-02-28 12:43:23,939:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:23,939:INFO:Creating metrics dataframe
2025-02-28 12:43:23,944:INFO:Initializing Gradient Boosting Regressor
2025-02-28 12:43:23,945:INFO:Total runtime is 0.06968005100886028 minutes
2025-02-28 12:43:23,947:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:23,947:INFO:Initializing create_model()
2025-02-28 12:43:23,948:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:23,948:INFO:Checking exceptions
2025-02-28 12:43:23,948:INFO:Importing libraries
2025-02-28 12:43:23,948:INFO:Copying training dataset
2025-02-28 12:43:23,950:INFO:Defining folds
2025-02-28 12:43:23,950:INFO:Declaring metric variables
2025-02-28 12:43:23,952:INFO:Importing untrained model
2025-02-28 12:43:23,954:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 12:43:23,958:INFO:Starting cross validation
2025-02-28 12:43:23,959:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:24,372:INFO:Calculating mean and std
2025-02-28 12:43:24,373:INFO:Creating metrics dataframe
2025-02-28 12:43:24,374:INFO:Uploading results into container
2025-02-28 12:43:24,374:INFO:Uploading model into container now
2025-02-28 12:43:24,374:INFO:_master_model_container: 16
2025-02-28 12:43:24,374:INFO:_display_container: 2
2025-02-28 12:43:24,375:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:43:24,375:INFO:create_model() successfully completed......................................
2025-02-28 12:43:24,453:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:24,453:INFO:Creating metrics dataframe
2025-02-28 12:43:24,458:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:43:24,458:INFO:Total runtime is 0.07822784185409547 minutes
2025-02-28 12:43:24,460:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:24,461:INFO:Initializing create_model()
2025-02-28 12:43:24,461:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:24,461:INFO:Checking exceptions
2025-02-28 12:43:24,461:INFO:Importing libraries
2025-02-28 12:43:24,461:INFO:Copying training dataset
2025-02-28 12:43:24,463:INFO:Defining folds
2025-02-28 12:43:24,463:INFO:Declaring metric variables
2025-02-28 12:43:24,465:INFO:Importing untrained model
2025-02-28 12:43:24,468:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:43:24,472:INFO:Starting cross validation
2025-02-28 12:43:24,473:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:24,963:INFO:Calculating mean and std
2025-02-28 12:43:24,964:INFO:Creating metrics dataframe
2025-02-28 12:43:24,965:INFO:Uploading results into container
2025-02-28 12:43:24,965:INFO:Uploading model into container now
2025-02-28 12:43:24,965:INFO:_master_model_container: 17
2025-02-28 12:43:24,965:INFO:_display_container: 2
2025-02-28 12:43:24,966:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:43:24,966:INFO:create_model() successfully completed......................................
2025-02-28 12:43:25,042:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:25,042:INFO:Creating metrics dataframe
2025-02-28 12:43:25,048:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:43:25,048:INFO:Total runtime is 0.08806429306666057 minutes
2025-02-28 12:43:25,050:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:25,050:INFO:Initializing create_model()
2025-02-28 12:43:25,050:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:25,050:INFO:Checking exceptions
2025-02-28 12:43:25,050:INFO:Importing libraries
2025-02-28 12:43:25,050:INFO:Copying training dataset
2025-02-28 12:43:25,053:INFO:Defining folds
2025-02-28 12:43:25,053:INFO:Declaring metric variables
2025-02-28 12:43:25,055:INFO:Importing untrained model
2025-02-28 12:43:25,058:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:43:25,062:INFO:Starting cross validation
2025-02-28 12:43:25,063:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:26,432:INFO:Calculating mean and std
2025-02-28 12:43:26,433:INFO:Creating metrics dataframe
2025-02-28 12:43:26,434:INFO:Uploading results into container
2025-02-28 12:43:26,435:INFO:Uploading model into container now
2025-02-28 12:43:26,435:INFO:_master_model_container: 18
2025-02-28 12:43:26,435:INFO:_display_container: 2
2025-02-28 12:43:26,435:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:43:26,435:INFO:create_model() successfully completed......................................
2025-02-28 12:43:26,531:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:26,531:INFO:Creating metrics dataframe
2025-02-28 12:43:26,537:INFO:Initializing CatBoost Regressor
2025-02-28 12:43:26,537:INFO:Total runtime is 0.11286828517913819 minutes
2025-02-28 12:43:26,539:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:26,540:INFO:Initializing create_model()
2025-02-28 12:43:26,540:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:26,540:INFO:Checking exceptions
2025-02-28 12:43:26,540:INFO:Importing libraries
2025-02-28 12:43:26,540:INFO:Copying training dataset
2025-02-28 12:43:26,543:INFO:Defining folds
2025-02-28 12:43:26,543:INFO:Declaring metric variables
2025-02-28 12:43:26,546:INFO:Importing untrained model
2025-02-28 12:43:26,548:INFO:CatBoost Regressor Imported successfully
2025-02-28 12:43:26,553:INFO:Starting cross validation
2025-02-28 12:43:26,554:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:29,176:INFO:Calculating mean and std
2025-02-28 12:43:29,177:INFO:Creating metrics dataframe
2025-02-28 12:43:29,178:INFO:Uploading results into container
2025-02-28 12:43:29,178:INFO:Uploading model into container now
2025-02-28 12:43:29,178:INFO:_master_model_container: 19
2025-02-28 12:43:29,178:INFO:_display_container: 2
2025-02-28 12:43:29,178:INFO:<catboost.core.CatBoostRegressor object at 0x000002188D17B1F0>
2025-02-28 12:43:29,179:INFO:create_model() successfully completed......................................
2025-02-28 12:43:29,258:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:29,258:INFO:Creating metrics dataframe
2025-02-28 12:43:29,264:INFO:Initializing Dummy Regressor
2025-02-28 12:43:29,264:INFO:Total runtime is 0.1583217223485311 minutes
2025-02-28 12:43:29,266:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:29,266:INFO:Initializing create_model()
2025-02-28 12:43:29,266:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:29,266:INFO:Checking exceptions
2025-02-28 12:43:29,266:INFO:Importing libraries
2025-02-28 12:43:29,266:INFO:Copying training dataset
2025-02-28 12:43:29,269:INFO:Defining folds
2025-02-28 12:43:29,269:INFO:Declaring metric variables
2025-02-28 12:43:29,271:INFO:Importing untrained model
2025-02-28 12:43:29,274:INFO:Dummy Regressor Imported successfully
2025-02-28 12:43:29,279:INFO:Starting cross validation
2025-02-28 12:43:29,280:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:29,363:INFO:Calculating mean and std
2025-02-28 12:43:29,364:INFO:Creating metrics dataframe
2025-02-28 12:43:29,365:INFO:Uploading results into container
2025-02-28 12:43:29,365:INFO:Uploading model into container now
2025-02-28 12:43:29,365:INFO:_master_model_container: 20
2025-02-28 12:43:29,365:INFO:_display_container: 2
2025-02-28 12:43:29,366:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:43:29,366:INFO:create_model() successfully completed......................................
2025-02-28 12:43:29,451:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:29,451:INFO:Creating metrics dataframe
2025-02-28 12:43:29,458:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:43:29,462:INFO:Initializing create_model()
2025-02-28 12:43:29,462:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:29,462:INFO:Checking exceptions
2025-02-28 12:43:29,463:INFO:Importing libraries
2025-02-28 12:43:29,463:INFO:Copying training dataset
2025-02-28 12:43:29,465:INFO:Defining folds
2025-02-28 12:43:29,465:INFO:Declaring metric variables
2025-02-28 12:43:29,465:INFO:Importing untrained model
2025-02-28 12:43:29,465:INFO:Declaring custom model
2025-02-28 12:43:29,465:INFO:Dummy Regressor Imported successfully
2025-02-28 12:43:29,467:INFO:Cross validation set to False
2025-02-28 12:43:29,467:INFO:Fitting Model
2025-02-28 12:43:29,490:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:43:29,490:INFO:create_model() successfully completed......................................
2025-02-28 12:43:29,589:INFO:_master_model_container: 20
2025-02-28 12:43:29,589:INFO:_display_container: 2
2025-02-28 12:43:29,589:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:43:29,589:INFO:compare_models() successfully completed......................................
2025-02-28 12:43:29,629:INFO:Initializing create_model()
2025-02-28 12:43:29,629:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:29,629:INFO:Checking exceptions
2025-02-28 12:43:29,638:INFO:Importing libraries
2025-02-28 12:43:29,638:INFO:Copying training dataset
2025-02-28 12:43:29,641:INFO:Defining folds
2025-02-28 12:43:29,641:INFO:Declaring metric variables
2025-02-28 12:43:29,643:INFO:Importing untrained model
2025-02-28 12:43:29,645:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:43:29,650:INFO:Starting cross validation
2025-02-28 12:43:29,651:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:30,125:INFO:Calculating mean and std
2025-02-28 12:43:30,125:INFO:Creating metrics dataframe
2025-02-28 12:43:30,128:INFO:Finalizing model
2025-02-28 12:43:30,295:INFO:Uploading results into container
2025-02-28 12:43:30,296:INFO:Uploading model into container now
2025-02-28 12:43:30,303:INFO:_master_model_container: 21
2025-02-28 12:43:30,303:INFO:_display_container: 3
2025-02-28 12:43:30,303:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:43:30,304:INFO:create_model() successfully completed......................................
2025-02-28 12:43:30,414:INFO:Initializing plot_model()
2025-02-28 12:43:30,414:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:43:30,414:INFO:Checking exceptions
2025-02-28 12:43:30,418:INFO:Preloading libraries
2025-02-28 12:43:30,423:INFO:Copying training dataset
2025-02-28 12:43:30,423:INFO:Plot type: residuals
2025-02-28 12:43:30,581:INFO:Fitting Model
2025-02-28 12:43:30,627:INFO:Scoring test/hold-out set
2025-02-28 12:43:30,930:INFO:Visual Rendered Successfully
2025-02-28 12:43:31,014:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:31,039:INFO:Initializing plot_model()
2025-02-28 12:43:31,039:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:43:31,039:INFO:Checking exceptions
2025-02-28 12:43:31,042:INFO:Preloading libraries
2025-02-28 12:43:31,047:INFO:Copying training dataset
2025-02-28 12:43:31,047:INFO:Plot type: error
2025-02-28 12:43:31,181:INFO:Fitting Model
2025-02-28 12:43:31,182:INFO:Scoring test/hold-out set
2025-02-28 12:43:31,343:INFO:Visual Rendered Successfully
2025-02-28 12:43:31,430:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:31,447:INFO:Initializing plot_model()
2025-02-28 12:43:31,447:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:43:31,447:INFO:Checking exceptions
2025-02-28 12:43:31,450:INFO:Preloading libraries
2025-02-28 12:43:31,454:INFO:Copying training dataset
2025-02-28 12:43:31,454:INFO:Plot type: feature
2025-02-28 12:43:31,455:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 12:43:31,585:INFO:Visual Rendered Successfully
2025-02-28 12:43:31,674:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:34,208:WARNING:C:\Users\dagir\AppData\Local\Temp\ipykernel_24096\1216279964.py:849: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  import pkg_resources

2025-02-28 12:43:54,140:INFO:Initializing create_model()
2025-02-28 12:43:54,140:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=Dummy Regressor, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:54,140:INFO:Checking exceptions
2025-02-28 12:44:15,140:INFO:Initializing create_model()
2025-02-28 12:44:15,141:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=dummy, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:44:15,141:INFO:Checking exceptions
2025-02-28 12:44:15,150:INFO:Importing libraries
2025-02-28 12:44:15,150:INFO:Copying training dataset
2025-02-28 12:44:15,153:INFO:Defining folds
2025-02-28 12:44:15,153:INFO:Declaring metric variables
2025-02-28 12:44:15,155:INFO:Importing untrained model
2025-02-28 12:44:15,157:INFO:Dummy Regressor Imported successfully
2025-02-28 12:44:15,160:INFO:Starting cross validation
2025-02-28 12:44:15,161:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:44:15,247:INFO:Calculating mean and std
2025-02-28 12:44:15,247:INFO:Creating metrics dataframe
2025-02-28 12:44:15,250:INFO:Finalizing model
2025-02-28 12:44:15,275:INFO:Uploading results into container
2025-02-28 12:44:15,275:INFO:Uploading model into container now
2025-02-28 12:44:15,281:INFO:_master_model_container: 22
2025-02-28 12:44:15,281:INFO:_display_container: 4
2025-02-28 12:44:15,281:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:44:15,281:INFO:create_model() successfully completed......................................
2025-02-28 12:44:17,652:INFO:Initializing plot_model()
2025-02-28 12:44:17,652:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:17,652:INFO:Checking exceptions
2025-02-28 12:44:17,655:INFO:Preloading libraries
2025-02-28 12:44:17,655:INFO:Copying training dataset
2025-02-28 12:44:17,655:INFO:Plot type: residuals
2025-02-28 12:44:17,766:INFO:Fitting Model
2025-02-28 12:44:17,782:INFO:Scoring test/hold-out set
2025-02-28 12:44:17,965:INFO:Visual Rendered Successfully
2025-02-28 12:44:18,074:INFO:plot_model() successfully completed......................................
2025-02-28 12:44:20,454:INFO:Initializing plot_model()
2025-02-28 12:44:20,454:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:20,454:INFO:Checking exceptions
2025-02-28 12:44:20,456:INFO:Preloading libraries
2025-02-28 12:44:20,456:INFO:Copying training dataset
2025-02-28 12:44:20,457:INFO:Plot type: error
2025-02-28 12:44:20,553:INFO:Fitting Model
2025-02-28 12:44:20,553:INFO:Scoring test/hold-out set
2025-02-28 12:44:20,661:INFO:Visual Rendered Successfully
2025-02-28 12:44:20,771:INFO:plot_model() successfully completed......................................
2025-02-28 12:44:22,542:INFO:Initializing plot_model()
2025-02-28 12:44:22,542:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:22,542:INFO:Checking exceptions
2025-02-28 12:44:41,286:INFO:Initializing create_model()
2025-02-28 12:44:41,287:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=omp, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:44:41,287:INFO:Checking exceptions
2025-02-28 12:44:41,294:INFO:Importing libraries
2025-02-28 12:44:41,295:INFO:Copying training dataset
2025-02-28 12:44:41,297:INFO:Defining folds
2025-02-28 12:44:41,297:INFO:Declaring metric variables
2025-02-28 12:44:41,299:INFO:Importing untrained model
2025-02-28 12:44:41,301:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 12:44:41,305:INFO:Starting cross validation
2025-02-28 12:44:41,307:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:44:41,405:INFO:Calculating mean and std
2025-02-28 12:44:41,405:INFO:Creating metrics dataframe
2025-02-28 12:44:41,408:INFO:Finalizing model
2025-02-28 12:44:41,433:INFO:Uploading results into container
2025-02-28 12:44:41,434:INFO:Uploading model into container now
2025-02-28 12:44:41,438:INFO:_master_model_container: 23
2025-02-28 12:44:41,438:INFO:_display_container: 5
2025-02-28 12:44:41,438:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 12:44:41,438:INFO:create_model() successfully completed......................................
2025-02-28 12:44:42,335:INFO:Initializing plot_model()
2025-02-28 12:44:42,335:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:42,335:INFO:Checking exceptions
2025-02-28 12:44:42,338:INFO:Preloading libraries
2025-02-28 12:44:42,338:INFO:Copying training dataset
2025-02-28 12:44:42,338:INFO:Plot type: residuals
2025-02-28 12:44:42,449:INFO:Fitting Model
2025-02-28 12:44:42,449:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 12:44:42,467:INFO:Scoring test/hold-out set
2025-02-28 12:44:42,716:INFO:Visual Rendered Successfully
2025-02-28 12:44:42,835:INFO:plot_model() successfully completed......................................
2025-02-28 12:44:44,334:INFO:Initializing plot_model()
2025-02-28 12:44:44,334:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:44,334:INFO:Checking exceptions
2025-02-28 12:44:44,337:INFO:Preloading libraries
2025-02-28 12:44:44,337:INFO:Copying training dataset
2025-02-28 12:44:44,337:INFO:Plot type: error
2025-02-28 12:44:44,433:INFO:Fitting Model
2025-02-28 12:44:44,433:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 12:44:44,433:INFO:Scoring test/hold-out set
2025-02-28 12:44:44,547:INFO:Visual Rendered Successfully
2025-02-28 12:44:44,654:INFO:plot_model() successfully completed......................................
2025-02-28 12:44:45,510:INFO:Initializing plot_model()
2025-02-28 12:44:45,510:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:45,510:INFO:Checking exceptions
2025-02-28 12:44:45,512:INFO:Preloading libraries
2025-02-28 12:44:45,512:INFO:Copying training dataset
2025-02-28 12:44:45,512:INFO:Plot type: feature
2025-02-28 12:44:45,640:INFO:Visual Rendered Successfully
2025-02-28 12:44:45,755:INFO:plot_model() successfully completed......................................
2025-02-28 13:00:12,662:INFO:PyCaret ClassificationExperiment
2025-02-28 13:00:12,663:INFO:Logging name: clf-default-name
2025-02-28 13:00:12,663:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 13:00:12,663:INFO:version 3.3.2
2025-02-28 13:00:12,663:INFO:Initializing setup()
2025-02-28 13:00:12,663:INFO:self.USI: cc4d
2025-02-28 13:00:12,664:INFO:self._variable_keys: {'gpu_param', 'target_param', 'exp_id', 'data', 'USI', 'gpu_n_jobs_param', 'fix_imbalance', '_ml_usecase', 'y_train', 'pipeline', '_available_plots', 'seed', 'X_train', 'fold_generator', 'is_multiclass', 'logging_param', 'X', 'fold_shuffle_param', 'html_param', 'idx', 'exp_name_log', 'memory', 'y', 'fold_groups_param', 'log_plots_param', 'y_test', 'X_test', 'n_jobs_param'}
2025-02-28 13:00:12,664:INFO:Checking environment
2025-02-28 13:00:12,664:INFO:python_version: 3.10.16
2025-02-28 13:00:12,664:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 13:00:12,664:INFO:machine: AMD64
2025-02-28 13:00:12,664:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 13:00:12,664:INFO:Memory: svmem(total=34200334336, available=18904117248, percent=44.7, used=15296217088, free=18904117248)
2025-02-28 13:00:12,664:INFO:Physical Core: 24
2025-02-28 13:00:12,664:INFO:Logical Core: 32
2025-02-28 13:00:12,664:INFO:Checking libraries
2025-02-28 13:00:12,664:INFO:System:
2025-02-28 13:00:12,664:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 13:00:12,665:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 13:00:12,665:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 13:00:12,665:INFO:PyCaret required dependencies:
2025-02-28 13:00:12,665:INFO:                 pip: 25.0
2025-02-28 13:00:12,665:INFO:          setuptools: 75.8.0
2025-02-28 13:00:12,665:INFO:             pycaret: 3.3.2
2025-02-28 13:00:12,665:INFO:             IPython: 8.30.0
2025-02-28 13:00:12,665:INFO:          ipywidgets: 8.1.5
2025-02-28 13:00:12,665:INFO:                tqdm: 4.67.1
2025-02-28 13:00:12,665:INFO:               numpy: 1.26.4
2025-02-28 13:00:12,665:INFO:              pandas: 2.1.4
2025-02-28 13:00:12,665:INFO:              jinja2: 3.1.5
2025-02-28 13:00:12,665:INFO:               scipy: 1.11.4
2025-02-28 13:00:12,665:INFO:              joblib: 1.3.2
2025-02-28 13:00:12,665:INFO:             sklearn: 1.4.2
2025-02-28 13:00:12,665:INFO:                pyod: 2.0.3
2025-02-28 13:00:12,665:INFO:            imblearn: 0.13.0
2025-02-28 13:00:12,665:INFO:   category_encoders: 2.7.0
2025-02-28 13:00:12,665:INFO:            lightgbm: 4.5.0
2025-02-28 13:00:12,665:INFO:               numba: 0.61.0
2025-02-28 13:00:12,665:INFO:            requests: 2.32.3
2025-02-28 13:00:12,666:INFO:          matplotlib: 3.7.5
2025-02-28 13:00:12,666:INFO:          scikitplot: 0.3.7
2025-02-28 13:00:12,666:INFO:         yellowbrick: 1.5
2025-02-28 13:00:12,666:INFO:              plotly: 5.24.1
2025-02-28 13:00:12,666:INFO:    plotly-resampler: Not installed
2025-02-28 13:00:12,666:INFO:             kaleido: 0.2.1
2025-02-28 13:00:12,666:INFO:           schemdraw: 0.15
2025-02-28 13:00:12,666:INFO:         statsmodels: 0.14.4
2025-02-28 13:00:12,666:INFO:              sktime: 0.26.0
2025-02-28 13:00:12,666:INFO:               tbats: 1.1.3
2025-02-28 13:00:12,666:INFO:            pmdarima: 2.0.4
2025-02-28 13:00:12,666:INFO:              psutil: 5.9.0
2025-02-28 13:00:12,666:INFO:          markupsafe: 2.1.5
2025-02-28 13:00:12,666:INFO:             pickle5: Not installed
2025-02-28 13:00:12,666:INFO:         cloudpickle: 3.1.1
2025-02-28 13:00:12,666:INFO:         deprecation: 2.1.0
2025-02-28 13:00:12,666:INFO:              xxhash: 3.5.0
2025-02-28 13:00:12,666:INFO:           wurlitzer: Not installed
2025-02-28 13:00:12,666:INFO:PyCaret optional dependencies:
2025-02-28 13:00:12,666:INFO:                shap: 0.44.1
2025-02-28 13:00:12,666:INFO:           interpret: 0.6.9
2025-02-28 13:00:12,666:INFO:                umap: 0.5.7
2025-02-28 13:00:12,667:INFO:     ydata_profiling: 4.12.2
2025-02-28 13:00:12,667:INFO:  explainerdashboard: 0.4.8
2025-02-28 13:00:12,667:INFO:             autoviz: Not installed
2025-02-28 13:00:12,667:INFO:           fairlearn: 0.7.0
2025-02-28 13:00:12,667:INFO:          deepchecks: Not installed
2025-02-28 13:00:12,667:INFO:             xgboost: 2.1.4
2025-02-28 13:00:12,667:INFO:            catboost: 1.2.7
2025-02-28 13:00:12,667:INFO:              kmodes: 0.12.2
2025-02-28 13:00:12,667:INFO:             mlxtend: 0.23.4
2025-02-28 13:00:12,667:INFO:       statsforecast: 1.5.0
2025-02-28 13:00:12,667:INFO:        tune_sklearn: Not installed
2025-02-28 13:00:12,667:INFO:                 ray: Not installed
2025-02-28 13:00:12,667:INFO:            hyperopt: 0.2.7
2025-02-28 13:00:12,667:INFO:              optuna: 4.2.0
2025-02-28 13:00:12,667:INFO:               skopt: 0.10.2
2025-02-28 13:00:12,667:INFO:              mlflow: 2.20.1
2025-02-28 13:00:12,667:INFO:              gradio: 5.15.0
2025-02-28 13:00:12,667:INFO:             fastapi: 0.115.8
2025-02-28 13:00:12,667:INFO:             uvicorn: 0.34.0
2025-02-28 13:00:12,667:INFO:              m2cgen: 0.10.0
2025-02-28 13:00:12,667:INFO:           evidently: 0.4.40
2025-02-28 13:00:12,667:INFO:               fugue: 0.8.7
2025-02-28 13:00:12,668:INFO:           streamlit: Not installed
2025-02-28 13:00:12,668:INFO:             prophet: Not installed
2025-02-28 13:00:12,668:INFO:None
2025-02-28 13:00:12,668:INFO:Set up data.
2025-02-28 13:00:12,671:INFO:Set up folding strategy.
2025-02-28 13:00:12,671:INFO:Set up train/test split.
2025-02-28 13:00:12,675:INFO:Set up index.
2025-02-28 13:00:12,675:INFO:Assigning column types.
2025-02-28 13:00:12,677:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 13:00:12,697:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,697:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,709:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,711:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,730:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,731:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,744:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,745:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,746:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 13:00:12,765:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,778:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,779:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,800:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,812:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,813:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,813:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 13:00:12,845:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,847:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,879:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,881:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,881:INFO:Preparing preprocessing pipeline...
2025-02-28 13:00:12,882:INFO:Set up simple imputation.
2025-02-28 13:00:12,883:INFO:Set up encoding of categorical features.
2025-02-28 13:00:12,914:INFO:Finished creating preprocessing pipeline.
2025-02-28 13:00:12,917:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 13:00:12,917:INFO:Creating final display dataframe.
2025-02-28 13:00:13,015:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              cc4d
2025-02-28 13:00:13,051:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:13,053:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:13,086:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:13,087:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:13,088:INFO:setup() successfully completed in 0.43s...............
2025-02-28 13:00:13,100:INFO:Initializing compare_models()
2025-02-28 13:00:13,100:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 13:00:13,100:INFO:Checking exceptions
2025-02-28 13:00:13,103:INFO:Preparing display monitor
2025-02-28 13:00:13,118:INFO:Initializing Logistic Regression
2025-02-28 13:00:13,119:INFO:Total runtime is 9.51687494913737e-06 minutes
2025-02-28 13:00:13,122:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:13,122:INFO:Initializing create_model()
2025-02-28 13:00:13,122:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:13,122:INFO:Checking exceptions
2025-02-28 13:00:13,122:INFO:Importing libraries
2025-02-28 13:00:13,122:INFO:Copying training dataset
2025-02-28 13:00:13,126:INFO:Defining folds
2025-02-28 13:00:13,126:INFO:Declaring metric variables
2025-02-28 13:00:13,127:INFO:Importing untrained model
2025-02-28 13:00:13,130:INFO:Logistic Regression Imported successfully
2025-02-28 13:00:13,136:INFO:Starting cross validation
2025-02-28 13:00:13,137:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:16,695:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,709:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,741:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,751:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,754:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,755:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,769:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,771:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,772:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,778:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,781:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,786:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,792:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,793:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,796:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,797:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,803:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,808:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,816:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,827:INFO:Calculating mean and std
2025-02-28 13:00:16,828:INFO:Creating metrics dataframe
2025-02-28 13:00:16,830:INFO:Uploading results into container
2025-02-28 13:00:16,830:INFO:Uploading model into container now
2025-02-28 13:00:16,830:INFO:_master_model_container: 1
2025-02-28 13:00:16,830:INFO:_display_container: 2
2025-02-28 13:00:16,831:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 13:00:16,831:INFO:create_model() successfully completed......................................
2025-02-28 13:00:16,960:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:16,960:INFO:Creating metrics dataframe
2025-02-28 13:00:16,964:INFO:Initializing K Neighbors Classifier
2025-02-28 13:00:16,964:INFO:Total runtime is 0.06410011847813925 minutes
2025-02-28 13:00:16,966:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:16,967:INFO:Initializing create_model()
2025-02-28 13:00:16,967:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:16,967:INFO:Checking exceptions
2025-02-28 13:00:16,967:INFO:Importing libraries
2025-02-28 13:00:16,967:INFO:Copying training dataset
2025-02-28 13:00:16,970:INFO:Defining folds
2025-02-28 13:00:16,970:INFO:Declaring metric variables
2025-02-28 13:00:16,973:INFO:Importing untrained model
2025-02-28 13:00:16,975:INFO:K Neighbors Classifier Imported successfully
2025-02-28 13:00:16,979:INFO:Starting cross validation
2025-02-28 13:00:16,980:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:19,793:INFO:Calculating mean and std
2025-02-28 13:00:19,794:INFO:Creating metrics dataframe
2025-02-28 13:00:19,796:INFO:Uploading results into container
2025-02-28 13:00:19,797:INFO:Uploading model into container now
2025-02-28 13:00:19,797:INFO:_master_model_container: 2
2025-02-28 13:00:19,797:INFO:_display_container: 2
2025-02-28 13:00:19,798:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 13:00:19,798:INFO:create_model() successfully completed......................................
2025-02-28 13:00:19,918:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:19,918:INFO:Creating metrics dataframe
2025-02-28 13:00:19,922:INFO:Initializing Naive Bayes
2025-02-28 13:00:19,922:INFO:Total runtime is 0.11340227921803793 minutes
2025-02-28 13:00:19,924:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:19,925:INFO:Initializing create_model()
2025-02-28 13:00:19,925:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:19,925:INFO:Checking exceptions
2025-02-28 13:00:19,925:INFO:Importing libraries
2025-02-28 13:00:19,925:INFO:Copying training dataset
2025-02-28 13:00:19,927:INFO:Defining folds
2025-02-28 13:00:19,928:INFO:Declaring metric variables
2025-02-28 13:00:19,930:INFO:Importing untrained model
2025-02-28 13:00:19,932:INFO:Naive Bayes Imported successfully
2025-02-28 13:00:19,936:INFO:Starting cross validation
2025-02-28 13:00:19,937:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:23,069:INFO:Calculating mean and std
2025-02-28 13:00:23,070:INFO:Creating metrics dataframe
2025-02-28 13:00:23,071:INFO:Uploading results into container
2025-02-28 13:00:23,071:INFO:Uploading model into container now
2025-02-28 13:00:23,072:INFO:_master_model_container: 3
2025-02-28 13:00:23,072:INFO:_display_container: 2
2025-02-28 13:00:23,072:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 13:00:23,072:INFO:create_model() successfully completed......................................
2025-02-28 13:00:23,195:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:23,197:INFO:Creating metrics dataframe
2025-02-28 13:00:23,200:INFO:Initializing Decision Tree Classifier
2025-02-28 13:00:23,200:INFO:Total runtime is 0.16803147395451865 minutes
2025-02-28 13:00:23,202:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:23,203:INFO:Initializing create_model()
2025-02-28 13:00:23,203:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:23,203:INFO:Checking exceptions
2025-02-28 13:00:23,203:INFO:Importing libraries
2025-02-28 13:00:23,203:INFO:Copying training dataset
2025-02-28 13:00:23,207:INFO:Defining folds
2025-02-28 13:00:23,207:INFO:Declaring metric variables
2025-02-28 13:00:23,210:INFO:Importing untrained model
2025-02-28 13:00:23,213:INFO:Decision Tree Classifier Imported successfully
2025-02-28 13:00:23,220:INFO:Starting cross validation
2025-02-28 13:00:23,221:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:25,328:INFO:Calculating mean and std
2025-02-28 13:00:25,329:INFO:Creating metrics dataframe
2025-02-28 13:00:25,329:INFO:Uploading results into container
2025-02-28 13:00:25,330:INFO:Uploading model into container now
2025-02-28 13:00:25,330:INFO:_master_model_container: 4
2025-02-28 13:00:25,330:INFO:_display_container: 2
2025-02-28 13:00:25,330:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 13:00:25,330:INFO:create_model() successfully completed......................................
2025-02-28 13:00:25,444:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:25,445:INFO:Creating metrics dataframe
2025-02-28 13:00:25,449:INFO:Initializing SVM - Linear Kernel
2025-02-28 13:00:25,449:INFO:Total runtime is 0.20550336837768557 minutes
2025-02-28 13:00:25,451:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:25,451:INFO:Initializing create_model()
2025-02-28 13:00:25,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:25,451:INFO:Checking exceptions
2025-02-28 13:00:25,451:INFO:Importing libraries
2025-02-28 13:00:25,451:INFO:Copying training dataset
2025-02-28 13:00:25,455:INFO:Defining folds
2025-02-28 13:00:25,455:INFO:Declaring metric variables
2025-02-28 13:00:25,458:INFO:Importing untrained model
2025-02-28 13:00:25,461:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 13:00:25,465:INFO:Starting cross validation
2025-02-28 13:00:25,466:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:25,568:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,573:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,585:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,586:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,588:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,590:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,602:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,602:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,602:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,602:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,605:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,621:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,629:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,629:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,631:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,631:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,640:INFO:Calculating mean and std
2025-02-28 13:00:25,641:INFO:Creating metrics dataframe
2025-02-28 13:00:25,642:INFO:Uploading results into container
2025-02-28 13:00:25,642:INFO:Uploading model into container now
2025-02-28 13:00:25,642:INFO:_master_model_container: 5
2025-02-28 13:00:25,642:INFO:_display_container: 2
2025-02-28 13:00:25,643:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 13:00:25,643:INFO:create_model() successfully completed......................................
2025-02-28 13:00:25,754:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:25,754:INFO:Creating metrics dataframe
2025-02-28 13:00:25,759:INFO:Initializing Ridge Classifier
2025-02-28 13:00:25,759:INFO:Total runtime is 0.2106738090515137 minutes
2025-02-28 13:00:25,760:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:25,760:INFO:Initializing create_model()
2025-02-28 13:00:25,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:25,761:INFO:Checking exceptions
2025-02-28 13:00:25,761:INFO:Importing libraries
2025-02-28 13:00:25,761:INFO:Copying training dataset
2025-02-28 13:00:25,763:INFO:Defining folds
2025-02-28 13:00:25,763:INFO:Declaring metric variables
2025-02-28 13:00:25,766:INFO:Importing untrained model
2025-02-28 13:00:25,768:INFO:Ridge Classifier Imported successfully
2025-02-28 13:00:25,773:INFO:Starting cross validation
2025-02-28 13:00:25,774:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:25,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,829:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,830:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,830:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,838:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,838:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,838:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,840:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,852:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,857:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,875:INFO:Calculating mean and std
2025-02-28 13:00:25,877:INFO:Creating metrics dataframe
2025-02-28 13:00:25,880:INFO:Uploading results into container
2025-02-28 13:00:25,881:INFO:Uploading model into container now
2025-02-28 13:00:25,881:INFO:_master_model_container: 6
2025-02-28 13:00:25,881:INFO:_display_container: 2
2025-02-28 13:00:25,881:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 13:00:25,881:INFO:create_model() successfully completed......................................
2025-02-28 13:00:25,998:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:25,998:INFO:Creating metrics dataframe
2025-02-28 13:00:26,003:INFO:Initializing Random Forest Classifier
2025-02-28 13:00:26,003:INFO:Total runtime is 0.21474105517069503 minutes
2025-02-28 13:00:26,005:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:26,005:INFO:Initializing create_model()
2025-02-28 13:00:26,005:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:26,005:INFO:Checking exceptions
2025-02-28 13:00:26,005:INFO:Importing libraries
2025-02-28 13:00:26,005:INFO:Copying training dataset
2025-02-28 13:00:26,008:INFO:Defining folds
2025-02-28 13:00:26,008:INFO:Declaring metric variables
2025-02-28 13:00:26,011:INFO:Importing untrained model
2025-02-28 13:00:26,014:INFO:Random Forest Classifier Imported successfully
2025-02-28 13:00:26,018:INFO:Starting cross validation
2025-02-28 13:00:26,019:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:26,420:INFO:Calculating mean and std
2025-02-28 13:00:26,421:INFO:Creating metrics dataframe
2025-02-28 13:00:26,423:INFO:Uploading results into container
2025-02-28 13:00:26,423:INFO:Uploading model into container now
2025-02-28 13:00:26,424:INFO:_master_model_container: 7
2025-02-28 13:00:26,424:INFO:_display_container: 2
2025-02-28 13:00:26,424:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 13:00:26,424:INFO:create_model() successfully completed......................................
2025-02-28 13:00:26,534:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:26,534:INFO:Creating metrics dataframe
2025-02-28 13:00:26,539:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 13:00:26,539:INFO:Total runtime is 0.2236805359522502 minutes
2025-02-28 13:00:26,541:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:26,541:INFO:Initializing create_model()
2025-02-28 13:00:26,541:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:26,541:INFO:Checking exceptions
2025-02-28 13:00:26,541:INFO:Importing libraries
2025-02-28 13:00:26,541:INFO:Copying training dataset
2025-02-28 13:00:26,544:INFO:Defining folds
2025-02-28 13:00:26,544:INFO:Declaring metric variables
2025-02-28 13:00:26,546:INFO:Importing untrained model
2025-02-28 13:00:26,549:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 13:00:26,553:INFO:Starting cross validation
2025-02-28 13:00:26,554:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:26,591:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,597:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,598:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,599:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,601:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,601:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,604:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,610:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,610:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,612:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,616:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,616:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,616:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,620:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,627:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,631:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,639:INFO:Calculating mean and std
2025-02-28 13:00:26,639:INFO:Creating metrics dataframe
2025-02-28 13:00:26,641:INFO:Uploading results into container
2025-02-28 13:00:26,641:INFO:Uploading model into container now
2025-02-28 13:00:26,641:INFO:_master_model_container: 8
2025-02-28 13:00:26,641:INFO:_display_container: 2
2025-02-28 13:00:26,642:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 13:00:26,642:INFO:create_model() successfully completed......................................
2025-02-28 13:00:26,759:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:26,759:INFO:Creating metrics dataframe
2025-02-28 13:00:26,764:INFO:Initializing Ada Boost Classifier
2025-02-28 13:00:26,764:INFO:Total runtime is 0.22743436495463057 minutes
2025-02-28 13:00:26,766:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:26,767:INFO:Initializing create_model()
2025-02-28 13:00:26,767:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:26,767:INFO:Checking exceptions
2025-02-28 13:00:26,767:INFO:Importing libraries
2025-02-28 13:00:26,767:INFO:Copying training dataset
2025-02-28 13:00:26,769:INFO:Defining folds
2025-02-28 13:00:26,769:INFO:Declaring metric variables
2025-02-28 13:00:26,772:INFO:Importing untrained model
2025-02-28 13:00:26,774:INFO:Ada Boost Classifier Imported successfully
2025-02-28 13:00:26,778:INFO:Starting cross validation
2025-02-28 13:00:26,779:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:26,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,816:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,817:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,820:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,820:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,821:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,822:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,829:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,962:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,969:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,984:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,987:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,991:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,995:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,999:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,999:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:27,003:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:27,005:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:27,014:INFO:Calculating mean and std
2025-02-28 13:00:27,015:INFO:Creating metrics dataframe
2025-02-28 13:00:27,016:INFO:Uploading results into container
2025-02-28 13:00:27,016:INFO:Uploading model into container now
2025-02-28 13:00:27,016:INFO:_master_model_container: 9
2025-02-28 13:00:27,016:INFO:_display_container: 2
2025-02-28 13:00:27,017:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 13:00:27,017:INFO:create_model() successfully completed......................................
2025-02-28 13:00:27,125:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:27,125:INFO:Creating metrics dataframe
2025-02-28 13:00:27,132:INFO:Initializing Gradient Boosting Classifier
2025-02-28 13:00:27,132:INFO:Total runtime is 0.2335558374722799 minutes
2025-02-28 13:00:27,134:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:27,134:INFO:Initializing create_model()
2025-02-28 13:00:27,134:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:27,135:INFO:Checking exceptions
2025-02-28 13:00:27,135:INFO:Importing libraries
2025-02-28 13:00:27,135:INFO:Copying training dataset
2025-02-28 13:00:27,137:INFO:Defining folds
2025-02-28 13:00:27,137:INFO:Declaring metric variables
2025-02-28 13:00:27,140:INFO:Importing untrained model
2025-02-28 13:00:27,143:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 13:00:27,147:INFO:Starting cross validation
2025-02-28 13:00:27,148:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:29,490:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,495:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,506:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,512:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,540:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,549:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,571:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,574:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,585:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,600:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,620:INFO:Calculating mean and std
2025-02-28 13:00:29,620:INFO:Creating metrics dataframe
2025-02-28 13:00:29,622:INFO:Uploading results into container
2025-02-28 13:00:29,623:INFO:Uploading model into container now
2025-02-28 13:00:29,623:INFO:_master_model_container: 10
2025-02-28 13:00:29,623:INFO:_display_container: 2
2025-02-28 13:00:29,623:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 13:00:29,623:INFO:create_model() successfully completed......................................
2025-02-28 13:00:29,728:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:29,728:INFO:Creating metrics dataframe
2025-02-28 13:00:29,733:INFO:Initializing Linear Discriminant Analysis
2025-02-28 13:00:29,734:INFO:Total runtime is 0.276926875114441 minutes
2025-02-28 13:00:29,736:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:29,736:INFO:Initializing create_model()
2025-02-28 13:00:29,736:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:29,736:INFO:Checking exceptions
2025-02-28 13:00:29,737:INFO:Importing libraries
2025-02-28 13:00:29,737:INFO:Copying training dataset
2025-02-28 13:00:29,739:INFO:Defining folds
2025-02-28 13:00:29,739:INFO:Declaring metric variables
2025-02-28 13:00:29,743:INFO:Importing untrained model
2025-02-28 13:00:29,746:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 13:00:29,753:INFO:Starting cross validation
2025-02-28 13:00:29,755:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:29,804:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,805:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,805:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,807:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,809:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,816:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,823:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,825:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,829:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,839:INFO:Calculating mean and std
2025-02-28 13:00:29,840:INFO:Creating metrics dataframe
2025-02-28 13:00:29,841:INFO:Uploading results into container
2025-02-28 13:00:29,841:INFO:Uploading model into container now
2025-02-28 13:00:29,841:INFO:_master_model_container: 11
2025-02-28 13:00:29,841:INFO:_display_container: 2
2025-02-28 13:00:29,842:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 13:00:29,842:INFO:create_model() successfully completed......................................
2025-02-28 13:00:29,949:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:29,949:INFO:Creating metrics dataframe
2025-02-28 13:00:29,955:INFO:Initializing Extra Trees Classifier
2025-02-28 13:00:29,955:INFO:Total runtime is 0.2806074579556784 minutes
2025-02-28 13:00:29,957:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:29,957:INFO:Initializing create_model()
2025-02-28 13:00:29,957:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:29,957:INFO:Checking exceptions
2025-02-28 13:00:29,958:INFO:Importing libraries
2025-02-28 13:00:29,958:INFO:Copying training dataset
2025-02-28 13:00:29,960:INFO:Defining folds
2025-02-28 13:00:29,960:INFO:Declaring metric variables
2025-02-28 13:00:29,963:INFO:Importing untrained model
2025-02-28 13:00:29,965:INFO:Extra Trees Classifier Imported successfully
2025-02-28 13:00:29,971:INFO:Starting cross validation
2025-02-28 13:00:29,972:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:30,341:INFO:Calculating mean and std
2025-02-28 13:00:30,342:INFO:Creating metrics dataframe
2025-02-28 13:00:30,344:INFO:Uploading results into container
2025-02-28 13:00:30,345:INFO:Uploading model into container now
2025-02-28 13:00:30,345:INFO:_master_model_container: 12
2025-02-28 13:00:30,345:INFO:_display_container: 2
2025-02-28 13:00:30,345:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 13:00:30,346:INFO:create_model() successfully completed......................................
2025-02-28 13:00:30,456:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:30,456:INFO:Creating metrics dataframe
2025-02-28 13:00:30,460:INFO:Initializing Extreme Gradient Boosting
2025-02-28 13:00:30,460:INFO:Total runtime is 0.2890308777491252 minutes
2025-02-28 13:00:30,463:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:30,463:INFO:Initializing create_model()
2025-02-28 13:00:30,463:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:30,463:INFO:Checking exceptions
2025-02-28 13:00:30,463:INFO:Importing libraries
2025-02-28 13:00:30,463:INFO:Copying training dataset
2025-02-28 13:00:30,466:INFO:Defining folds
2025-02-28 13:00:30,466:INFO:Declaring metric variables
2025-02-28 13:00:30,469:INFO:Importing untrained model
2025-02-28 13:00:30,471:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 13:00:30,475:INFO:Starting cross validation
2025-02-28 13:00:30,476:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:31,387:INFO:Calculating mean and std
2025-02-28 13:00:31,388:INFO:Creating metrics dataframe
2025-02-28 13:00:31,390:INFO:Uploading results into container
2025-02-28 13:00:31,391:INFO:Uploading model into container now
2025-02-28 13:00:31,391:INFO:_master_model_container: 13
2025-02-28 13:00:31,391:INFO:_display_container: 2
2025-02-28 13:00:31,392:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 13:00:31,393:INFO:create_model() successfully completed......................................
2025-02-28 13:00:31,528:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:31,528:INFO:Creating metrics dataframe
2025-02-28 13:00:31,533:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 13:00:31,534:INFO:Total runtime is 0.306930700937907 minutes
2025-02-28 13:00:31,537:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:31,537:INFO:Initializing create_model()
2025-02-28 13:00:31,537:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:31,537:INFO:Checking exceptions
2025-02-28 13:00:31,537:INFO:Importing libraries
2025-02-28 13:00:31,537:INFO:Copying training dataset
2025-02-28 13:00:31,540:INFO:Defining folds
2025-02-28 13:00:31,540:INFO:Declaring metric variables
2025-02-28 13:00:31,544:INFO:Importing untrained model
2025-02-28 13:00:31,547:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:00:31,551:INFO:Starting cross validation
2025-02-28 13:00:31,552:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:39,491:INFO:Calculating mean and std
2025-02-28 13:00:39,492:INFO:Creating metrics dataframe
2025-02-28 13:00:39,494:INFO:Uploading results into container
2025-02-28 13:00:39,494:INFO:Uploading model into container now
2025-02-28 13:00:39,495:INFO:_master_model_container: 14
2025-02-28 13:00:39,495:INFO:_display_container: 2
2025-02-28 13:00:39,495:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:00:39,495:INFO:create_model() successfully completed......................................
2025-02-28 13:00:39,625:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:39,625:INFO:Creating metrics dataframe
2025-02-28 13:00:39,630:INFO:Initializing CatBoost Classifier
2025-02-28 13:00:39,630:INFO:Total runtime is 0.44186822970708217 minutes
2025-02-28 13:00:39,634:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:39,634:INFO:Initializing create_model()
2025-02-28 13:00:39,634:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:39,634:INFO:Checking exceptions
2025-02-28 13:00:39,634:INFO:Importing libraries
2025-02-28 13:00:39,634:INFO:Copying training dataset
2025-02-28 13:00:39,637:INFO:Defining folds
2025-02-28 13:00:39,637:INFO:Declaring metric variables
2025-02-28 13:00:39,639:INFO:Importing untrained model
2025-02-28 13:00:39,641:INFO:CatBoost Classifier Imported successfully
2025-02-28 13:00:39,645:INFO:Starting cross validation
2025-02-28 13:00:39,646:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:46,614:INFO:Calculating mean and std
2025-02-28 13:00:46,615:INFO:Creating metrics dataframe
2025-02-28 13:00:46,616:INFO:Uploading results into container
2025-02-28 13:00:46,616:INFO:Uploading model into container now
2025-02-28 13:00:46,617:INFO:_master_model_container: 15
2025-02-28 13:00:46,617:INFO:_display_container: 2
2025-02-28 13:00:46,617:INFO:<catboost.core.CatBoostClassifier object at 0x0000021913E86830>
2025-02-28 13:00:46,617:INFO:create_model() successfully completed......................................
2025-02-28 13:00:46,726:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:46,726:INFO:Creating metrics dataframe
2025-02-28 13:00:46,731:INFO:Initializing Dummy Classifier
2025-02-28 13:00:46,731:INFO:Total runtime is 0.5602180043856304 minutes
2025-02-28 13:00:46,733:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:46,734:INFO:Initializing create_model()
2025-02-28 13:00:46,734:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:46,734:INFO:Checking exceptions
2025-02-28 13:00:46,734:INFO:Importing libraries
2025-02-28 13:00:46,734:INFO:Copying training dataset
2025-02-28 13:00:46,736:INFO:Defining folds
2025-02-28 13:00:46,737:INFO:Declaring metric variables
2025-02-28 13:00:46,739:INFO:Importing untrained model
2025-02-28 13:00:46,741:INFO:Dummy Classifier Imported successfully
2025-02-28 13:00:46,745:INFO:Starting cross validation
2025-02-28 13:00:46,746:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:46,792:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,793:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,793:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,798:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,801:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,802:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,802:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,804:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,807:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,809:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,819:INFO:Calculating mean and std
2025-02-28 13:00:46,819:INFO:Creating metrics dataframe
2025-02-28 13:00:46,820:INFO:Uploading results into container
2025-02-28 13:00:46,820:INFO:Uploading model into container now
2025-02-28 13:00:46,820:INFO:_master_model_container: 16
2025-02-28 13:00:46,820:INFO:_display_container: 2
2025-02-28 13:00:46,820:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 13:00:46,820:INFO:create_model() successfully completed......................................
2025-02-28 13:00:46,929:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:46,929:INFO:Creating metrics dataframe
2025-02-28 13:00:46,936:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 13:00:46,942:INFO:Initializing create_model()
2025-02-28 13:00:46,942:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:46,942:INFO:Checking exceptions
2025-02-28 13:00:46,943:INFO:Importing libraries
2025-02-28 13:00:46,943:INFO:Copying training dataset
2025-02-28 13:00:46,945:INFO:Defining folds
2025-02-28 13:00:46,945:INFO:Declaring metric variables
2025-02-28 13:00:46,945:INFO:Importing untrained model
2025-02-28 13:00:46,945:INFO:Declaring custom model
2025-02-28 13:00:46,945:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:00:46,946:INFO:Cross validation set to False
2025-02-28 13:00:46,946:INFO:Fitting Model
2025-02-28 13:00:46,969:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.
2025-02-28 13:00:46,969:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 20
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.773639
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.804411
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.765257
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.800944
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.827239
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:00:47,279:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:00:47,279:INFO:create_model() successfully completed......................................
2025-02-28 13:00:47,417:INFO:_master_model_container: 16
2025-02-28 13:00:47,418:INFO:_display_container: 2
2025-02-28 13:00:47,418:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:00:47,418:INFO:compare_models() successfully completed......................................
2025-02-28 13:00:47,432:INFO:Initializing plot_model()
2025-02-28 13:00:47,432:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, system=True)
2025-02-28 13:00:47,432:INFO:Checking exceptions
2025-02-28 13:00:47,434:INFO:Preloading libraries
2025-02-28 13:00:47,471:INFO:Copying training dataset
2025-02-28 13:00:47,471:INFO:Plot type: confusion_matrix
2025-02-28 13:00:47,594:INFO:Fitting Model
2025-02-28 13:00:47,594:INFO:Scoring test/hold-out set
2025-02-28 13:00:47,726:INFO:Visual Rendered Successfully
2025-02-28 13:00:47,843:INFO:plot_model() successfully completed......................................
2025-02-28 13:00:47,855:INFO:Initializing plot_model()
2025-02-28 13:00:47,855:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, system=True)
2025-02-28 13:00:47,855:INFO:Checking exceptions
2025-02-28 13:00:47,858:INFO:Preloading libraries
2025-02-28 13:00:47,900:INFO:Copying training dataset
2025-02-28 13:00:47,901:INFO:Plot type: auc
2025-02-28 13:00:48,023:INFO:Fitting Model
2025-02-28 13:00:48,023:INFO:Scoring test/hold-out set
2025-02-28 13:00:48,164:INFO:Visual Rendered Successfully
2025-02-28 13:00:48,277:INFO:plot_model() successfully completed......................................
2025-02-28 13:00:48,298:INFO:Initializing plot_model()
2025-02-28 13:00:48,298:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, system=True)
2025-02-28 13:00:48,298:INFO:Checking exceptions
2025-02-28 13:00:48,300:INFO:Preloading libraries
2025-02-28 13:00:48,341:INFO:Copying training dataset
2025-02-28 13:00:48,341:INFO:Plot type: feature
2025-02-28 13:00:48,341:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 13:00:48,462:INFO:Visual Rendered Successfully
2025-02-28 13:00:48,576:INFO:plot_model() successfully completed......................................
2025-02-28 13:00:48,603:INFO:PyCaret RegressionExperiment
2025-02-28 13:00:48,603:INFO:Logging name: reg-default-name
2025-02-28 13:00:48,603:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 13:00:48,603:INFO:version 3.3.2
2025-02-28 13:00:48,603:INFO:Initializing setup()
2025-02-28 13:00:48,603:INFO:self.USI: cbca
2025-02-28 13:00:48,603:INFO:self._variable_keys: {'transform_target_param', 'gpu_param', 'target_param', 'exp_id', 'data', 'USI', 'gpu_n_jobs_param', '_ml_usecase', 'y_train', 'pipeline', '_available_plots', 'seed', 'X_train', 'fold_generator', 'logging_param', 'X', 'fold_shuffle_param', 'html_param', 'idx', 'exp_name_log', 'memory', 'y', 'fold_groups_param', 'log_plots_param', 'y_test', 'X_test', 'n_jobs_param'}
2025-02-28 13:00:48,603:INFO:Checking environment
2025-02-28 13:00:48,603:INFO:python_version: 3.10.16
2025-02-28 13:00:48,603:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 13:00:48,603:INFO:machine: AMD64
2025-02-28 13:00:48,603:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 13:00:48,603:INFO:Memory: svmem(total=34200334336, available=11507613696, percent=66.4, used=22692720640, free=11507613696)
2025-02-28 13:00:48,604:INFO:Physical Core: 24
2025-02-28 13:00:48,604:INFO:Logical Core: 32
2025-02-28 13:00:48,604:INFO:Checking libraries
2025-02-28 13:00:48,604:INFO:System:
2025-02-28 13:00:48,604:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 13:00:48,604:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 13:00:48,604:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 13:00:48,604:INFO:PyCaret required dependencies:
2025-02-28 13:00:48,604:INFO:                 pip: 25.0
2025-02-28 13:00:48,604:INFO:          setuptools: 75.8.0
2025-02-28 13:00:48,604:INFO:             pycaret: 3.3.2
2025-02-28 13:00:48,604:INFO:             IPython: 8.30.0
2025-02-28 13:00:48,604:INFO:          ipywidgets: 8.1.5
2025-02-28 13:00:48,605:INFO:                tqdm: 4.67.1
2025-02-28 13:00:48,605:INFO:               numpy: 1.26.4
2025-02-28 13:00:48,605:INFO:              pandas: 2.1.4
2025-02-28 13:00:48,605:INFO:              jinja2: 3.1.5
2025-02-28 13:00:48,605:INFO:               scipy: 1.11.4
2025-02-28 13:00:48,605:INFO:              joblib: 1.3.2
2025-02-28 13:00:48,605:INFO:             sklearn: 1.4.2
2025-02-28 13:00:48,605:INFO:                pyod: 2.0.3
2025-02-28 13:00:48,605:INFO:            imblearn: 0.13.0
2025-02-28 13:00:48,605:INFO:   category_encoders: 2.7.0
2025-02-28 13:00:48,605:INFO:            lightgbm: 4.5.0
2025-02-28 13:00:48,605:INFO:               numba: 0.61.0
2025-02-28 13:00:48,605:INFO:            requests: 2.32.3
2025-02-28 13:00:48,605:INFO:          matplotlib: 3.7.5
2025-02-28 13:00:48,605:INFO:          scikitplot: 0.3.7
2025-02-28 13:00:48,605:INFO:         yellowbrick: 1.5
2025-02-28 13:00:48,606:INFO:              plotly: 5.24.1
2025-02-28 13:00:48,606:INFO:    plotly-resampler: Not installed
2025-02-28 13:00:48,606:INFO:             kaleido: 0.2.1
2025-02-28 13:00:48,606:INFO:           schemdraw: 0.15
2025-02-28 13:00:48,606:INFO:         statsmodels: 0.14.4
2025-02-28 13:00:48,606:INFO:              sktime: 0.26.0
2025-02-28 13:00:48,606:INFO:               tbats: 1.1.3
2025-02-28 13:00:48,606:INFO:            pmdarima: 2.0.4
2025-02-28 13:00:48,606:INFO:              psutil: 5.9.0
2025-02-28 13:00:48,606:INFO:          markupsafe: 2.1.5
2025-02-28 13:00:48,606:INFO:             pickle5: Not installed
2025-02-28 13:00:48,606:INFO:         cloudpickle: 3.1.1
2025-02-28 13:00:48,606:INFO:         deprecation: 2.1.0
2025-02-28 13:00:48,606:INFO:              xxhash: 3.5.0
2025-02-28 13:00:48,606:INFO:           wurlitzer: Not installed
2025-02-28 13:00:48,606:INFO:PyCaret optional dependencies:
2025-02-28 13:00:48,606:INFO:                shap: 0.44.1
2025-02-28 13:00:48,606:INFO:           interpret: 0.6.9
2025-02-28 13:00:48,606:INFO:                umap: 0.5.7
2025-02-28 13:00:48,606:INFO:     ydata_profiling: 4.12.2
2025-02-28 13:00:48,607:INFO:  explainerdashboard: 0.4.8
2025-02-28 13:00:48,607:INFO:             autoviz: Not installed
2025-02-28 13:00:48,607:INFO:           fairlearn: 0.7.0
2025-02-28 13:00:48,607:INFO:          deepchecks: Not installed
2025-02-28 13:00:48,607:INFO:             xgboost: 2.1.4
2025-02-28 13:00:48,607:INFO:            catboost: 1.2.7
2025-02-28 13:00:48,607:INFO:              kmodes: 0.12.2
2025-02-28 13:00:48,607:INFO:             mlxtend: 0.23.4
2025-02-28 13:00:48,607:INFO:       statsforecast: 1.5.0
2025-02-28 13:00:48,607:INFO:        tune_sklearn: Not installed
2025-02-28 13:00:48,607:INFO:                 ray: Not installed
2025-02-28 13:00:48,607:INFO:            hyperopt: 0.2.7
2025-02-28 13:00:48,607:INFO:              optuna: 4.2.0
2025-02-28 13:00:48,607:INFO:               skopt: 0.10.2
2025-02-28 13:00:48,607:INFO:              mlflow: 2.20.1
2025-02-28 13:00:48,607:INFO:              gradio: 5.15.0
2025-02-28 13:00:48,607:INFO:             fastapi: 0.115.8
2025-02-28 13:00:48,608:INFO:             uvicorn: 0.34.0
2025-02-28 13:00:48,608:INFO:              m2cgen: 0.10.0
2025-02-28 13:00:48,608:INFO:           evidently: 0.4.40
2025-02-28 13:00:48,608:INFO:               fugue: 0.8.7
2025-02-28 13:00:48,608:INFO:           streamlit: Not installed
2025-02-28 13:00:48,608:INFO:             prophet: Not installed
2025-02-28 13:00:48,608:INFO:None
2025-02-28 13:00:48,608:INFO:Set up data.
2025-02-28 13:00:48,611:INFO:Set up folding strategy.
2025-02-28 13:00:48,611:INFO:Set up train/test split.
2025-02-28 13:00:48,614:INFO:Set up index.
2025-02-28 13:00:48,614:INFO:Assigning column types.
2025-02-28 13:00:48,616:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 13:00:48,616:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,618:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,620:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,646:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,664:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,665:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,666:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,666:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,668:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,670:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,697:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,716:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,716:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,718:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,718:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 13:00:48,720:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,722:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,748:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,767:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,768:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,769:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,771:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,773:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,799:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,819:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,820:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,821:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,821:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 13:00:48,825:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,852:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,871:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,871:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,872:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,876:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,903:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,922:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,922:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,923:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,923:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 13:00:48,954:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,972:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,973:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,974:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,002:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:49,023:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:49,024:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,025:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,025:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 13:00:49,056:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:49,075:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,077:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,106:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:49,125:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,126:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,126:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 13:00:49,175:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,176:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,228:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,229:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,229:INFO:Preparing preprocessing pipeline...
2025-02-28 13:00:49,229:INFO:Set up simple imputation.
2025-02-28 13:00:49,231:INFO:Set up encoding of categorical features.
2025-02-28 13:00:49,259:INFO:Finished creating preprocessing pipeline.
2025-02-28 13:00:49,262:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 13:00:49,262:INFO:Creating final display dataframe.
2025-02-28 13:00:49,375:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              cbca
2025-02-28 13:00:49,430:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,431:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,482:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,484:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,484:INFO:setup() successfully completed in 0.88s...............
2025-02-28 13:00:49,503:INFO:Initializing compare_models()
2025-02-28 13:00:49,503:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 13:00:49,503:INFO:Checking exceptions
2025-02-28 13:00:49,505:INFO:Preparing display monitor
2025-02-28 13:00:49,521:INFO:Initializing Linear Regression
2025-02-28 13:00:49,521:INFO:Total runtime is 0.0 minutes
2025-02-28 13:00:49,523:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:49,523:INFO:Initializing create_model()
2025-02-28 13:00:49,523:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:49,524:INFO:Checking exceptions
2025-02-28 13:00:49,524:INFO:Importing libraries
2025-02-28 13:00:49,524:INFO:Copying training dataset
2025-02-28 13:00:49,528:INFO:Defining folds
2025-02-28 13:00:49,529:INFO:Declaring metric variables
2025-02-28 13:00:49,530:INFO:Importing untrained model
2025-02-28 13:00:49,533:INFO:Linear Regression Imported successfully
2025-02-28 13:00:49,539:INFO:Starting cross validation
2025-02-28 13:00:49,540:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:49,650:INFO:Calculating mean and std
2025-02-28 13:00:49,651:INFO:Creating metrics dataframe
2025-02-28 13:00:49,652:INFO:Uploading results into container
2025-02-28 13:00:49,653:INFO:Uploading model into container now
2025-02-28 13:00:49,653:INFO:_master_model_container: 1
2025-02-28 13:00:49,653:INFO:_display_container: 2
2025-02-28 13:00:49,653:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 13:00:49,653:INFO:create_model() successfully completed......................................
2025-02-28 13:00:49,763:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:49,763:INFO:Creating metrics dataframe
2025-02-28 13:00:49,766:INFO:Initializing Lasso Regression
2025-02-28 13:00:49,766:INFO:Total runtime is 0.004083216190338135 minutes
2025-02-28 13:00:49,768:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:49,768:INFO:Initializing create_model()
2025-02-28 13:00:49,768:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:49,769:INFO:Checking exceptions
2025-02-28 13:00:49,769:INFO:Importing libraries
2025-02-28 13:00:49,769:INFO:Copying training dataset
2025-02-28 13:00:49,771:INFO:Defining folds
2025-02-28 13:00:49,771:INFO:Declaring metric variables
2025-02-28 13:00:49,772:INFO:Importing untrained model
2025-02-28 13:00:49,775:INFO:Lasso Regression Imported successfully
2025-02-28 13:00:49,778:INFO:Starting cross validation
2025-02-28 13:00:49,779:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:49,853:INFO:Calculating mean and std
2025-02-28 13:00:49,853:INFO:Creating metrics dataframe
2025-02-28 13:00:49,854:INFO:Uploading results into container
2025-02-28 13:00:49,854:INFO:Uploading model into container now
2025-02-28 13:00:49,855:INFO:_master_model_container: 2
2025-02-28 13:00:49,855:INFO:_display_container: 2
2025-02-28 13:00:49,855:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 13:00:49,855:INFO:create_model() successfully completed......................................
2025-02-28 13:00:49,968:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:49,968:INFO:Creating metrics dataframe
2025-02-28 13:00:49,972:INFO:Initializing Ridge Regression
2025-02-28 13:00:49,972:INFO:Total runtime is 0.0075266361236572266 minutes
2025-02-28 13:00:49,974:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:49,974:INFO:Initializing create_model()
2025-02-28 13:00:49,974:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:49,974:INFO:Checking exceptions
2025-02-28 13:00:49,974:INFO:Importing libraries
2025-02-28 13:00:49,974:INFO:Copying training dataset
2025-02-28 13:00:49,977:INFO:Defining folds
2025-02-28 13:00:49,977:INFO:Declaring metric variables
2025-02-28 13:00:49,979:INFO:Importing untrained model
2025-02-28 13:00:49,981:INFO:Ridge Regression Imported successfully
2025-02-28 13:00:49,985:INFO:Starting cross validation
2025-02-28 13:00:49,986:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,056:INFO:Calculating mean and std
2025-02-28 13:00:50,056:INFO:Creating metrics dataframe
2025-02-28 13:00:50,057:INFO:Uploading results into container
2025-02-28 13:00:50,057:INFO:Uploading model into container now
2025-02-28 13:00:50,058:INFO:_master_model_container: 3
2025-02-28 13:00:50,058:INFO:_display_container: 2
2025-02-28 13:00:50,058:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 13:00:50,058:INFO:create_model() successfully completed......................................
2025-02-28 13:00:50,174:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:50,174:INFO:Creating metrics dataframe
2025-02-28 13:00:50,178:INFO:Initializing Elastic Net
2025-02-28 13:00:50,178:INFO:Total runtime is 0.010950211683909097 minutes
2025-02-28 13:00:50,180:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:50,180:INFO:Initializing create_model()
2025-02-28 13:00:50,180:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:50,181:INFO:Checking exceptions
2025-02-28 13:00:50,181:INFO:Importing libraries
2025-02-28 13:00:50,181:INFO:Copying training dataset
2025-02-28 13:00:50,183:INFO:Defining folds
2025-02-28 13:00:50,183:INFO:Declaring metric variables
2025-02-28 13:00:50,185:INFO:Importing untrained model
2025-02-28 13:00:50,187:INFO:Elastic Net Imported successfully
2025-02-28 13:00:50,191:INFO:Starting cross validation
2025-02-28 13:00:50,192:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,273:INFO:Calculating mean and std
2025-02-28 13:00:50,274:INFO:Creating metrics dataframe
2025-02-28 13:00:50,275:INFO:Uploading results into container
2025-02-28 13:00:50,276:INFO:Uploading model into container now
2025-02-28 13:00:50,276:INFO:_master_model_container: 4
2025-02-28 13:00:50,276:INFO:_display_container: 2
2025-02-28 13:00:50,276:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 13:00:50,276:INFO:create_model() successfully completed......................................
2025-02-28 13:00:50,392:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:50,393:INFO:Creating metrics dataframe
2025-02-28 13:00:50,397:INFO:Initializing Least Angle Regression
2025-02-28 13:00:50,397:INFO:Total runtime is 0.014599299430847167 minutes
2025-02-28 13:00:50,399:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:50,399:INFO:Initializing create_model()
2025-02-28 13:00:50,399:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:50,399:INFO:Checking exceptions
2025-02-28 13:00:50,399:INFO:Importing libraries
2025-02-28 13:00:50,399:INFO:Copying training dataset
2025-02-28 13:00:50,402:INFO:Defining folds
2025-02-28 13:00:50,402:INFO:Declaring metric variables
2025-02-28 13:00:50,404:INFO:Importing untrained model
2025-02-28 13:00:50,407:INFO:Least Angle Regression Imported successfully
2025-02-28 13:00:50,411:INFO:Starting cross validation
2025-02-28 13:00:50,412:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,489:INFO:Calculating mean and std
2025-02-28 13:00:50,489:INFO:Creating metrics dataframe
2025-02-28 13:00:50,490:INFO:Uploading results into container
2025-02-28 13:00:50,490:INFO:Uploading model into container now
2025-02-28 13:00:50,490:INFO:_master_model_container: 5
2025-02-28 13:00:50,490:INFO:_display_container: 2
2025-02-28 13:00:50,490:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 13:00:50,490:INFO:create_model() successfully completed......................................
2025-02-28 13:00:50,599:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:50,599:INFO:Creating metrics dataframe
2025-02-28 13:00:50,604:INFO:Initializing Lasso Least Angle Regression
2025-02-28 13:00:50,604:INFO:Total runtime is 0.018060855070749917 minutes
2025-02-28 13:00:50,606:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:50,606:INFO:Initializing create_model()
2025-02-28 13:00:50,606:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:50,606:INFO:Checking exceptions
2025-02-28 13:00:50,606:INFO:Importing libraries
2025-02-28 13:00:50,606:INFO:Copying training dataset
2025-02-28 13:00:50,609:INFO:Defining folds
2025-02-28 13:00:50,609:INFO:Declaring metric variables
2025-02-28 13:00:50,612:INFO:Importing untrained model
2025-02-28 13:00:50,613:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 13:00:50,618:INFO:Starting cross validation
2025-02-28 13:00:50,619:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,691:INFO:Calculating mean and std
2025-02-28 13:00:50,691:INFO:Creating metrics dataframe
2025-02-28 13:00:50,692:INFO:Uploading results into container
2025-02-28 13:00:50,692:INFO:Uploading model into container now
2025-02-28 13:00:50,693:INFO:_master_model_container: 6
2025-02-28 13:00:50,693:INFO:_display_container: 2
2025-02-28 13:00:50,693:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 13:00:50,693:INFO:create_model() successfully completed......................................
2025-02-28 13:00:50,806:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:50,806:INFO:Creating metrics dataframe
2025-02-28 13:00:50,811:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 13:00:50,811:INFO:Total runtime is 0.0215025266011556 minutes
2025-02-28 13:00:50,813:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:50,813:INFO:Initializing create_model()
2025-02-28 13:00:50,813:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:50,813:INFO:Checking exceptions
2025-02-28 13:00:50,813:INFO:Importing libraries
2025-02-28 13:00:50,813:INFO:Copying training dataset
2025-02-28 13:00:50,816:INFO:Defining folds
2025-02-28 13:00:50,817:INFO:Declaring metric variables
2025-02-28 13:00:50,819:INFO:Importing untrained model
2025-02-28 13:00:50,822:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 13:00:50,826:INFO:Starting cross validation
2025-02-28 13:00:50,827:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,907:INFO:Calculating mean and std
2025-02-28 13:00:50,908:INFO:Creating metrics dataframe
2025-02-28 13:00:50,908:INFO:Uploading results into container
2025-02-28 13:00:50,908:INFO:Uploading model into container now
2025-02-28 13:00:50,908:INFO:_master_model_container: 7
2025-02-28 13:00:50,908:INFO:_display_container: 2
2025-02-28 13:00:50,909:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 13:00:50,909:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,021:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,021:INFO:Creating metrics dataframe
2025-02-28 13:00:51,025:INFO:Initializing Bayesian Ridge
2025-02-28 13:00:51,025:INFO:Total runtime is 0.02508093516031901 minutes
2025-02-28 13:00:51,027:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,027:INFO:Initializing create_model()
2025-02-28 13:00:51,027:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,027:INFO:Checking exceptions
2025-02-28 13:00:51,027:INFO:Importing libraries
2025-02-28 13:00:51,027:INFO:Copying training dataset
2025-02-28 13:00:51,031:INFO:Defining folds
2025-02-28 13:00:51,031:INFO:Declaring metric variables
2025-02-28 13:00:51,033:INFO:Importing untrained model
2025-02-28 13:00:51,036:INFO:Bayesian Ridge Imported successfully
2025-02-28 13:00:51,039:INFO:Starting cross validation
2025-02-28 13:00:51,040:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:51,124:INFO:Calculating mean and std
2025-02-28 13:00:51,124:INFO:Creating metrics dataframe
2025-02-28 13:00:51,125:INFO:Uploading results into container
2025-02-28 13:00:51,125:INFO:Uploading model into container now
2025-02-28 13:00:51,126:INFO:_master_model_container: 8
2025-02-28 13:00:51,126:INFO:_display_container: 2
2025-02-28 13:00:51,126:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 13:00:51,126:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,233:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,234:INFO:Creating metrics dataframe
2025-02-28 13:00:51,238:INFO:Initializing Passive Aggressive Regressor
2025-02-28 13:00:51,238:INFO:Total runtime is 0.02862518628438314 minutes
2025-02-28 13:00:51,241:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,241:INFO:Initializing create_model()
2025-02-28 13:00:51,241:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,241:INFO:Checking exceptions
2025-02-28 13:00:51,241:INFO:Importing libraries
2025-02-28 13:00:51,241:INFO:Copying training dataset
2025-02-28 13:00:51,244:INFO:Defining folds
2025-02-28 13:00:51,244:INFO:Declaring metric variables
2025-02-28 13:00:51,246:INFO:Importing untrained model
2025-02-28 13:00:51,248:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 13:00:51,252:INFO:Starting cross validation
2025-02-28 13:00:51,253:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:51,327:INFO:Calculating mean and std
2025-02-28 13:00:51,327:INFO:Creating metrics dataframe
2025-02-28 13:00:51,328:INFO:Uploading results into container
2025-02-28 13:00:51,328:INFO:Uploading model into container now
2025-02-28 13:00:51,329:INFO:_master_model_container: 9
2025-02-28 13:00:51,329:INFO:_display_container: 2
2025-02-28 13:00:51,329:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 13:00:51,329:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,438:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,438:INFO:Creating metrics dataframe
2025-02-28 13:00:51,443:INFO:Initializing Huber Regressor
2025-02-28 13:00:51,443:INFO:Total runtime is 0.032040309906005864 minutes
2025-02-28 13:00:51,445:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,445:INFO:Initializing create_model()
2025-02-28 13:00:51,445:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,445:INFO:Checking exceptions
2025-02-28 13:00:51,445:INFO:Importing libraries
2025-02-28 13:00:51,445:INFO:Copying training dataset
2025-02-28 13:00:51,448:INFO:Defining folds
2025-02-28 13:00:51,448:INFO:Declaring metric variables
2025-02-28 13:00:51,450:INFO:Importing untrained model
2025-02-28 13:00:51,453:INFO:Huber Regressor Imported successfully
2025-02-28 13:00:51,457:INFO:Starting cross validation
2025-02-28 13:00:51,458:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:51,516:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,520:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,521:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,522:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,527:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,529:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,532:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,533:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,534:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,542:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,560:INFO:Calculating mean and std
2025-02-28 13:00:51,560:INFO:Creating metrics dataframe
2025-02-28 13:00:51,562:INFO:Uploading results into container
2025-02-28 13:00:51,562:INFO:Uploading model into container now
2025-02-28 13:00:51,562:INFO:_master_model_container: 10
2025-02-28 13:00:51,562:INFO:_display_container: 2
2025-02-28 13:00:51,563:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 13:00:51,563:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,677:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,677:INFO:Creating metrics dataframe
2025-02-28 13:00:51,681:INFO:Initializing K Neighbors Regressor
2025-02-28 13:00:51,681:INFO:Total runtime is 0.03601033290227255 minutes
2025-02-28 13:00:51,684:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,684:INFO:Initializing create_model()
2025-02-28 13:00:51,684:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,684:INFO:Checking exceptions
2025-02-28 13:00:51,684:INFO:Importing libraries
2025-02-28 13:00:51,684:INFO:Copying training dataset
2025-02-28 13:00:51,688:INFO:Defining folds
2025-02-28 13:00:51,688:INFO:Declaring metric variables
2025-02-28 13:00:51,690:INFO:Importing untrained model
2025-02-28 13:00:51,692:INFO:K Neighbors Regressor Imported successfully
2025-02-28 13:00:51,696:INFO:Starting cross validation
2025-02-28 13:00:51,697:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:51,779:INFO:Calculating mean and std
2025-02-28 13:00:51,779:INFO:Creating metrics dataframe
2025-02-28 13:00:51,779:INFO:Uploading results into container
2025-02-28 13:00:51,781:INFO:Uploading model into container now
2025-02-28 13:00:51,781:INFO:_master_model_container: 11
2025-02-28 13:00:51,781:INFO:_display_container: 2
2025-02-28 13:00:51,781:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 13:00:51,781:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,897:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,897:INFO:Creating metrics dataframe
2025-02-28 13:00:51,901:INFO:Initializing Decision Tree Regressor
2025-02-28 13:00:51,901:INFO:Total runtime is 0.03967777490615845 minutes
2025-02-28 13:00:51,904:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,904:INFO:Initializing create_model()
2025-02-28 13:00:51,904:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,904:INFO:Checking exceptions
2025-02-28 13:00:51,904:INFO:Importing libraries
2025-02-28 13:00:51,904:INFO:Copying training dataset
2025-02-28 13:00:51,907:INFO:Defining folds
2025-02-28 13:00:51,907:INFO:Declaring metric variables
2025-02-28 13:00:51,909:INFO:Importing untrained model
2025-02-28 13:00:51,911:INFO:Decision Tree Regressor Imported successfully
2025-02-28 13:00:51,916:INFO:Starting cross validation
2025-02-28 13:00:51,916:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:52,010:INFO:Calculating mean and std
2025-02-28 13:00:52,010:INFO:Creating metrics dataframe
2025-02-28 13:00:52,011:INFO:Uploading results into container
2025-02-28 13:00:52,012:INFO:Uploading model into container now
2025-02-28 13:00:52,012:INFO:_master_model_container: 12
2025-02-28 13:00:52,012:INFO:_display_container: 2
2025-02-28 13:00:52,012:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 13:00:52,012:INFO:create_model() successfully completed......................................
2025-02-28 13:00:52,130:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:52,130:INFO:Creating metrics dataframe
2025-02-28 13:00:52,135:INFO:Initializing Random Forest Regressor
2025-02-28 13:00:52,136:INFO:Total runtime is 0.043579455216725675 minutes
2025-02-28 13:00:52,138:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:52,138:INFO:Initializing create_model()
2025-02-28 13:00:52,138:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:52,138:INFO:Checking exceptions
2025-02-28 13:00:52,138:INFO:Importing libraries
2025-02-28 13:00:52,138:INFO:Copying training dataset
2025-02-28 13:00:52,141:INFO:Defining folds
2025-02-28 13:00:52,141:INFO:Declaring metric variables
2025-02-28 13:00:52,144:INFO:Importing untrained model
2025-02-28 13:00:52,146:INFO:Random Forest Regressor Imported successfully
2025-02-28 13:00:52,151:INFO:Starting cross validation
2025-02-28 13:00:52,153:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:53,024:INFO:Calculating mean and std
2025-02-28 13:00:53,025:INFO:Creating metrics dataframe
2025-02-28 13:00:53,026:INFO:Uploading results into container
2025-02-28 13:00:53,027:INFO:Uploading model into container now
2025-02-28 13:00:53,027:INFO:_master_model_container: 13
2025-02-28 13:00:53,027:INFO:_display_container: 2
2025-02-28 13:00:53,027:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 13:00:53,027:INFO:create_model() successfully completed......................................
2025-02-28 13:00:53,138:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:53,138:INFO:Creating metrics dataframe
2025-02-28 13:00:53,144:INFO:Initializing Extra Trees Regressor
2025-02-28 13:00:53,144:INFO:Total runtime is 0.06039078235626222 minutes
2025-02-28 13:00:53,146:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:53,146:INFO:Initializing create_model()
2025-02-28 13:00:53,146:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:53,146:INFO:Checking exceptions
2025-02-28 13:00:53,146:INFO:Importing libraries
2025-02-28 13:00:53,146:INFO:Copying training dataset
2025-02-28 13:00:53,149:INFO:Defining folds
2025-02-28 13:00:53,149:INFO:Declaring metric variables
2025-02-28 13:00:53,153:INFO:Importing untrained model
2025-02-28 13:00:53,155:INFO:Extra Trees Regressor Imported successfully
2025-02-28 13:00:53,159:INFO:Starting cross validation
2025-02-28 13:00:53,160:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:53,697:INFO:Calculating mean and std
2025-02-28 13:00:53,698:INFO:Creating metrics dataframe
2025-02-28 13:00:53,700:INFO:Uploading results into container
2025-02-28 13:00:53,700:INFO:Uploading model into container now
2025-02-28 13:00:53,700:INFO:_master_model_container: 14
2025-02-28 13:00:53,700:INFO:_display_container: 2
2025-02-28 13:00:53,702:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 13:00:53,702:INFO:create_model() successfully completed......................................
2025-02-28 13:00:53,825:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:53,825:INFO:Creating metrics dataframe
2025-02-28 13:00:53,830:INFO:Initializing AdaBoost Regressor
2025-02-28 13:00:53,830:INFO:Total runtime is 0.07182797590891521 minutes
2025-02-28 13:00:53,834:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:53,834:INFO:Initializing create_model()
2025-02-28 13:00:53,834:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:53,834:INFO:Checking exceptions
2025-02-28 13:00:53,834:INFO:Importing libraries
2025-02-28 13:00:53,834:INFO:Copying training dataset
2025-02-28 13:00:53,837:INFO:Defining folds
2025-02-28 13:00:53,837:INFO:Declaring metric variables
2025-02-28 13:00:53,839:INFO:Importing untrained model
2025-02-28 13:00:53,842:INFO:AdaBoost Regressor Imported successfully
2025-02-28 13:00:53,846:INFO:Starting cross validation
2025-02-28 13:00:53,847:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:54,042:INFO:Calculating mean and std
2025-02-28 13:00:54,042:INFO:Creating metrics dataframe
2025-02-28 13:00:54,044:INFO:Uploading results into container
2025-02-28 13:00:54,044:INFO:Uploading model into container now
2025-02-28 13:00:54,044:INFO:_master_model_container: 15
2025-02-28 13:00:54,044:INFO:_display_container: 2
2025-02-28 13:00:54,044:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 13:00:54,044:INFO:create_model() successfully completed......................................
2025-02-28 13:00:54,154:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:54,154:INFO:Creating metrics dataframe
2025-02-28 13:00:54,159:INFO:Initializing Gradient Boosting Regressor
2025-02-28 13:00:54,159:INFO:Total runtime is 0.07730949719746909 minutes
2025-02-28 13:00:54,161:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:54,161:INFO:Initializing create_model()
2025-02-28 13:00:54,161:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:54,161:INFO:Checking exceptions
2025-02-28 13:00:54,161:INFO:Importing libraries
2025-02-28 13:00:54,161:INFO:Copying training dataset
2025-02-28 13:00:54,164:INFO:Defining folds
2025-02-28 13:00:54,164:INFO:Declaring metric variables
2025-02-28 13:00:54,166:INFO:Importing untrained model
2025-02-28 13:00:54,169:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 13:00:54,173:INFO:Starting cross validation
2025-02-28 13:00:54,174:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:54,589:INFO:Calculating mean and std
2025-02-28 13:00:54,589:INFO:Creating metrics dataframe
2025-02-28 13:00:54,590:INFO:Uploading results into container
2025-02-28 13:00:54,591:INFO:Uploading model into container now
2025-02-28 13:00:54,591:INFO:_master_model_container: 16
2025-02-28 13:00:54,591:INFO:_display_container: 2
2025-02-28 13:00:54,591:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 13:00:54,591:INFO:create_model() successfully completed......................................
2025-02-28 13:00:54,702:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:54,702:INFO:Creating metrics dataframe
2025-02-28 13:00:54,708:INFO:Initializing Extreme Gradient Boosting
2025-02-28 13:00:54,708:INFO:Total runtime is 0.08645181258519492 minutes
2025-02-28 13:00:54,709:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:54,710:INFO:Initializing create_model()
2025-02-28 13:00:54,710:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:54,710:INFO:Checking exceptions
2025-02-28 13:00:54,710:INFO:Importing libraries
2025-02-28 13:00:54,710:INFO:Copying training dataset
2025-02-28 13:00:54,712:INFO:Defining folds
2025-02-28 13:00:54,712:INFO:Declaring metric variables
2025-02-28 13:00:54,715:INFO:Importing untrained model
2025-02-28 13:00:54,718:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 13:00:54,722:INFO:Starting cross validation
2025-02-28 13:00:54,723:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:55,196:INFO:Calculating mean and std
2025-02-28 13:00:55,197:INFO:Creating metrics dataframe
2025-02-28 13:00:55,198:INFO:Uploading results into container
2025-02-28 13:00:55,198:INFO:Uploading model into container now
2025-02-28 13:00:55,198:INFO:_master_model_container: 17
2025-02-28 13:00:55,198:INFO:_display_container: 2
2025-02-28 13:00:55,199:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 13:00:55,199:INFO:create_model() successfully completed......................................
2025-02-28 13:00:55,310:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:55,311:INFO:Creating metrics dataframe
2025-02-28 13:00:55,315:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 13:00:55,316:INFO:Total runtime is 0.09658335844675701 minutes
2025-02-28 13:00:55,318:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:55,318:INFO:Initializing create_model()
2025-02-28 13:00:55,318:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:55,318:INFO:Checking exceptions
2025-02-28 13:00:55,318:INFO:Importing libraries
2025-02-28 13:00:55,318:INFO:Copying training dataset
2025-02-28 13:00:55,320:INFO:Defining folds
2025-02-28 13:00:55,321:INFO:Declaring metric variables
2025-02-28 13:00:55,323:INFO:Importing untrained model
2025-02-28 13:00:55,325:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:00:55,329:INFO:Starting cross validation
2025-02-28 13:00:55,330:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:56,716:INFO:Calculating mean and std
2025-02-28 13:00:56,717:INFO:Creating metrics dataframe
2025-02-28 13:00:56,718:INFO:Uploading results into container
2025-02-28 13:00:56,719:INFO:Uploading model into container now
2025-02-28 13:00:56,719:INFO:_master_model_container: 18
2025-02-28 13:00:56,719:INFO:_display_container: 2
2025-02-28 13:00:56,720:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:00:56,720:INFO:create_model() successfully completed......................................
2025-02-28 13:00:56,855:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:56,855:INFO:Creating metrics dataframe
2025-02-28 13:00:56,863:INFO:Initializing CatBoost Regressor
2025-02-28 13:00:56,863:INFO:Total runtime is 0.12236943244934084 minutes
2025-02-28 13:00:56,865:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:56,865:INFO:Initializing create_model()
2025-02-28 13:00:56,865:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:56,865:INFO:Checking exceptions
2025-02-28 13:00:56,865:INFO:Importing libraries
2025-02-28 13:00:56,865:INFO:Copying training dataset
2025-02-28 13:00:56,868:INFO:Defining folds
2025-02-28 13:00:56,868:INFO:Declaring metric variables
2025-02-28 13:00:56,870:INFO:Importing untrained model
2025-02-28 13:00:56,873:INFO:CatBoost Regressor Imported successfully
2025-02-28 13:00:56,877:INFO:Starting cross validation
2025-02-28 13:00:56,878:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:59,657:INFO:Calculating mean and std
2025-02-28 13:00:59,658:INFO:Creating metrics dataframe
2025-02-28 13:00:59,660:INFO:Uploading results into container
2025-02-28 13:00:59,661:INFO:Uploading model into container now
2025-02-28 13:00:59,661:INFO:_master_model_container: 19
2025-02-28 13:00:59,661:INFO:_display_container: 2
2025-02-28 13:00:59,661:INFO:<catboost.core.CatBoostRegressor object at 0x0000021913F031C0>
2025-02-28 13:00:59,662:INFO:create_model() successfully completed......................................
2025-02-28 13:00:59,775:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:59,775:INFO:Creating metrics dataframe
2025-02-28 13:00:59,780:INFO:Initializing Dummy Regressor
2025-02-28 13:00:59,780:INFO:Total runtime is 0.17099305788675945 minutes
2025-02-28 13:00:59,782:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:59,782:INFO:Initializing create_model()
2025-02-28 13:00:59,782:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:59,782:INFO:Checking exceptions
2025-02-28 13:00:59,783:INFO:Importing libraries
2025-02-28 13:00:59,783:INFO:Copying training dataset
2025-02-28 13:00:59,786:INFO:Defining folds
2025-02-28 13:00:59,786:INFO:Declaring metric variables
2025-02-28 13:00:59,789:INFO:Importing untrained model
2025-02-28 13:00:59,791:INFO:Dummy Regressor Imported successfully
2025-02-28 13:00:59,795:INFO:Starting cross validation
2025-02-28 13:00:59,796:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:59,874:INFO:Calculating mean and std
2025-02-28 13:00:59,874:INFO:Creating metrics dataframe
2025-02-28 13:00:59,875:INFO:Uploading results into container
2025-02-28 13:00:59,876:INFO:Uploading model into container now
2025-02-28 13:00:59,876:INFO:_master_model_container: 20
2025-02-28 13:00:59,876:INFO:_display_container: 2
2025-02-28 13:00:59,876:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 13:00:59,876:INFO:create_model() successfully completed......................................
2025-02-28 13:00:59,985:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:59,986:INFO:Creating metrics dataframe
2025-02-28 13:00:59,992:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 13:00:59,997:INFO:Initializing create_model()
2025-02-28 13:00:59,997:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:59,997:INFO:Checking exceptions
2025-02-28 13:00:59,999:INFO:Importing libraries
2025-02-28 13:00:59,999:INFO:Copying training dataset
2025-02-28 13:01:00,002:INFO:Defining folds
2025-02-28 13:01:00,002:INFO:Declaring metric variables
2025-02-28 13:01:00,002:INFO:Importing untrained model
2025-02-28 13:01:00,002:INFO:Declaring custom model
2025-02-28 13:01:00,003:INFO:Dummy Regressor Imported successfully
2025-02-28 13:01:00,003:INFO:Cross validation set to False
2025-02-28 13:01:00,003:INFO:Fitting Model
2025-02-28 13:01:00,025:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 13:01:00,025:INFO:create_model() successfully completed......................................
2025-02-28 13:01:00,153:INFO:_master_model_container: 20
2025-02-28 13:01:00,153:INFO:_display_container: 2
2025-02-28 13:01:00,154:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 13:01:00,154:INFO:compare_models() successfully completed......................................
2025-02-28 13:01:00,191:INFO:Initializing create_model()
2025-02-28 13:01:00,191:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=omp, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:01:00,191:INFO:Checking exceptions
2025-02-28 13:01:00,199:INFO:Importing libraries
2025-02-28 13:01:00,199:INFO:Copying training dataset
2025-02-28 13:01:00,202:INFO:Defining folds
2025-02-28 13:01:00,202:INFO:Declaring metric variables
2025-02-28 13:01:00,204:INFO:Importing untrained model
2025-02-28 13:01:00,207:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 13:01:00,213:INFO:Starting cross validation
2025-02-28 13:01:00,214:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:01:00,294:INFO:Calculating mean and std
2025-02-28 13:01:00,294:INFO:Creating metrics dataframe
2025-02-28 13:01:00,297:INFO:Finalizing model
2025-02-28 13:01:00,322:INFO:Uploading results into container
2025-02-28 13:01:00,322:INFO:Uploading model into container now
2025-02-28 13:01:00,327:INFO:_master_model_container: 21
2025-02-28 13:01:00,327:INFO:_display_container: 3
2025-02-28 13:01:00,327:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 13:01:00,327:INFO:create_model() successfully completed......................................
2025-02-28 13:01:00,458:INFO:Initializing plot_model()
2025-02-28 13:01:00,458:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, system=True)
2025-02-28 13:01:00,458:INFO:Checking exceptions
2025-02-28 13:01:00,461:INFO:Preloading libraries
2025-02-28 13:01:00,461:INFO:Copying training dataset
2025-02-28 13:01:00,461:INFO:Plot type: residuals
2025-02-28 13:01:00,568:INFO:Fitting Model
2025-02-28 13:01:00,568:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 13:01:00,585:INFO:Scoring test/hold-out set
2025-02-28 13:01:00,825:INFO:Visual Rendered Successfully
2025-02-28 13:01:00,941:INFO:plot_model() successfully completed......................................
2025-02-28 13:01:00,957:INFO:Initializing plot_model()
2025-02-28 13:01:00,957:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, system=True)
2025-02-28 13:01:00,957:INFO:Checking exceptions
2025-02-28 13:01:00,959:INFO:Preloading libraries
2025-02-28 13:01:00,959:INFO:Copying training dataset
2025-02-28 13:01:00,960:INFO:Plot type: error
2025-02-28 13:01:01,056:INFO:Fitting Model
2025-02-28 13:01:01,056:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 13:01:01,056:INFO:Scoring test/hold-out set
2025-02-28 13:01:01,163:INFO:Visual Rendered Successfully
2025-02-28 13:01:01,277:INFO:plot_model() successfully completed......................................
2025-02-28 13:01:01,300:INFO:Initializing plot_model()
2025-02-28 13:01:01,300:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, system=True)
2025-02-28 13:01:01,300:INFO:Checking exceptions
2025-02-28 13:01:01,303:INFO:Preloading libraries
2025-02-28 13:01:01,303:INFO:Copying training dataset
2025-02-28 13:01:01,303:INFO:Plot type: feature
2025-02-28 13:01:01,421:INFO:Visual Rendered Successfully
2025-02-28 13:01:01,532:INFO:plot_model() successfully completed......................................
