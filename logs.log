2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:13:04,151:INFO:PyCaret ClassificationExperiment
2025-02-28 12:13:04,151:INFO:Logging name: clf-default-name
2025-02-28 12:13:04,151:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 12:13:04,151:INFO:version 3.3.2
2025-02-28 12:13:04,151:INFO:Initializing setup()
2025-02-28 12:13:04,151:INFO:self.USI: cfb5
2025-02-28 12:13:04,152:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fix_imbalance', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'is_multiclass', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:13:04,152:INFO:Checking environment
2025-02-28 12:13:04,152:INFO:python_version: 3.10.16
2025-02-28 12:13:04,152:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:13:04,152:INFO:machine: AMD64
2025-02-28 12:13:04,152:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:13:04,152:INFO:Memory: svmem(total=34200334336, available=17714921472, percent=48.2, used=16485412864, free=17714921472)
2025-02-28 12:13:04,152:INFO:Physical Core: 24
2025-02-28 12:13:04,152:INFO:Logical Core: 32
2025-02-28 12:13:04,152:INFO:Checking libraries
2025-02-28 12:13:04,152:INFO:System:
2025-02-28 12:13:04,152:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:13:04,152:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:13:04,152:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:13:04,152:INFO:PyCaret required dependencies:
2025-02-28 12:13:07,640:INFO:                 pip: 25.0
2025-02-28 12:13:07,640:INFO:          setuptools: 75.8.0
2025-02-28 12:13:07,640:INFO:             pycaret: 3.3.2
2025-02-28 12:13:07,640:INFO:             IPython: 8.30.0
2025-02-28 12:13:07,640:INFO:          ipywidgets: 8.1.5
2025-02-28 12:13:07,640:INFO:                tqdm: 4.67.1
2025-02-28 12:13:07,640:INFO:               numpy: 1.26.4
2025-02-28 12:13:07,640:INFO:              pandas: 2.1.4
2025-02-28 12:13:07,640:INFO:              jinja2: 3.1.5
2025-02-28 12:13:07,640:INFO:               scipy: 1.11.4
2025-02-28 12:13:07,640:INFO:              joblib: 1.3.2
2025-02-28 12:13:07,640:INFO:             sklearn: 1.4.2
2025-02-28 12:13:07,640:INFO:                pyod: 2.0.3
2025-02-28 12:13:07,640:INFO:            imblearn: 0.13.0
2025-02-28 12:13:07,640:INFO:   category_encoders: 2.7.0
2025-02-28 12:13:07,640:INFO:            lightgbm: 4.5.0
2025-02-28 12:13:07,640:INFO:               numba: 0.61.0
2025-02-28 12:13:07,640:INFO:            requests: 2.32.3
2025-02-28 12:13:07,640:INFO:          matplotlib: 3.7.5
2025-02-28 12:13:07,640:INFO:          scikitplot: 0.3.7
2025-02-28 12:13:07,640:INFO:         yellowbrick: 1.5
2025-02-28 12:13:07,640:INFO:              plotly: 5.24.1
2025-02-28 12:13:07,640:INFO:    plotly-resampler: Not installed
2025-02-28 12:13:07,640:INFO:             kaleido: 0.2.1
2025-02-28 12:13:07,640:INFO:           schemdraw: 0.15
2025-02-28 12:13:07,640:INFO:         statsmodels: 0.14.4
2025-02-28 12:13:07,640:INFO:              sktime: 0.26.0
2025-02-28 12:13:07,640:INFO:               tbats: 1.1.3
2025-02-28 12:13:07,640:INFO:            pmdarima: 2.0.4
2025-02-28 12:13:07,640:INFO:              psutil: 5.9.0
2025-02-28 12:13:07,640:INFO:          markupsafe: 2.1.5
2025-02-28 12:13:07,640:INFO:             pickle5: Not installed
2025-02-28 12:13:07,640:INFO:         cloudpickle: 3.1.1
2025-02-28 12:13:07,640:INFO:         deprecation: 2.1.0
2025-02-28 12:13:07,640:INFO:              xxhash: 3.5.0
2025-02-28 12:13:07,640:INFO:           wurlitzer: Not installed
2025-02-28 12:13:07,640:INFO:PyCaret optional dependencies:
2025-02-28 12:13:15,610:INFO:                shap: 0.44.1
2025-02-28 12:13:15,610:INFO:           interpret: 0.6.9
2025-02-28 12:13:15,610:INFO:                umap: 0.5.7
2025-02-28 12:13:15,610:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:13:15,610:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:13:15,610:INFO:             autoviz: Not installed
2025-02-28 12:13:15,610:INFO:           fairlearn: 0.7.0
2025-02-28 12:13:15,610:INFO:          deepchecks: Not installed
2025-02-28 12:13:15,610:INFO:             xgboost: 2.1.4
2025-02-28 12:13:15,610:INFO:            catboost: 1.2.7
2025-02-28 12:13:15,610:INFO:              kmodes: 0.12.2
2025-02-28 12:13:15,610:INFO:             mlxtend: 0.23.4
2025-02-28 12:13:15,610:INFO:       statsforecast: 1.5.0
2025-02-28 12:13:15,610:INFO:        tune_sklearn: Not installed
2025-02-28 12:13:15,610:INFO:                 ray: Not installed
2025-02-28 12:13:15,610:INFO:            hyperopt: 0.2.7
2025-02-28 12:13:15,610:INFO:              optuna: 4.2.0
2025-02-28 12:13:15,610:INFO:               skopt: 0.10.2
2025-02-28 12:13:15,610:INFO:              mlflow: 2.20.1
2025-02-28 12:13:15,610:INFO:              gradio: 5.15.0
2025-02-28 12:13:15,610:INFO:             fastapi: 0.115.8
2025-02-28 12:13:15,610:INFO:             uvicorn: 0.34.0
2025-02-28 12:13:15,610:INFO:              m2cgen: 0.10.0
2025-02-28 12:13:15,610:INFO:           evidently: 0.4.40
2025-02-28 12:13:15,610:INFO:               fugue: 0.8.7
2025-02-28 12:13:15,610:INFO:           streamlit: Not installed
2025-02-28 12:13:15,610:INFO:             prophet: Not installed
2025-02-28 12:13:15,610:INFO:None
2025-02-28 12:13:15,610:INFO:Set up data.
2025-02-28 12:13:15,621:INFO:Set up folding strategy.
2025-02-28 12:13:15,621:INFO:Set up train/test split.
2025-02-28 12:13:15,630:INFO:Set up index.
2025-02-28 12:13:15,630:INFO:Assigning column types.
2025-02-28 12:13:15,633:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:13:15,652:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:13:15,655:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:15,672:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:15,673:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,024:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,025:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,036:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,038:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,038:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:13:35,058:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,070:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,071:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,091:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,103:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,104:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,105:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 12:13:35,137:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,138:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,170:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,171:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,174:INFO:Preparing preprocessing pipeline...
2025-02-28 12:13:35,175:INFO:Set up simple imputation.
2025-02-28 12:13:35,177:INFO:Set up encoding of ordinal features.
2025-02-28 12:13:35,179:INFO:Set up encoding of categorical features.
2025-02-28 12:13:35,261:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:13:35,271:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score',
                                             'Starting_Salary',
                                             'Career_Sa...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Student_ID'],
                                    transformer=TargetEncoder(cols=['Student_ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:13:35,271:INFO:Creating final display dataframe.
2025-02-28 12:13:35,468:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 20)
4        Transformed data shape        (5000, 31)
5   Transformed train set shape        (3500, 31)
6    Transformed test set shape        (1500, 31)
7              Numeric features                14
8          Categorical features                 5
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              cfb5
2025-02-28 12:13:35,505:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,506:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,546:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,547:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,548:INFO:setup() successfully completed in 31.4s...............
2025-02-28 12:14:03,115:INFO:Initializing compare_models()
2025-02-28 12:14:03,116:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 12:14:03,116:INFO:Checking exceptions
2025-02-28 12:14:03,119:INFO:Preparing display monitor
2025-02-28 12:14:03,133:INFO:Initializing Logistic Regression
2025-02-28 12:14:03,133:INFO:Total runtime is 0.0 minutes
2025-02-28 12:14:03,135:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:03,135:INFO:Initializing create_model()
2025-02-28 12:14:03,135:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:03,135:INFO:Checking exceptions
2025-02-28 12:14:03,135:INFO:Importing libraries
2025-02-28 12:14:03,135:INFO:Copying training dataset
2025-02-28 12:14:03,139:INFO:Defining folds
2025-02-28 12:14:03,139:INFO:Declaring metric variables
2025-02-28 12:14:03,141:INFO:Importing untrained model
2025-02-28 12:14:03,143:INFO:Logistic Regression Imported successfully
2025-02-28 12:14:03,148:INFO:Starting cross validation
2025-02-28 12:14:03,150:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:06,874:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,890:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,902:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,908:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,910:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,919:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,925:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,928:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,928:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,935:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,937:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,939:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,941:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,947:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,949:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,951:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,954:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,961:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,970:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,977:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,995:INFO:Calculating mean and std
2025-02-28 12:14:06,996:INFO:Creating metrics dataframe
2025-02-28 12:14:06,998:INFO:Uploading results into container
2025-02-28 12:14:06,999:INFO:Uploading model into container now
2025-02-28 12:14:06,999:INFO:_master_model_container: 1
2025-02-28 12:14:06,999:INFO:_display_container: 2
2025-02-28 12:14:06,999:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 12:14:06,999:INFO:create_model() successfully completed......................................
2025-02-28 12:14:07,116:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:07,116:INFO:Creating metrics dataframe
2025-02-28 12:14:07,119:INFO:Initializing K Neighbors Classifier
2025-02-28 12:14:07,119:INFO:Total runtime is 0.06642432610193888 minutes
2025-02-28 12:14:07,121:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:07,121:INFO:Initializing create_model()
2025-02-28 12:14:07,121:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:07,122:INFO:Checking exceptions
2025-02-28 12:14:07,122:INFO:Importing libraries
2025-02-28 12:14:07,122:INFO:Copying training dataset
2025-02-28 12:14:07,125:INFO:Defining folds
2025-02-28 12:14:07,125:INFO:Declaring metric variables
2025-02-28 12:14:07,128:INFO:Importing untrained model
2025-02-28 12:14:07,130:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:14:07,134:INFO:Starting cross validation
2025-02-28 12:14:07,135:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:10,005:INFO:Calculating mean and std
2025-02-28 12:14:10,006:INFO:Creating metrics dataframe
2025-02-28 12:14:10,007:INFO:Uploading results into container
2025-02-28 12:14:10,008:INFO:Uploading model into container now
2025-02-28 12:14:10,008:INFO:_master_model_container: 2
2025-02-28 12:14:10,008:INFO:_display_container: 2
2025-02-28 12:14:10,008:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:14:10,008:INFO:create_model() successfully completed......................................
2025-02-28 12:14:10,122:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:10,122:INFO:Creating metrics dataframe
2025-02-28 12:14:10,126:INFO:Initializing Naive Bayes
2025-02-28 12:14:10,126:INFO:Total runtime is 0.11655208269755045 minutes
2025-02-28 12:14:10,128:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:10,128:INFO:Initializing create_model()
2025-02-28 12:14:10,128:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:10,128:INFO:Checking exceptions
2025-02-28 12:14:10,129:INFO:Importing libraries
2025-02-28 12:14:10,129:INFO:Copying training dataset
2025-02-28 12:14:10,132:INFO:Defining folds
2025-02-28 12:14:10,132:INFO:Declaring metric variables
2025-02-28 12:14:10,135:INFO:Importing untrained model
2025-02-28 12:14:10,137:INFO:Naive Bayes Imported successfully
2025-02-28 12:14:10,141:INFO:Starting cross validation
2025-02-28 12:14:10,142:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:13,006:INFO:Calculating mean and std
2025-02-28 12:14:13,007:INFO:Creating metrics dataframe
2025-02-28 12:14:13,008:INFO:Uploading results into container
2025-02-28 12:14:13,009:INFO:Uploading model into container now
2025-02-28 12:14:13,009:INFO:_master_model_container: 3
2025-02-28 12:14:13,009:INFO:_display_container: 2
2025-02-28 12:14:13,009:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 12:14:13,010:INFO:create_model() successfully completed......................................
2025-02-28 12:14:13,123:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:13,123:INFO:Creating metrics dataframe
2025-02-28 12:14:13,126:INFO:Initializing Decision Tree Classifier
2025-02-28 12:14:13,127:INFO:Total runtime is 0.16656994024912516 minutes
2025-02-28 12:14:13,129:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:13,129:INFO:Initializing create_model()
2025-02-28 12:14:13,129:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:13,129:INFO:Checking exceptions
2025-02-28 12:14:13,129:INFO:Importing libraries
2025-02-28 12:14:13,129:INFO:Copying training dataset
2025-02-28 12:14:13,134:INFO:Defining folds
2025-02-28 12:14:13,134:INFO:Declaring metric variables
2025-02-28 12:14:13,137:INFO:Importing untrained model
2025-02-28 12:14:13,139:INFO:Decision Tree Classifier Imported successfully
2025-02-28 12:14:13,143:INFO:Starting cross validation
2025-02-28 12:14:13,145:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:13,239:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,241:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,246:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,251:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,251:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,254:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,255:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,255:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,162:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,164:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,178:INFO:Calculating mean and std
2025-02-28 12:14:15,179:INFO:Creating metrics dataframe
2025-02-28 12:14:15,180:INFO:Uploading results into container
2025-02-28 12:14:15,180:INFO:Uploading model into container now
2025-02-28 12:14:15,180:INFO:_master_model_container: 4
2025-02-28 12:14:15,181:INFO:_display_container: 2
2025-02-28 12:14:15,181:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:14:15,181:INFO:create_model() successfully completed......................................
2025-02-28 12:14:15,290:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:15,290:INFO:Creating metrics dataframe
2025-02-28 12:14:15,296:INFO:Initializing SVM - Linear Kernel
2025-02-28 12:14:15,296:INFO:Total runtime is 0.20270905494689942 minutes
2025-02-28 12:14:15,298:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:15,298:INFO:Initializing create_model()
2025-02-28 12:14:15,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:15,298:INFO:Checking exceptions
2025-02-28 12:14:15,298:INFO:Importing libraries
2025-02-28 12:14:15,298:INFO:Copying training dataset
2025-02-28 12:14:15,301:INFO:Defining folds
2025-02-28 12:14:15,301:INFO:Declaring metric variables
2025-02-28 12:14:15,304:INFO:Importing untrained model
2025-02-28 12:14:15,306:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 12:14:15,311:INFO:Starting cross validation
2025-02-28 12:14:15,312:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:15,467:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,472:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,475:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,476:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,477:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,478:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,478:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,480:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,480:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,481:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,489:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,489:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,491:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,491:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,501:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,501:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,502:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,504:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,504:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,505:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,508:INFO:Calculating mean and std
2025-02-28 12:14:15,508:INFO:Creating metrics dataframe
2025-02-28 12:14:15,510:INFO:Uploading results into container
2025-02-28 12:14:15,510:INFO:Uploading model into container now
2025-02-28 12:14:15,511:INFO:_master_model_container: 5
2025-02-28 12:14:15,511:INFO:_display_container: 2
2025-02-28 12:14:15,511:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:14:15,511:INFO:create_model() successfully completed......................................
2025-02-28 12:14:15,619:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:15,620:INFO:Creating metrics dataframe
2025-02-28 12:14:15,624:INFO:Initializing Ridge Classifier
2025-02-28 12:14:15,624:INFO:Total runtime is 0.20817364851633707 minutes
2025-02-28 12:14:15,627:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:15,627:INFO:Initializing create_model()
2025-02-28 12:14:15,627:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:15,627:INFO:Checking exceptions
2025-02-28 12:14:15,627:INFO:Importing libraries
2025-02-28 12:14:15,627:INFO:Copying training dataset
2025-02-28 12:14:15,630:INFO:Defining folds
2025-02-28 12:14:15,630:INFO:Declaring metric variables
2025-02-28 12:14:15,632:INFO:Importing untrained model
2025-02-28 12:14:15,635:INFO:Ridge Classifier Imported successfully
2025-02-28 12:14:15,639:INFO:Starting cross validation
2025-02-28 12:14:15,640:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:15,744:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,745:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,747:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,748:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,748:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,749:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,750:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,751:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,754:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,757:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,777:INFO:Calculating mean and std
2025-02-28 12:14:15,778:INFO:Creating metrics dataframe
2025-02-28 12:14:15,779:INFO:Uploading results into container
2025-02-28 12:14:15,779:INFO:Uploading model into container now
2025-02-28 12:14:15,780:INFO:_master_model_container: 6
2025-02-28 12:14:15,780:INFO:_display_container: 2
2025-02-28 12:14:15,780:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 12:14:15,780:INFO:create_model() successfully completed......................................
2025-02-28 12:14:15,889:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:15,890:INFO:Creating metrics dataframe
2025-02-28 12:14:15,894:INFO:Initializing Random Forest Classifier
2025-02-28 12:14:15,894:INFO:Total runtime is 0.2126800020535787 minutes
2025-02-28 12:14:15,896:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:15,896:INFO:Initializing create_model()
2025-02-28 12:14:15,896:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:15,897:INFO:Checking exceptions
2025-02-28 12:14:15,897:INFO:Importing libraries
2025-02-28 12:14:15,897:INFO:Copying training dataset
2025-02-28 12:14:15,900:INFO:Defining folds
2025-02-28 12:14:15,900:INFO:Declaring metric variables
2025-02-28 12:14:15,903:INFO:Importing untrained model
2025-02-28 12:14:15,906:INFO:Random Forest Classifier Imported successfully
2025-02-28 12:14:15,910:INFO:Starting cross validation
2025-02-28 12:14:15,911:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:16,256:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,257:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,284:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,300:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,361:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,370:INFO:Calculating mean and std
2025-02-28 12:14:16,371:INFO:Creating metrics dataframe
2025-02-28 12:14:16,372:INFO:Uploading results into container
2025-02-28 12:14:16,373:INFO:Uploading model into container now
2025-02-28 12:14:16,373:INFO:_master_model_container: 7
2025-02-28 12:14:16,373:INFO:_display_container: 2
2025-02-28 12:14:16,373:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 12:14:16,373:INFO:create_model() successfully completed......................................
2025-02-28 12:14:16,489:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:16,489:INFO:Creating metrics dataframe
2025-02-28 12:14:16,494:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 12:14:16,494:INFO:Total runtime is 0.2226726253827413 minutes
2025-02-28 12:14:16,496:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:16,496:INFO:Initializing create_model()
2025-02-28 12:14:16,496:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:16,496:INFO:Checking exceptions
2025-02-28 12:14:16,496:INFO:Importing libraries
2025-02-28 12:14:16,496:INFO:Copying training dataset
2025-02-28 12:14:16,500:INFO:Defining folds
2025-02-28 12:14:16,500:INFO:Declaring metric variables
2025-02-28 12:14:16,502:INFO:Importing untrained model
2025-02-28 12:14:16,504:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 12:14:16,508:INFO:Starting cross validation
2025-02-28 12:14:16,509:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,596:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,600:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,613:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,613:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,618:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,619:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,620:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,621:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,622:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,623:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,626:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,628:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,631:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,647:INFO:Calculating mean and std
2025-02-28 12:14:16,647:INFO:Creating metrics dataframe
2025-02-28 12:14:16,648:INFO:Uploading results into container
2025-02-28 12:14:16,648:INFO:Uploading model into container now
2025-02-28 12:14:16,649:INFO:_master_model_container: 8
2025-02-28 12:14:16,649:INFO:_display_container: 2
2025-02-28 12:14:16,649:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 12:14:16,649:INFO:create_model() successfully completed......................................
2025-02-28 12:14:16,764:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:16,764:INFO:Creating metrics dataframe
2025-02-28 12:14:16,769:INFO:Initializing Ada Boost Classifier
2025-02-28 12:14:16,770:INFO:Total runtime is 0.22727389335632325 minutes
2025-02-28 12:14:16,771:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:16,771:INFO:Initializing create_model()
2025-02-28 12:14:16,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:16,772:INFO:Checking exceptions
2025-02-28 12:14:16,772:INFO:Importing libraries
2025-02-28 12:14:16,772:INFO:Copying training dataset
2025-02-28 12:14:16,775:INFO:Defining folds
2025-02-28 12:14:16,776:INFO:Declaring metric variables
2025-02-28 12:14:16,778:INFO:Importing untrained model
2025-02-28 12:14:16,780:INFO:Ada Boost Classifier Imported successfully
2025-02-28 12:14:16,784:INFO:Starting cross validation
2025-02-28 12:14:16,785:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:16,854:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,856:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,857:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,863:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,863:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,864:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,865:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,876:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:17,033:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,036:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,037:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,040:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,041:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,041:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,042:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,043:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,044:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,046:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,048:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,051:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,062:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,064:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,066:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,066:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,068:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,068:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,083:INFO:Calculating mean and std
2025-02-28 12:14:17,084:INFO:Creating metrics dataframe
2025-02-28 12:14:17,085:INFO:Uploading results into container
2025-02-28 12:14:17,085:INFO:Uploading model into container now
2025-02-28 12:14:17,085:INFO:_master_model_container: 9
2025-02-28 12:14:17,086:INFO:_display_container: 2
2025-02-28 12:14:17,086:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 12:14:17,086:INFO:create_model() successfully completed......................................
2025-02-28 12:14:17,201:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:17,201:INFO:Creating metrics dataframe
2025-02-28 12:14:17,206:INFO:Initializing Gradient Boosting Classifier
2025-02-28 12:14:17,206:INFO:Total runtime is 0.23455353180567426 minutes
2025-02-28 12:14:17,208:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:17,208:INFO:Initializing create_model()
2025-02-28 12:14:17,208:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:17,208:INFO:Checking exceptions
2025-02-28 12:14:17,208:INFO:Importing libraries
2025-02-28 12:14:17,208:INFO:Copying training dataset
2025-02-28 12:14:17,212:INFO:Defining folds
2025-02-28 12:14:17,212:INFO:Declaring metric variables
2025-02-28 12:14:17,215:INFO:Importing untrained model
2025-02-28 12:14:17,217:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 12:14:17,222:INFO:Starting cross validation
2025-02-28 12:14:17,223:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:19,280:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,284:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,317:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,320:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,321:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,323:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,323:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,325:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,325:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,328:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,333:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,335:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,357:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,359:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,365:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,367:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,382:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,384:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,390:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,393:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,406:INFO:Calculating mean and std
2025-02-28 12:14:19,407:INFO:Creating metrics dataframe
2025-02-28 12:14:19,408:INFO:Uploading results into container
2025-02-28 12:14:19,408:INFO:Uploading model into container now
2025-02-28 12:14:19,408:INFO:_master_model_container: 10
2025-02-28 12:14:19,408:INFO:_display_container: 2
2025-02-28 12:14:19,409:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:14:19,409:INFO:create_model() successfully completed......................................
2025-02-28 12:14:19,519:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:19,519:INFO:Creating metrics dataframe
2025-02-28 12:14:19,523:INFO:Initializing Linear Discriminant Analysis
2025-02-28 12:14:19,524:INFO:Total runtime is 0.27318072716395064 minutes
2025-02-28 12:14:19,526:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:19,526:INFO:Initializing create_model()
2025-02-28 12:14:19,526:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:19,526:INFO:Checking exceptions
2025-02-28 12:14:19,526:INFO:Importing libraries
2025-02-28 12:14:19,526:INFO:Copying training dataset
2025-02-28 12:14:19,530:INFO:Defining folds
2025-02-28 12:14:19,530:INFO:Declaring metric variables
2025-02-28 12:14:19,533:INFO:Importing untrained model
2025-02-28 12:14:19,535:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 12:14:19,540:INFO:Starting cross validation
2025-02-28 12:14:19,541:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:19,636:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,637:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,638:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,638:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,639:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,639:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,640:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,641:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,646:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,646:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,648:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,649:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,653:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,654:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,655:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,656:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,658:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,659:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,661:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,662:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,674:INFO:Calculating mean and std
2025-02-28 12:14:19,675:INFO:Creating metrics dataframe
2025-02-28 12:14:19,676:INFO:Uploading results into container
2025-02-28 12:14:19,676:INFO:Uploading model into container now
2025-02-28 12:14:19,676:INFO:_master_model_container: 11
2025-02-28 12:14:19,676:INFO:_display_container: 2
2025-02-28 12:14:19,677:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 12:14:19,677:INFO:create_model() successfully completed......................................
2025-02-28 12:14:19,784:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:19,784:INFO:Creating metrics dataframe
2025-02-28 12:14:19,789:INFO:Initializing Extra Trees Classifier
2025-02-28 12:14:19,789:INFO:Total runtime is 0.27759943803151454 minutes
2025-02-28 12:14:19,791:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:19,792:INFO:Initializing create_model()
2025-02-28 12:14:19,792:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:19,792:INFO:Checking exceptions
2025-02-28 12:14:19,792:INFO:Importing libraries
2025-02-28 12:14:19,792:INFO:Copying training dataset
2025-02-28 12:14:19,795:INFO:Defining folds
2025-02-28 12:14:19,795:INFO:Declaring metric variables
2025-02-28 12:14:19,797:INFO:Importing untrained model
2025-02-28 12:14:19,799:INFO:Extra Trees Classifier Imported successfully
2025-02-28 12:14:19,804:INFO:Starting cross validation
2025-02-28 12:14:19,806:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:20,119:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,119:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,121:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,134:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,134:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,135:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,149:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,158:INFO:Calculating mean and std
2025-02-28 12:14:20,159:INFO:Creating metrics dataframe
2025-02-28 12:14:20,160:INFO:Uploading results into container
2025-02-28 12:14:20,160:INFO:Uploading model into container now
2025-02-28 12:14:20,160:INFO:_master_model_container: 12
2025-02-28 12:14:20,160:INFO:_display_container: 2
2025-02-28 12:14:20,161:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 12:14:20,161:INFO:create_model() successfully completed......................................
2025-02-28 12:14:20,270:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:20,270:INFO:Creating metrics dataframe
2025-02-28 12:14:20,276:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:14:20,276:INFO:Total runtime is 0.28570960362752285 minutes
2025-02-28 12:14:20,278:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:20,279:INFO:Initializing create_model()
2025-02-28 12:14:20,279:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:20,279:INFO:Checking exceptions
2025-02-28 12:14:20,279:INFO:Importing libraries
2025-02-28 12:14:20,279:INFO:Copying training dataset
2025-02-28 12:14:20,281:INFO:Defining folds
2025-02-28 12:14:20,281:INFO:Declaring metric variables
2025-02-28 12:14:20,284:INFO:Importing untrained model
2025-02-28 12:14:20,287:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:14:20,291:INFO:Starting cross validation
2025-02-28 12:14:20,292:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:20,910:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,925:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,926:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,932:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,938:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,943:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,965:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,977:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,981:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,986:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,998:INFO:Calculating mean and std
2025-02-28 12:14:20,999:INFO:Creating metrics dataframe
2025-02-28 12:14:21,000:INFO:Uploading results into container
2025-02-28 12:14:21,000:INFO:Uploading model into container now
2025-02-28 12:14:21,001:INFO:_master_model_container: 13
2025-02-28 12:14:21,001:INFO:_display_container: 2
2025-02-28 12:14:21,001:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 12:14:21,001:INFO:create_model() successfully completed......................................
2025-02-28 12:14:21,110:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:21,110:INFO:Creating metrics dataframe
2025-02-28 12:14:21,115:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:14:21,116:INFO:Total runtime is 0.2997180938720704 minutes
2025-02-28 12:14:21,118:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:21,118:INFO:Initializing create_model()
2025-02-28 12:14:21,118:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:21,118:INFO:Checking exceptions
2025-02-28 12:14:21,118:INFO:Importing libraries
2025-02-28 12:14:21,118:INFO:Copying training dataset
2025-02-28 12:14:21,122:INFO:Defining folds
2025-02-28 12:14:21,122:INFO:Declaring metric variables
2025-02-28 12:14:21,125:INFO:Importing untrained model
2025-02-28 12:14:21,127:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:14:21,131:INFO:Starting cross validation
2025-02-28 12:14:21,132:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:24,555:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,579:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,626:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,664:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,665:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,713:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,759:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,783:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,988:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:25,007:INFO:Calculating mean and std
2025-02-28 12:14:25,008:INFO:Creating metrics dataframe
2025-02-28 12:14:25,009:INFO:Uploading results into container
2025-02-28 12:14:25,009:INFO:Uploading model into container now
2025-02-28 12:14:25,010:INFO:_master_model_container: 14
2025-02-28 12:14:25,010:INFO:_display_container: 2
2025-02-28 12:14:25,010:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:14:25,010:INFO:create_model() successfully completed......................................
2025-02-28 12:14:25,131:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:25,131:INFO:Creating metrics dataframe
2025-02-28 12:14:25,137:INFO:Initializing CatBoost Classifier
2025-02-28 12:14:25,137:INFO:Total runtime is 0.36673409541447965 minutes
2025-02-28 12:14:25,139:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:25,139:INFO:Initializing create_model()
2025-02-28 12:14:25,139:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:25,139:INFO:Checking exceptions
2025-02-28 12:14:25,139:INFO:Importing libraries
2025-02-28 12:14:25,139:INFO:Copying training dataset
2025-02-28 12:14:25,143:INFO:Defining folds
2025-02-28 12:14:25,143:INFO:Declaring metric variables
2025-02-28 12:14:25,146:INFO:Importing untrained model
2025-02-28 12:14:25,148:INFO:CatBoost Classifier Imported successfully
2025-02-28 12:14:25,153:INFO:Starting cross validation
2025-02-28 12:14:25,154:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:32,425:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,430:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,432:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,442:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,448:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,454:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,454:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,456:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,457:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,461:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
1 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\catboost\core.py", line 5245, in fit
    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\catboost\core.py", line 2410, in _fit
    self._train(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\catboost\core.py", line 1790, in _train
    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)
  File "_catboost.pyx", line 5017, in _catboost._CatBoost._train
  File "_catboost.pyx", line 5066, in _catboost._CatBoost._train
_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:26: Can't create train tmp dir: tmp

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-02-28 12:14:32,461:INFO:Calculating mean and std
2025-02-28 12:14:32,462:INFO:Creating metrics dataframe
2025-02-28 12:14:32,462:INFO:Uploading results into container
2025-02-28 12:14:32,464:INFO:Uploading model into container now
2025-02-28 12:14:32,464:INFO:_master_model_container: 15
2025-02-28 12:14:32,464:INFO:_display_container: 2
2025-02-28 12:14:32,464:INFO:<catboost.core.CatBoostClassifier object at 0x0000024E59816C80>
2025-02-28 12:14:32,464:INFO:create_model() successfully completed......................................
2025-02-28 12:14:32,569:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:32,569:INFO:Creating metrics dataframe
2025-02-28 12:14:32,576:INFO:Initializing Dummy Classifier
2025-02-28 12:14:32,576:INFO:Total runtime is 0.49071048498153697 minutes
2025-02-28 12:14:32,578:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:32,579:INFO:Initializing create_model()
2025-02-28 12:14:32,579:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:32,579:INFO:Checking exceptions
2025-02-28 12:14:32,579:INFO:Importing libraries
2025-02-28 12:14:32,579:INFO:Copying training dataset
2025-02-28 12:14:32,581:INFO:Defining folds
2025-02-28 12:14:32,582:INFO:Declaring metric variables
2025-02-28 12:14:32,584:INFO:Importing untrained model
2025-02-28 12:14:32,587:INFO:Dummy Classifier Imported successfully
2025-02-28 12:14:32,591:INFO:Starting cross validation
2025-02-28 12:14:32,592:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:32,677:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,685:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,688:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,689:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,696:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,699:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,707:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,710:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,710:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,714:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,726:INFO:Calculating mean and std
2025-02-28 12:14:32,727:INFO:Creating metrics dataframe
2025-02-28 12:14:32,728:INFO:Uploading results into container
2025-02-28 12:14:32,728:INFO:Uploading model into container now
2025-02-28 12:14:32,728:INFO:_master_model_container: 16
2025-02-28 12:14:32,729:INFO:_display_container: 2
2025-02-28 12:14:32,729:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 12:14:32,729:INFO:create_model() successfully completed......................................
2025-02-28 12:14:32,835:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:32,835:INFO:Creating metrics dataframe
2025-02-28 12:14:32,842:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:14:32,847:INFO:Initializing create_model()
2025-02-28 12:14:32,847:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:32,847:INFO:Checking exceptions
2025-02-28 12:14:32,848:INFO:Importing libraries
2025-02-28 12:14:32,848:INFO:Copying training dataset
2025-02-28 12:14:32,854:INFO:Defining folds
2025-02-28 12:14:32,854:INFO:Declaring metric variables
2025-02-28 12:14:32,854:INFO:Importing untrained model
2025-02-28 12:14:32,854:INFO:Declaring custom model
2025-02-28 12:14:32,854:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:14:32,856:INFO:Cross validation set to False
2025-02-28 12:14:32,856:INFO:Fitting Model
2025-02-28 12:14:32,905:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:14:32,905:INFO:create_model() successfully completed......................................
2025-02-28 12:14:33,026:INFO:_master_model_container: 16
2025-02-28 12:14:33,026:INFO:_display_container: 2
2025-02-28 12:14:33,026:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:14:33,026:INFO:compare_models() successfully completed......................................
2025-02-28 12:17:20,901:INFO:Initializing plot_model()
2025-02-28 12:17:20,901:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, system=True)
2025-02-28 12:17:20,901:INFO:Checking exceptions
2025-02-28 12:17:20,905:INFO:Preloading libraries
2025-02-28 12:17:20,906:INFO:Copying training dataset
2025-02-28 12:17:20,906:INFO:Plot type: confusion_matrix
2025-02-28 12:17:21,114:INFO:Fitting Model
2025-02-28 12:17:21,115:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:17:21,116:INFO:Scoring test/hold-out set
2025-02-28 12:17:21,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\externals\loky\backend\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:
[WinError 2] The system cannot find the file specified
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(

2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\externals\loky\backend\context.py", line 257, in _count_physical_cores
2025-02-28 12:17:21,121:WARNING:    cpu_info = subprocess.run(
2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\subprocess.py", line 503, in run
2025-02-28 12:17:21,121:WARNING:    with Popen(*popenargs, **kwargs) as process:
2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\subprocess.py", line 971, in __init__
2025-02-28 12:17:21,121:WARNING:    self._execute_child(args, executable, preexec_fn, close_fds,
2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\subprocess.py", line 1456, in _execute_child
2025-02-28 12:17:21,121:WARNING:    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
2025-02-28 12:17:21,491:INFO:Visual Rendered Successfully
2025-02-28 12:17:21,611:INFO:plot_model() successfully completed......................................
2025-02-28 12:17:34,242:INFO:Initializing plot_model()
2025-02-28 12:17:34,242:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, system=True)
2025-02-28 12:17:34,243:INFO:Checking exceptions
2025-02-28 12:17:34,246:INFO:Preloading libraries
2025-02-28 12:17:34,246:INFO:Copying training dataset
2025-02-28 12:17:34,246:INFO:Plot type: auc
2025-02-28 12:17:34,444:INFO:Fitting Model
2025-02-28 12:17:34,444:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:17:34,444:INFO:Scoring test/hold-out set
2025-02-28 12:17:34,773:INFO:Visual Rendered Successfully
2025-02-28 12:17:34,881:INFO:plot_model() successfully completed......................................
2025-02-28 12:17:43,719:INFO:Initializing plot_model()
2025-02-28 12:17:43,720:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, system=True)
2025-02-28 12:17:43,720:INFO:Checking exceptions
2025-02-28 12:19:23,147:INFO:PyCaret RegressionExperiment
2025-02-28 12:19:23,147:INFO:Logging name: reg-default-name
2025-02-28 12:19:23,147:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 12:19:23,147:INFO:version 3.3.2
2025-02-28 12:19:23,147:INFO:Initializing setup()
2025-02-28 12:19:23,147:INFO:self.USI: eab2
2025-02-28 12:19:23,148:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'transform_target_param', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:19:23,148:INFO:Checking environment
2025-02-28 12:19:23,148:INFO:python_version: 3.10.16
2025-02-28 12:19:23,148:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:19:23,148:INFO:machine: AMD64
2025-02-28 12:19:23,148:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:19:23,148:INFO:Memory: svmem(total=34200334336, available=12409036800, percent=63.7, used=21791297536, free=12409036800)
2025-02-28 12:19:23,148:INFO:Physical Core: 24
2025-02-28 12:19:23,148:INFO:Logical Core: 32
2025-02-28 12:19:23,148:INFO:Checking libraries
2025-02-28 12:19:23,148:INFO:System:
2025-02-28 12:19:23,148:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:19:23,148:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:19:23,148:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:19:23,148:INFO:PyCaret required dependencies:
2025-02-28 12:19:23,148:INFO:                 pip: 25.0
2025-02-28 12:19:23,148:INFO:          setuptools: 75.8.0
2025-02-28 12:19:23,148:INFO:             pycaret: 3.3.2
2025-02-28 12:19:23,148:INFO:             IPython: 8.30.0
2025-02-28 12:19:23,148:INFO:          ipywidgets: 8.1.5
2025-02-28 12:19:23,148:INFO:                tqdm: 4.67.1
2025-02-28 12:19:23,149:INFO:               numpy: 1.26.4
2025-02-28 12:19:23,149:INFO:              pandas: 2.1.4
2025-02-28 12:19:23,149:INFO:              jinja2: 3.1.5
2025-02-28 12:19:23,149:INFO:               scipy: 1.11.4
2025-02-28 12:19:23,149:INFO:              joblib: 1.3.2
2025-02-28 12:19:23,149:INFO:             sklearn: 1.4.2
2025-02-28 12:19:23,149:INFO:                pyod: 2.0.3
2025-02-28 12:19:23,149:INFO:            imblearn: 0.13.0
2025-02-28 12:19:23,149:INFO:   category_encoders: 2.7.0
2025-02-28 12:19:23,149:INFO:            lightgbm: 4.5.0
2025-02-28 12:19:23,149:INFO:               numba: 0.61.0
2025-02-28 12:19:23,149:INFO:            requests: 2.32.3
2025-02-28 12:19:23,149:INFO:          matplotlib: 3.7.5
2025-02-28 12:19:23,149:INFO:          scikitplot: 0.3.7
2025-02-28 12:19:23,149:INFO:         yellowbrick: 1.5
2025-02-28 12:19:23,149:INFO:              plotly: 5.24.1
2025-02-28 12:19:23,149:INFO:    plotly-resampler: Not installed
2025-02-28 12:19:23,149:INFO:             kaleido: 0.2.1
2025-02-28 12:19:23,149:INFO:           schemdraw: 0.15
2025-02-28 12:19:23,149:INFO:         statsmodels: 0.14.4
2025-02-28 12:19:23,149:INFO:              sktime: 0.26.0
2025-02-28 12:19:23,149:INFO:               tbats: 1.1.3
2025-02-28 12:19:23,149:INFO:            pmdarima: 2.0.4
2025-02-28 12:19:23,149:INFO:              psutil: 5.9.0
2025-02-28 12:19:23,149:INFO:          markupsafe: 2.1.5
2025-02-28 12:19:23,149:INFO:             pickle5: Not installed
2025-02-28 12:19:23,149:INFO:         cloudpickle: 3.1.1
2025-02-28 12:19:23,149:INFO:         deprecation: 2.1.0
2025-02-28 12:19:23,149:INFO:              xxhash: 3.5.0
2025-02-28 12:19:23,149:INFO:           wurlitzer: Not installed
2025-02-28 12:19:23,149:INFO:PyCaret optional dependencies:
2025-02-28 12:19:23,149:INFO:                shap: 0.44.1
2025-02-28 12:19:23,149:INFO:           interpret: 0.6.9
2025-02-28 12:19:23,149:INFO:                umap: 0.5.7
2025-02-28 12:19:23,149:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:19:23,149:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:19:23,149:INFO:             autoviz: Not installed
2025-02-28 12:19:23,149:INFO:           fairlearn: 0.7.0
2025-02-28 12:19:23,149:INFO:          deepchecks: Not installed
2025-02-28 12:19:23,149:INFO:             xgboost: 2.1.4
2025-02-28 12:19:23,150:INFO:            catboost: 1.2.7
2025-02-28 12:19:23,150:INFO:              kmodes: 0.12.2
2025-02-28 12:19:23,150:INFO:             mlxtend: 0.23.4
2025-02-28 12:19:23,150:INFO:       statsforecast: 1.5.0
2025-02-28 12:19:23,150:INFO:        tune_sklearn: Not installed
2025-02-28 12:19:23,150:INFO:                 ray: Not installed
2025-02-28 12:19:23,150:INFO:            hyperopt: 0.2.7
2025-02-28 12:19:23,150:INFO:              optuna: 4.2.0
2025-02-28 12:19:23,150:INFO:               skopt: 0.10.2
2025-02-28 12:19:23,150:INFO:              mlflow: 2.20.1
2025-02-28 12:19:23,150:INFO:              gradio: 5.15.0
2025-02-28 12:19:23,150:INFO:             fastapi: 0.115.8
2025-02-28 12:19:23,150:INFO:             uvicorn: 0.34.0
2025-02-28 12:19:23,150:INFO:              m2cgen: 0.10.0
2025-02-28 12:19:23,150:INFO:           evidently: 0.4.40
2025-02-28 12:19:23,150:INFO:               fugue: 0.8.7
2025-02-28 12:19:23,150:INFO:           streamlit: Not installed
2025-02-28 12:19:23,150:INFO:             prophet: Not installed
2025-02-28 12:19:23,150:INFO:None
2025-02-28 12:19:23,150:INFO:Set up data.
2025-02-28 12:19:23,156:INFO:Set up folding strategy.
2025-02-28 12:19:23,157:INFO:Set up train/test split.
2025-02-28 12:19:23,160:INFO:Set up index.
2025-02-28 12:19:23,161:INFO:Assigning column types.
2025-02-28 12:19:23,163:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:19:23,163:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,165:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,167:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,193:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,213:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,213:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,214:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,215:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,217:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,219:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,245:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,264:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,265:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,266:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,266:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 12:19:23,268:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,270:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,296:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,315:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,315:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,317:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,319:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,321:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,346:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,365:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,366:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,367:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,367:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 12:19:23,372:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,396:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,415:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,415:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,416:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,420:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,445:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,465:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,465:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,466:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,467:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 12:19:23,497:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,516:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,517:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,518:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,548:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,567:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,567:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,568:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,569:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:19:23,599:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,618:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,619:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,649:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,668:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,670:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,670:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 12:19:23,719:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,720:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,768:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,769:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,772:INFO:Preparing preprocessing pipeline...
2025-02-28 12:19:23,772:INFO:Set up simple imputation.
2025-02-28 12:19:23,774:INFO:Set up encoding of ordinal features.
2025-02-28 12:19:23,775:INFO:Set up encoding of categorical features.
2025-02-28 12:19:23,846:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:19:23,854:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score', 'Job_Offers',
                                             'Career_Satisfa...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Student_ID'],
                                    transformer=TargetEncoder(cols=['Student_ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:19:23,854:INFO:Creating final display dataframe.
2025-02-28 12:19:24,054:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 20)
4        Transformed data shape        (5000, 31)
5   Transformed train set shape        (3500, 31)
6    Transformed test set shape        (1500, 31)
7              Numeric features                14
8          Categorical features                 5
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              eab2
2025-02-28 12:19:24,109:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:24,110:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:24,160:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:24,161:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:24,161:INFO:setup() successfully completed in 1.02s...............
2025-02-28 12:19:24,182:INFO:Initializing compare_models()
2025-02-28 12:19:24,182:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 12:19:24,182:INFO:Checking exceptions
2025-02-28 12:19:24,184:INFO:Preparing display monitor
2025-02-28 12:19:24,202:INFO:Initializing Linear Regression
2025-02-28 12:19:24,202:INFO:Total runtime is 0.0 minutes
2025-02-28 12:19:24,205:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:24,206:INFO:Initializing create_model()
2025-02-28 12:19:24,206:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:24,206:INFO:Checking exceptions
2025-02-28 12:19:24,206:INFO:Importing libraries
2025-02-28 12:19:24,206:INFO:Copying training dataset
2025-02-28 12:19:24,209:INFO:Defining folds
2025-02-28 12:19:24,209:INFO:Declaring metric variables
2025-02-28 12:19:24,212:INFO:Importing untrained model
2025-02-28 12:19:24,216:INFO:Linear Regression Imported successfully
2025-02-28 12:19:24,222:INFO:Starting cross validation
2025-02-28 12:19:24,223:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:24,376:INFO:Calculating mean and std
2025-02-28 12:19:24,376:INFO:Creating metrics dataframe
2025-02-28 12:19:24,377:INFO:Uploading results into container
2025-02-28 12:19:24,377:INFO:Uploading model into container now
2025-02-28 12:19:24,378:INFO:_master_model_container: 1
2025-02-28 12:19:24,378:INFO:_display_container: 2
2025-02-28 12:19:24,378:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 12:19:24,378:INFO:create_model() successfully completed......................................
2025-02-28 12:19:24,507:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:24,507:INFO:Creating metrics dataframe
2025-02-28 12:19:24,511:INFO:Initializing Lasso Regression
2025-02-28 12:19:24,511:INFO:Total runtime is 0.005150417486826579 minutes
2025-02-28 12:19:24,513:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:24,513:INFO:Initializing create_model()
2025-02-28 12:19:24,513:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:24,514:INFO:Checking exceptions
2025-02-28 12:19:24,514:INFO:Importing libraries
2025-02-28 12:19:24,514:INFO:Copying training dataset
2025-02-28 12:19:24,517:INFO:Defining folds
2025-02-28 12:19:24,517:INFO:Declaring metric variables
2025-02-28 12:19:24,519:INFO:Importing untrained model
2025-02-28 12:19:24,521:INFO:Lasso Regression Imported successfully
2025-02-28 12:19:24,526:INFO:Starting cross validation
2025-02-28 12:19:24,527:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:24,654:INFO:Calculating mean and std
2025-02-28 12:19:24,654:INFO:Creating metrics dataframe
2025-02-28 12:19:24,655:INFO:Uploading results into container
2025-02-28 12:19:24,655:INFO:Uploading model into container now
2025-02-28 12:19:24,656:INFO:_master_model_container: 2
2025-02-28 12:19:24,656:INFO:_display_container: 2
2025-02-28 12:19:24,656:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 12:19:24,656:INFO:create_model() successfully completed......................................
2025-02-28 12:19:24,784:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:24,784:INFO:Creating metrics dataframe
2025-02-28 12:19:24,788:INFO:Initializing Ridge Regression
2025-02-28 12:19:24,788:INFO:Total runtime is 0.009762648741404215 minutes
2025-02-28 12:19:24,790:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:24,790:INFO:Initializing create_model()
2025-02-28 12:19:24,790:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:24,790:INFO:Checking exceptions
2025-02-28 12:19:24,790:INFO:Importing libraries
2025-02-28 12:19:24,790:INFO:Copying training dataset
2025-02-28 12:19:24,793:INFO:Defining folds
2025-02-28 12:19:24,793:INFO:Declaring metric variables
2025-02-28 12:19:24,795:INFO:Importing untrained model
2025-02-28 12:19:24,797:INFO:Ridge Regression Imported successfully
2025-02-28 12:19:24,801:INFO:Starting cross validation
2025-02-28 12:19:24,803:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:24,937:INFO:Calculating mean and std
2025-02-28 12:19:24,938:INFO:Creating metrics dataframe
2025-02-28 12:19:24,939:INFO:Uploading results into container
2025-02-28 12:19:24,939:INFO:Uploading model into container now
2025-02-28 12:19:24,939:INFO:_master_model_container: 3
2025-02-28 12:19:24,939:INFO:_display_container: 2
2025-02-28 12:19:24,939:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 12:19:24,939:INFO:create_model() successfully completed......................................
2025-02-28 12:19:25,066:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:25,066:INFO:Creating metrics dataframe
2025-02-28 12:19:25,069:INFO:Initializing Elastic Net
2025-02-28 12:19:25,071:INFO:Total runtime is 0.014487226804097492 minutes
2025-02-28 12:19:25,073:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:25,073:INFO:Initializing create_model()
2025-02-28 12:19:25,073:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:25,073:INFO:Checking exceptions
2025-02-28 12:19:25,073:INFO:Importing libraries
2025-02-28 12:19:25,073:INFO:Copying training dataset
2025-02-28 12:19:25,076:INFO:Defining folds
2025-02-28 12:19:25,076:INFO:Declaring metric variables
2025-02-28 12:19:25,078:INFO:Importing untrained model
2025-02-28 12:19:25,081:INFO:Elastic Net Imported successfully
2025-02-28 12:19:25,086:INFO:Starting cross validation
2025-02-28 12:19:25,087:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:25,218:INFO:Calculating mean and std
2025-02-28 12:19:25,219:INFO:Creating metrics dataframe
2025-02-28 12:19:25,220:INFO:Uploading results into container
2025-02-28 12:19:25,220:INFO:Uploading model into container now
2025-02-28 12:19:25,220:INFO:_master_model_container: 4
2025-02-28 12:19:25,220:INFO:_display_container: 2
2025-02-28 12:19:25,221:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 12:19:25,221:INFO:create_model() successfully completed......................................
2025-02-28 12:19:25,345:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:25,345:INFO:Creating metrics dataframe
2025-02-28 12:19:25,349:INFO:Initializing Least Angle Regression
2025-02-28 12:19:25,349:INFO:Total runtime is 0.019125668207804362 minutes
2025-02-28 12:19:25,352:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:25,352:INFO:Initializing create_model()
2025-02-28 12:19:25,352:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:25,352:INFO:Checking exceptions
2025-02-28 12:19:25,352:INFO:Importing libraries
2025-02-28 12:19:25,352:INFO:Copying training dataset
2025-02-28 12:19:25,355:INFO:Defining folds
2025-02-28 12:19:25,355:INFO:Declaring metric variables
2025-02-28 12:19:25,358:INFO:Importing untrained model
2025-02-28 12:19:25,360:INFO:Least Angle Regression Imported successfully
2025-02-28 12:19:25,364:INFO:Starting cross validation
2025-02-28 12:19:25,365:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:27,424:INFO:Calculating mean and std
2025-02-28 12:19:27,425:INFO:Creating metrics dataframe
2025-02-28 12:19:27,426:INFO:Uploading results into container
2025-02-28 12:19:27,426:INFO:Uploading model into container now
2025-02-28 12:19:27,427:INFO:_master_model_container: 5
2025-02-28 12:19:27,427:INFO:_display_container: 2
2025-02-28 12:19:27,427:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 12:19:27,427:INFO:create_model() successfully completed......................................
2025-02-28 12:19:27,559:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:27,559:INFO:Creating metrics dataframe
2025-02-28 12:19:27,562:INFO:Initializing Lasso Least Angle Regression
2025-02-28 12:19:27,562:INFO:Total runtime is 0.0560053547223409 minutes
2025-02-28 12:19:27,565:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:27,565:INFO:Initializing create_model()
2025-02-28 12:19:27,565:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:27,566:INFO:Checking exceptions
2025-02-28 12:19:27,566:INFO:Importing libraries
2025-02-28 12:19:27,566:INFO:Copying training dataset
2025-02-28 12:19:27,569:INFO:Defining folds
2025-02-28 12:19:27,569:INFO:Declaring metric variables
2025-02-28 12:19:27,572:INFO:Importing untrained model
2025-02-28 12:19:27,575:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 12:19:27,579:INFO:Starting cross validation
2025-02-28 12:19:27,581:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:27,721:INFO:Calculating mean and std
2025-02-28 12:19:27,721:INFO:Creating metrics dataframe
2025-02-28 12:19:27,723:INFO:Uploading results into container
2025-02-28 12:19:27,723:INFO:Uploading model into container now
2025-02-28 12:19:27,723:INFO:_master_model_container: 6
2025-02-28 12:19:27,723:INFO:_display_container: 2
2025-02-28 12:19:27,724:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 12:19:27,724:INFO:create_model() successfully completed......................................
2025-02-28 12:19:27,850:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:27,850:INFO:Creating metrics dataframe
2025-02-28 12:19:27,855:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 12:19:27,855:INFO:Total runtime is 0.06088508764902751 minutes
2025-02-28 12:19:27,857:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:27,857:INFO:Initializing create_model()
2025-02-28 12:19:27,858:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:27,858:INFO:Checking exceptions
2025-02-28 12:19:27,858:INFO:Importing libraries
2025-02-28 12:19:27,858:INFO:Copying training dataset
2025-02-28 12:19:27,861:INFO:Defining folds
2025-02-28 12:19:27,861:INFO:Declaring metric variables
2025-02-28 12:19:27,863:INFO:Importing untrained model
2025-02-28 12:19:27,865:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 12:19:27,868:INFO:Starting cross validation
2025-02-28 12:19:27,870:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:27,940:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,943:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,950:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,958:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,960:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,961:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,962:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,984:INFO:Calculating mean and std
2025-02-28 12:19:27,984:INFO:Creating metrics dataframe
2025-02-28 12:19:27,985:INFO:Uploading results into container
2025-02-28 12:19:27,986:INFO:Uploading model into container now
2025-02-28 12:19:27,986:INFO:_master_model_container: 7
2025-02-28 12:19:27,986:INFO:_display_container: 2
2025-02-28 12:19:27,986:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 12:19:27,987:INFO:create_model() successfully completed......................................
2025-02-28 12:19:28,118:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:28,118:INFO:Creating metrics dataframe
2025-02-28 12:19:28,123:INFO:Initializing Bayesian Ridge
2025-02-28 12:19:28,123:INFO:Total runtime is 0.06535278161366781 minutes
2025-02-28 12:19:28,125:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:28,125:INFO:Initializing create_model()
2025-02-28 12:19:28,126:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:28,126:INFO:Checking exceptions
2025-02-28 12:19:28,126:INFO:Importing libraries
2025-02-28 12:19:28,126:INFO:Copying training dataset
2025-02-28 12:19:28,129:INFO:Defining folds
2025-02-28 12:19:28,129:INFO:Declaring metric variables
2025-02-28 12:19:28,131:INFO:Importing untrained model
2025-02-28 12:19:28,134:INFO:Bayesian Ridge Imported successfully
2025-02-28 12:19:28,138:INFO:Starting cross validation
2025-02-28 12:19:28,139:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:28,279:INFO:Calculating mean and std
2025-02-28 12:19:28,280:INFO:Creating metrics dataframe
2025-02-28 12:19:28,281:INFO:Uploading results into container
2025-02-28 12:19:28,281:INFO:Uploading model into container now
2025-02-28 12:19:28,281:INFO:_master_model_container: 8
2025-02-28 12:19:28,281:INFO:_display_container: 2
2025-02-28 12:19:28,281:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 12:19:28,281:INFO:create_model() successfully completed......................................
2025-02-28 12:19:28,406:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:28,406:INFO:Creating metrics dataframe
2025-02-28 12:19:28,410:INFO:Initializing Passive Aggressive Regressor
2025-02-28 12:19:28,410:INFO:Total runtime is 0.0701412041982015 minutes
2025-02-28 12:19:28,412:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:28,412:INFO:Initializing create_model()
2025-02-28 12:19:28,412:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:28,412:INFO:Checking exceptions
2025-02-28 12:19:28,412:INFO:Importing libraries
2025-02-28 12:19:28,412:INFO:Copying training dataset
2025-02-28 12:19:28,415:INFO:Defining folds
2025-02-28 12:19:28,416:INFO:Declaring metric variables
2025-02-28 12:19:28,418:INFO:Importing untrained model
2025-02-28 12:19:28,421:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 12:19:28,425:INFO:Starting cross validation
2025-02-28 12:19:28,426:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:28,561:INFO:Calculating mean and std
2025-02-28 12:19:28,561:INFO:Creating metrics dataframe
2025-02-28 12:19:28,562:INFO:Uploading results into container
2025-02-28 12:19:28,562:INFO:Uploading model into container now
2025-02-28 12:19:28,563:INFO:_master_model_container: 9
2025-02-28 12:19:28,563:INFO:_display_container: 2
2025-02-28 12:19:28,563:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:19:28,563:INFO:create_model() successfully completed......................................
2025-02-28 12:19:28,692:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:28,692:INFO:Creating metrics dataframe
2025-02-28 12:19:28,696:INFO:Initializing Huber Regressor
2025-02-28 12:19:28,696:INFO:Total runtime is 0.07489873568216959 minutes
2025-02-28 12:19:28,698:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:28,699:INFO:Initializing create_model()
2025-02-28 12:19:28,699:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:28,699:INFO:Checking exceptions
2025-02-28 12:19:28,699:INFO:Importing libraries
2025-02-28 12:19:28,699:INFO:Copying training dataset
2025-02-28 12:19:28,702:INFO:Defining folds
2025-02-28 12:19:28,702:INFO:Declaring metric variables
2025-02-28 12:19:28,704:INFO:Importing untrained model
2025-02-28 12:19:28,707:INFO:Huber Regressor Imported successfully
2025-02-28 12:19:28,713:INFO:Starting cross validation
2025-02-28 12:19:28,715:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:28,822:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:19:28,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:19:28,840:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:19:28,874:INFO:Calculating mean and std
2025-02-28 12:19:28,874:INFO:Creating metrics dataframe
2025-02-28 12:19:28,876:INFO:Uploading results into container
2025-02-28 12:19:28,876:INFO:Uploading model into container now
2025-02-28 12:19:28,876:INFO:_master_model_container: 10
2025-02-28 12:19:28,876:INFO:_display_container: 2
2025-02-28 12:19:28,876:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 12:19:28,876:INFO:create_model() successfully completed......................................
2025-02-28 12:19:29,001:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:29,001:INFO:Creating metrics dataframe
2025-02-28 12:19:29,005:INFO:Initializing K Neighbors Regressor
2025-02-28 12:19:29,006:INFO:Total runtime is 0.08007542292277017 minutes
2025-02-28 12:19:29,008:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:29,008:INFO:Initializing create_model()
2025-02-28 12:19:29,008:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:29,008:INFO:Checking exceptions
2025-02-28 12:19:29,008:INFO:Importing libraries
2025-02-28 12:19:29,008:INFO:Copying training dataset
2025-02-28 12:19:29,011:INFO:Defining folds
2025-02-28 12:19:29,012:INFO:Declaring metric variables
2025-02-28 12:19:29,014:INFO:Importing untrained model
2025-02-28 12:19:29,017:INFO:K Neighbors Regressor Imported successfully
2025-02-28 12:19:29,021:INFO:Starting cross validation
2025-02-28 12:19:29,022:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:29,156:INFO:Calculating mean and std
2025-02-28 12:19:29,157:INFO:Creating metrics dataframe
2025-02-28 12:19:29,158:INFO:Uploading results into container
2025-02-28 12:19:29,158:INFO:Uploading model into container now
2025-02-28 12:19:29,158:INFO:_master_model_container: 11
2025-02-28 12:19:29,158:INFO:_display_container: 2
2025-02-28 12:19:29,159:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 12:19:29,159:INFO:create_model() successfully completed......................................
2025-02-28 12:19:29,289:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:29,289:INFO:Creating metrics dataframe
2025-02-28 12:19:29,293:INFO:Initializing Decision Tree Regressor
2025-02-28 12:19:29,293:INFO:Total runtime is 0.08485944271087646 minutes
2025-02-28 12:19:29,295:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:29,296:INFO:Initializing create_model()
2025-02-28 12:19:29,296:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:29,296:INFO:Checking exceptions
2025-02-28 12:19:29,297:INFO:Importing libraries
2025-02-28 12:19:29,297:INFO:Copying training dataset
2025-02-28 12:19:29,300:INFO:Defining folds
2025-02-28 12:19:29,300:INFO:Declaring metric variables
2025-02-28 12:19:29,302:INFO:Importing untrained model
2025-02-28 12:19:29,304:INFO:Decision Tree Regressor Imported successfully
2025-02-28 12:19:29,309:INFO:Starting cross validation
2025-02-28 12:19:29,309:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:29,454:INFO:Calculating mean and std
2025-02-28 12:19:29,454:INFO:Creating metrics dataframe
2025-02-28 12:19:29,456:INFO:Uploading results into container
2025-02-28 12:19:29,456:INFO:Uploading model into container now
2025-02-28 12:19:29,456:INFO:_master_model_container: 12
2025-02-28 12:19:29,456:INFO:_display_container: 2
2025-02-28 12:19:29,456:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:19:29,456:INFO:create_model() successfully completed......................................
2025-02-28 12:19:29,584:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:29,585:INFO:Creating metrics dataframe
2025-02-28 12:19:29,589:INFO:Initializing Random Forest Regressor
2025-02-28 12:19:29,589:INFO:Total runtime is 0.08978945811589559 minutes
2025-02-28 12:19:29,591:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:29,592:INFO:Initializing create_model()
2025-02-28 12:19:29,592:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:29,592:INFO:Checking exceptions
2025-02-28 12:19:29,592:INFO:Importing libraries
2025-02-28 12:19:29,592:INFO:Copying training dataset
2025-02-28 12:19:29,595:INFO:Defining folds
2025-02-28 12:19:29,595:INFO:Declaring metric variables
2025-02-28 12:19:29,598:INFO:Importing untrained model
2025-02-28 12:19:29,601:INFO:Random Forest Regressor Imported successfully
2025-02-28 12:19:29,605:INFO:Starting cross validation
2025-02-28 12:19:29,607:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:30,343:INFO:Calculating mean and std
2025-02-28 12:19:30,344:INFO:Creating metrics dataframe
2025-02-28 12:19:30,345:INFO:Uploading results into container
2025-02-28 12:19:30,345:INFO:Uploading model into container now
2025-02-28 12:19:30,346:INFO:_master_model_container: 13
2025-02-28 12:19:30,346:INFO:_display_container: 2
2025-02-28 12:19:30,346:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 12:19:30,346:INFO:create_model() successfully completed......................................
2025-02-28 12:19:30,469:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:30,469:INFO:Creating metrics dataframe
2025-02-28 12:19:30,474:INFO:Initializing Extra Trees Regressor
2025-02-28 12:19:30,474:INFO:Total runtime is 0.1045389453570048 minutes
2025-02-28 12:19:30,476:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:30,476:INFO:Initializing create_model()
2025-02-28 12:19:30,477:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:30,477:INFO:Checking exceptions
2025-02-28 12:19:30,477:INFO:Importing libraries
2025-02-28 12:19:30,477:INFO:Copying training dataset
2025-02-28 12:19:30,480:INFO:Defining folds
2025-02-28 12:19:30,480:INFO:Declaring metric variables
2025-02-28 12:19:30,482:INFO:Importing untrained model
2025-02-28 12:19:30,484:INFO:Extra Trees Regressor Imported successfully
2025-02-28 12:19:30,489:INFO:Starting cross validation
2025-02-28 12:19:30,490:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:31,007:INFO:Calculating mean and std
2025-02-28 12:19:31,007:INFO:Creating metrics dataframe
2025-02-28 12:19:31,009:INFO:Uploading results into container
2025-02-28 12:19:31,009:INFO:Uploading model into container now
2025-02-28 12:19:31,009:INFO:_master_model_container: 14
2025-02-28 12:19:31,009:INFO:_display_container: 2
2025-02-28 12:19:31,010:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 12:19:31,010:INFO:create_model() successfully completed......................................
2025-02-28 12:19:31,141:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:31,141:INFO:Creating metrics dataframe
2025-02-28 12:19:31,147:INFO:Initializing AdaBoost Regressor
2025-02-28 12:19:31,147:INFO:Total runtime is 0.11575092077255249 minutes
2025-02-28 12:19:31,149:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:31,149:INFO:Initializing create_model()
2025-02-28 12:19:31,149:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:31,149:INFO:Checking exceptions
2025-02-28 12:19:31,149:INFO:Importing libraries
2025-02-28 12:19:31,149:INFO:Copying training dataset
2025-02-28 12:19:31,152:INFO:Defining folds
2025-02-28 12:19:31,153:INFO:Declaring metric variables
2025-02-28 12:19:31,155:INFO:Importing untrained model
2025-02-28 12:19:31,157:INFO:AdaBoost Regressor Imported successfully
2025-02-28 12:19:31,162:INFO:Starting cross validation
2025-02-28 12:19:31,164:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:31,555:INFO:Calculating mean and std
2025-02-28 12:19:31,556:INFO:Creating metrics dataframe
2025-02-28 12:19:31,557:INFO:Uploading results into container
2025-02-28 12:19:31,557:INFO:Uploading model into container now
2025-02-28 12:19:31,557:INFO:_master_model_container: 15
2025-02-28 12:19:31,558:INFO:_display_container: 2
2025-02-28 12:19:31,558:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 12:19:31,558:INFO:create_model() successfully completed......................................
2025-02-28 12:19:31,687:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:31,687:INFO:Creating metrics dataframe
2025-02-28 12:19:31,693:INFO:Initializing Gradient Boosting Regressor
2025-02-28 12:19:31,693:INFO:Total runtime is 0.12485628922780355 minutes
2025-02-28 12:19:31,695:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:31,695:INFO:Initializing create_model()
2025-02-28 12:19:31,695:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:31,695:INFO:Checking exceptions
2025-02-28 12:19:31,695:INFO:Importing libraries
2025-02-28 12:19:31,695:INFO:Copying training dataset
2025-02-28 12:19:31,699:INFO:Defining folds
2025-02-28 12:19:31,699:INFO:Declaring metric variables
2025-02-28 12:19:31,702:INFO:Importing untrained model
2025-02-28 12:19:31,705:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 12:19:31,712:INFO:Starting cross validation
2025-02-28 12:19:31,713:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:32,319:INFO:Calculating mean and std
2025-02-28 12:19:32,319:INFO:Creating metrics dataframe
2025-02-28 12:19:32,321:INFO:Uploading results into container
2025-02-28 12:19:32,321:INFO:Uploading model into container now
2025-02-28 12:19:32,321:INFO:_master_model_container: 16
2025-02-28 12:19:32,321:INFO:_display_container: 2
2025-02-28 12:19:32,321:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:19:32,321:INFO:create_model() successfully completed......................................
2025-02-28 12:19:32,447:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:32,447:INFO:Creating metrics dataframe
2025-02-28 12:19:32,453:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:19:32,453:INFO:Total runtime is 0.13752080202102662 minutes
2025-02-28 12:19:32,455:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:32,455:INFO:Initializing create_model()
2025-02-28 12:19:32,455:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:32,455:INFO:Checking exceptions
2025-02-28 12:19:32,455:INFO:Importing libraries
2025-02-28 12:19:32,455:INFO:Copying training dataset
2025-02-28 12:19:32,459:INFO:Defining folds
2025-02-28 12:19:32,459:INFO:Declaring metric variables
2025-02-28 12:19:32,461:INFO:Importing untrained model
2025-02-28 12:19:32,464:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:19:32,469:INFO:Starting cross validation
2025-02-28 12:19:32,470:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:33,052:INFO:Calculating mean and std
2025-02-28 12:19:33,052:INFO:Creating metrics dataframe
2025-02-28 12:19:33,054:INFO:Uploading results into container
2025-02-28 12:19:33,054:INFO:Uploading model into container now
2025-02-28 12:19:33,054:INFO:_master_model_container: 17
2025-02-28 12:19:33,054:INFO:_display_container: 2
2025-02-28 12:19:33,055:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:19:33,055:INFO:create_model() successfully completed......................................
2025-02-28 12:19:33,185:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:33,185:INFO:Creating metrics dataframe
2025-02-28 12:19:33,191:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:19:33,191:INFO:Total runtime is 0.14982364575068158 minutes
2025-02-28 12:19:33,193:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:33,193:INFO:Initializing create_model()
2025-02-28 12:19:33,193:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:33,194:INFO:Checking exceptions
2025-02-28 12:19:33,194:INFO:Importing libraries
2025-02-28 12:19:33,194:INFO:Copying training dataset
2025-02-28 12:19:33,197:INFO:Defining folds
2025-02-28 12:19:33,197:INFO:Declaring metric variables
2025-02-28 12:19:33,199:INFO:Importing untrained model
2025-02-28 12:19:33,202:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:19:33,207:INFO:Starting cross validation
2025-02-28 12:19:33,208:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:34,647:INFO:Calculating mean and std
2025-02-28 12:19:34,648:INFO:Creating metrics dataframe
2025-02-28 12:19:34,649:INFO:Uploading results into container
2025-02-28 12:19:34,650:INFO:Uploading model into container now
2025-02-28 12:19:34,650:INFO:_master_model_container: 18
2025-02-28 12:19:34,650:INFO:_display_container: 2
2025-02-28 12:19:34,651:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:19:34,651:INFO:create_model() successfully completed......................................
2025-02-28 12:19:34,792:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:34,792:INFO:Creating metrics dataframe
2025-02-28 12:19:34,798:INFO:Initializing CatBoost Regressor
2025-02-28 12:19:34,798:INFO:Total runtime is 0.1765997131665548 minutes
2025-02-28 12:19:34,800:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:34,800:INFO:Initializing create_model()
2025-02-28 12:19:34,800:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:34,801:INFO:Checking exceptions
2025-02-28 12:19:34,801:INFO:Importing libraries
2025-02-28 12:19:34,801:INFO:Copying training dataset
2025-02-28 12:19:34,803:INFO:Defining folds
2025-02-28 12:19:34,803:INFO:Declaring metric variables
2025-02-28 12:19:34,806:INFO:Importing untrained model
2025-02-28 12:19:34,808:INFO:CatBoost Regressor Imported successfully
2025-02-28 12:19:34,811:INFO:Starting cross validation
2025-02-28 12:19:34,813:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:37,832:INFO:Calculating mean and std
2025-02-28 12:19:37,833:INFO:Creating metrics dataframe
2025-02-28 12:19:37,834:INFO:Uploading results into container
2025-02-28 12:19:37,834:INFO:Uploading model into container now
2025-02-28 12:19:37,834:INFO:_master_model_container: 19
2025-02-28 12:19:37,834:INFO:_display_container: 2
2025-02-28 12:19:37,835:INFO:<catboost.core.CatBoostRegressor object at 0x0000024E5BBD15A0>
2025-02-28 12:19:37,835:INFO:create_model() successfully completed......................................
2025-02-28 12:19:37,958:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:37,958:INFO:Creating metrics dataframe
2025-02-28 12:19:37,963:INFO:Initializing Dummy Regressor
2025-02-28 12:19:37,963:INFO:Total runtime is 0.2293560028076172 minutes
2025-02-28 12:19:37,966:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:37,966:INFO:Initializing create_model()
2025-02-28 12:19:37,966:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:37,966:INFO:Checking exceptions
2025-02-28 12:19:37,966:INFO:Importing libraries
2025-02-28 12:19:37,966:INFO:Copying training dataset
2025-02-28 12:19:37,970:INFO:Defining folds
2025-02-28 12:19:37,970:INFO:Declaring metric variables
2025-02-28 12:19:37,973:INFO:Importing untrained model
2025-02-28 12:19:37,975:INFO:Dummy Regressor Imported successfully
2025-02-28 12:19:37,979:INFO:Starting cross validation
2025-02-28 12:19:37,980:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:38,114:INFO:Calculating mean and std
2025-02-28 12:19:38,115:INFO:Creating metrics dataframe
2025-02-28 12:19:38,116:INFO:Uploading results into container
2025-02-28 12:19:38,116:INFO:Uploading model into container now
2025-02-28 12:19:38,116:INFO:_master_model_container: 20
2025-02-28 12:19:38,116:INFO:_display_container: 2
2025-02-28 12:19:38,116:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:19:38,117:INFO:create_model() successfully completed......................................
2025-02-28 12:19:38,243:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:38,243:INFO:Creating metrics dataframe
2025-02-28 12:19:38,249:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:19:38,256:INFO:Initializing create_model()
2025-02-28 12:19:38,256:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:38,256:INFO:Checking exceptions
2025-02-28 12:19:38,257:INFO:Importing libraries
2025-02-28 12:19:38,257:INFO:Copying training dataset
2025-02-28 12:19:38,260:INFO:Defining folds
2025-02-28 12:19:38,260:INFO:Declaring metric variables
2025-02-28 12:19:38,260:INFO:Importing untrained model
2025-02-28 12:19:38,260:INFO:Declaring custom model
2025-02-28 12:19:38,260:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:19:38,261:INFO:Cross validation set to False
2025-02-28 12:19:38,261:INFO:Fitting Model
2025-02-28 12:19:38,500:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:19:38,501:INFO:create_model() successfully completed......................................
2025-02-28 12:19:38,656:INFO:_master_model_container: 20
2025-02-28 12:19:38,656:INFO:_display_container: 2
2025-02-28 12:19:38,656:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:19:38,656:INFO:compare_models() successfully completed......................................
2025-02-28 12:20:33,517:INFO:Initializing create_model()
2025-02-28 12:20:33,517:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:20:33,517:INFO:Checking exceptions
2025-02-28 12:20:33,525:INFO:Importing libraries
2025-02-28 12:20:33,525:INFO:Copying training dataset
2025-02-28 12:20:33,528:INFO:Defining folds
2025-02-28 12:20:33,528:INFO:Declaring metric variables
2025-02-28 12:20:33,530:INFO:Importing untrained model
2025-02-28 12:20:33,533:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:20:33,539:INFO:Starting cross validation
2025-02-28 12:20:33,540:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:20:34,172:INFO:Calculating mean and std
2025-02-28 12:20:34,172:INFO:Creating metrics dataframe
2025-02-28 12:20:34,175:INFO:Finalizing model
2025-02-28 12:20:34,286:INFO:Uploading results into container
2025-02-28 12:20:34,287:INFO:Uploading model into container now
2025-02-28 12:20:34,293:INFO:_master_model_container: 21
2025-02-28 12:20:34,293:INFO:_display_container: 3
2025-02-28 12:20:34,293:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:20:34,294:INFO:create_model() successfully completed......................................
2025-02-28 12:20:53,870:INFO:Initializing plot_model()
2025-02-28 12:20:53,870:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, system=True)
2025-02-28 12:20:53,870:INFO:Checking exceptions
2025-02-28 12:20:53,873:INFO:Preloading libraries
2025-02-28 12:20:53,878:INFO:Copying training dataset
2025-02-28 12:20:53,878:INFO:Plot type: residuals
2025-02-28 12:20:54,129:INFO:Fitting Model
2025-02-28 12:20:54,160:INFO:Scoring test/hold-out set
2025-02-28 12:20:54,434:INFO:Visual Rendered Successfully
2025-02-28 12:20:54,575:INFO:plot_model() successfully completed......................................
2025-02-28 12:21:02,113:INFO:Initializing plot_model()
2025-02-28 12:21:02,113:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, system=True)
2025-02-28 12:21:02,113:INFO:Checking exceptions
2025-02-28 12:21:02,116:INFO:Preloading libraries
2025-02-28 12:21:02,121:INFO:Copying training dataset
2025-02-28 12:21:02,121:INFO:Plot type: error
2025-02-28 12:21:02,342:INFO:Fitting Model
2025-02-28 12:21:02,342:INFO:Scoring test/hold-out set
2025-02-28 12:21:02,486:INFO:Visual Rendered Successfully
2025-02-28 12:21:02,617:INFO:plot_model() successfully completed......................................
2025-02-28 12:21:10,033:INFO:Initializing plot_model()
2025-02-28 12:21:10,033:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, system=True)
2025-02-28 12:21:10,033:INFO:Checking exceptions
2025-02-28 12:21:10,036:INFO:Preloading libraries
2025-02-28 12:21:10,041:INFO:Copying training dataset
2025-02-28 12:21:10,041:INFO:Plot type: feature
2025-02-28 12:21:10,041:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 12:21:10,189:INFO:Visual Rendered Successfully
2025-02-28 12:21:10,322:INFO:plot_model() successfully completed......................................
