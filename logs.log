2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:12:54,445:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:13:04,151:INFO:PyCaret ClassificationExperiment
2025-02-28 12:13:04,151:INFO:Logging name: clf-default-name
2025-02-28 12:13:04,151:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 12:13:04,151:INFO:version 3.3.2
2025-02-28 12:13:04,151:INFO:Initializing setup()
2025-02-28 12:13:04,151:INFO:self.USI: cfb5
2025-02-28 12:13:04,152:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fix_imbalance', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'is_multiclass', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:13:04,152:INFO:Checking environment
2025-02-28 12:13:04,152:INFO:python_version: 3.10.16
2025-02-28 12:13:04,152:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:13:04,152:INFO:machine: AMD64
2025-02-28 12:13:04,152:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:13:04,152:INFO:Memory: svmem(total=34200334336, available=17714921472, percent=48.2, used=16485412864, free=17714921472)
2025-02-28 12:13:04,152:INFO:Physical Core: 24
2025-02-28 12:13:04,152:INFO:Logical Core: 32
2025-02-28 12:13:04,152:INFO:Checking libraries
2025-02-28 12:13:04,152:INFO:System:
2025-02-28 12:13:04,152:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:13:04,152:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:13:04,152:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:13:04,152:INFO:PyCaret required dependencies:
2025-02-28 12:13:07,640:INFO:                 pip: 25.0
2025-02-28 12:13:07,640:INFO:          setuptools: 75.8.0
2025-02-28 12:13:07,640:INFO:             pycaret: 3.3.2
2025-02-28 12:13:07,640:INFO:             IPython: 8.30.0
2025-02-28 12:13:07,640:INFO:          ipywidgets: 8.1.5
2025-02-28 12:13:07,640:INFO:                tqdm: 4.67.1
2025-02-28 12:13:07,640:INFO:               numpy: 1.26.4
2025-02-28 12:13:07,640:INFO:              pandas: 2.1.4
2025-02-28 12:13:07,640:INFO:              jinja2: 3.1.5
2025-02-28 12:13:07,640:INFO:               scipy: 1.11.4
2025-02-28 12:13:07,640:INFO:              joblib: 1.3.2
2025-02-28 12:13:07,640:INFO:             sklearn: 1.4.2
2025-02-28 12:13:07,640:INFO:                pyod: 2.0.3
2025-02-28 12:13:07,640:INFO:            imblearn: 0.13.0
2025-02-28 12:13:07,640:INFO:   category_encoders: 2.7.0
2025-02-28 12:13:07,640:INFO:            lightgbm: 4.5.0
2025-02-28 12:13:07,640:INFO:               numba: 0.61.0
2025-02-28 12:13:07,640:INFO:            requests: 2.32.3
2025-02-28 12:13:07,640:INFO:          matplotlib: 3.7.5
2025-02-28 12:13:07,640:INFO:          scikitplot: 0.3.7
2025-02-28 12:13:07,640:INFO:         yellowbrick: 1.5
2025-02-28 12:13:07,640:INFO:              plotly: 5.24.1
2025-02-28 12:13:07,640:INFO:    plotly-resampler: Not installed
2025-02-28 12:13:07,640:INFO:             kaleido: 0.2.1
2025-02-28 12:13:07,640:INFO:           schemdraw: 0.15
2025-02-28 12:13:07,640:INFO:         statsmodels: 0.14.4
2025-02-28 12:13:07,640:INFO:              sktime: 0.26.0
2025-02-28 12:13:07,640:INFO:               tbats: 1.1.3
2025-02-28 12:13:07,640:INFO:            pmdarima: 2.0.4
2025-02-28 12:13:07,640:INFO:              psutil: 5.9.0
2025-02-28 12:13:07,640:INFO:          markupsafe: 2.1.5
2025-02-28 12:13:07,640:INFO:             pickle5: Not installed
2025-02-28 12:13:07,640:INFO:         cloudpickle: 3.1.1
2025-02-28 12:13:07,640:INFO:         deprecation: 2.1.0
2025-02-28 12:13:07,640:INFO:              xxhash: 3.5.0
2025-02-28 12:13:07,640:INFO:           wurlitzer: Not installed
2025-02-28 12:13:07,640:INFO:PyCaret optional dependencies:
2025-02-28 12:13:15,610:INFO:                shap: 0.44.1
2025-02-28 12:13:15,610:INFO:           interpret: 0.6.9
2025-02-28 12:13:15,610:INFO:                umap: 0.5.7
2025-02-28 12:13:15,610:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:13:15,610:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:13:15,610:INFO:             autoviz: Not installed
2025-02-28 12:13:15,610:INFO:           fairlearn: 0.7.0
2025-02-28 12:13:15,610:INFO:          deepchecks: Not installed
2025-02-28 12:13:15,610:INFO:             xgboost: 2.1.4
2025-02-28 12:13:15,610:INFO:            catboost: 1.2.7
2025-02-28 12:13:15,610:INFO:              kmodes: 0.12.2
2025-02-28 12:13:15,610:INFO:             mlxtend: 0.23.4
2025-02-28 12:13:15,610:INFO:       statsforecast: 1.5.0
2025-02-28 12:13:15,610:INFO:        tune_sklearn: Not installed
2025-02-28 12:13:15,610:INFO:                 ray: Not installed
2025-02-28 12:13:15,610:INFO:            hyperopt: 0.2.7
2025-02-28 12:13:15,610:INFO:              optuna: 4.2.0
2025-02-28 12:13:15,610:INFO:               skopt: 0.10.2
2025-02-28 12:13:15,610:INFO:              mlflow: 2.20.1
2025-02-28 12:13:15,610:INFO:              gradio: 5.15.0
2025-02-28 12:13:15,610:INFO:             fastapi: 0.115.8
2025-02-28 12:13:15,610:INFO:             uvicorn: 0.34.0
2025-02-28 12:13:15,610:INFO:              m2cgen: 0.10.0
2025-02-28 12:13:15,610:INFO:           evidently: 0.4.40
2025-02-28 12:13:15,610:INFO:               fugue: 0.8.7
2025-02-28 12:13:15,610:INFO:           streamlit: Not installed
2025-02-28 12:13:15,610:INFO:             prophet: Not installed
2025-02-28 12:13:15,610:INFO:None
2025-02-28 12:13:15,610:INFO:Set up data.
2025-02-28 12:13:15,621:INFO:Set up folding strategy.
2025-02-28 12:13:15,621:INFO:Set up train/test split.
2025-02-28 12:13:15,630:INFO:Set up index.
2025-02-28 12:13:15,630:INFO:Assigning column types.
2025-02-28 12:13:15,633:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:13:15,652:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:13:15,655:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:15,672:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:15,673:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,024:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,025:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,036:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,038:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,038:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:13:35,058:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,070:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,071:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,091:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:13:35,103:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,104:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,105:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 12:13:35,137:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,138:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,170:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,171:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,174:INFO:Preparing preprocessing pipeline...
2025-02-28 12:13:35,175:INFO:Set up simple imputation.
2025-02-28 12:13:35,177:INFO:Set up encoding of ordinal features.
2025-02-28 12:13:35,179:INFO:Set up encoding of categorical features.
2025-02-28 12:13:35,261:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:13:35,271:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score',
                                             'Starting_Salary',
                                             'Career_Sa...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Student_ID'],
                                    transformer=TargetEncoder(cols=['Student_ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:13:35,271:INFO:Creating final display dataframe.
2025-02-28 12:13:35,468:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 20)
4        Transformed data shape        (5000, 31)
5   Transformed train set shape        (3500, 31)
6    Transformed test set shape        (1500, 31)
7              Numeric features                14
8          Categorical features                 5
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              cfb5
2025-02-28 12:13:35,505:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,506:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,546:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:13:35,547:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:13:35,548:INFO:setup() successfully completed in 31.4s...............
2025-02-28 12:14:03,115:INFO:Initializing compare_models()
2025-02-28 12:14:03,116:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 12:14:03,116:INFO:Checking exceptions
2025-02-28 12:14:03,119:INFO:Preparing display monitor
2025-02-28 12:14:03,133:INFO:Initializing Logistic Regression
2025-02-28 12:14:03,133:INFO:Total runtime is 0.0 minutes
2025-02-28 12:14:03,135:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:03,135:INFO:Initializing create_model()
2025-02-28 12:14:03,135:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:03,135:INFO:Checking exceptions
2025-02-28 12:14:03,135:INFO:Importing libraries
2025-02-28 12:14:03,135:INFO:Copying training dataset
2025-02-28 12:14:03,139:INFO:Defining folds
2025-02-28 12:14:03,139:INFO:Declaring metric variables
2025-02-28 12:14:03,141:INFO:Importing untrained model
2025-02-28 12:14:03,143:INFO:Logistic Regression Imported successfully
2025-02-28 12:14:03,148:INFO:Starting cross validation
2025-02-28 12:14:03,150:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:06,874:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,890:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,902:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,908:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,910:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,919:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,925:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,928:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,928:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,935:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,937:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,939:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,941:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,947:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,949:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,951:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,954:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:14:06,961:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,970:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,977:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:06,995:INFO:Calculating mean and std
2025-02-28 12:14:06,996:INFO:Creating metrics dataframe
2025-02-28 12:14:06,998:INFO:Uploading results into container
2025-02-28 12:14:06,999:INFO:Uploading model into container now
2025-02-28 12:14:06,999:INFO:_master_model_container: 1
2025-02-28 12:14:06,999:INFO:_display_container: 2
2025-02-28 12:14:06,999:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 12:14:06,999:INFO:create_model() successfully completed......................................
2025-02-28 12:14:07,116:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:07,116:INFO:Creating metrics dataframe
2025-02-28 12:14:07,119:INFO:Initializing K Neighbors Classifier
2025-02-28 12:14:07,119:INFO:Total runtime is 0.06642432610193888 minutes
2025-02-28 12:14:07,121:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:07,121:INFO:Initializing create_model()
2025-02-28 12:14:07,121:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:07,122:INFO:Checking exceptions
2025-02-28 12:14:07,122:INFO:Importing libraries
2025-02-28 12:14:07,122:INFO:Copying training dataset
2025-02-28 12:14:07,125:INFO:Defining folds
2025-02-28 12:14:07,125:INFO:Declaring metric variables
2025-02-28 12:14:07,128:INFO:Importing untrained model
2025-02-28 12:14:07,130:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:14:07,134:INFO:Starting cross validation
2025-02-28 12:14:07,135:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:10,005:INFO:Calculating mean and std
2025-02-28 12:14:10,006:INFO:Creating metrics dataframe
2025-02-28 12:14:10,007:INFO:Uploading results into container
2025-02-28 12:14:10,008:INFO:Uploading model into container now
2025-02-28 12:14:10,008:INFO:_master_model_container: 2
2025-02-28 12:14:10,008:INFO:_display_container: 2
2025-02-28 12:14:10,008:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:14:10,008:INFO:create_model() successfully completed......................................
2025-02-28 12:14:10,122:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:10,122:INFO:Creating metrics dataframe
2025-02-28 12:14:10,126:INFO:Initializing Naive Bayes
2025-02-28 12:14:10,126:INFO:Total runtime is 0.11655208269755045 minutes
2025-02-28 12:14:10,128:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:10,128:INFO:Initializing create_model()
2025-02-28 12:14:10,128:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:10,128:INFO:Checking exceptions
2025-02-28 12:14:10,129:INFO:Importing libraries
2025-02-28 12:14:10,129:INFO:Copying training dataset
2025-02-28 12:14:10,132:INFO:Defining folds
2025-02-28 12:14:10,132:INFO:Declaring metric variables
2025-02-28 12:14:10,135:INFO:Importing untrained model
2025-02-28 12:14:10,137:INFO:Naive Bayes Imported successfully
2025-02-28 12:14:10,141:INFO:Starting cross validation
2025-02-28 12:14:10,142:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:13,006:INFO:Calculating mean and std
2025-02-28 12:14:13,007:INFO:Creating metrics dataframe
2025-02-28 12:14:13,008:INFO:Uploading results into container
2025-02-28 12:14:13,009:INFO:Uploading model into container now
2025-02-28 12:14:13,009:INFO:_master_model_container: 3
2025-02-28 12:14:13,009:INFO:_display_container: 2
2025-02-28 12:14:13,009:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 12:14:13,010:INFO:create_model() successfully completed......................................
2025-02-28 12:14:13,123:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:13,123:INFO:Creating metrics dataframe
2025-02-28 12:14:13,126:INFO:Initializing Decision Tree Classifier
2025-02-28 12:14:13,127:INFO:Total runtime is 0.16656994024912516 minutes
2025-02-28 12:14:13,129:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:13,129:INFO:Initializing create_model()
2025-02-28 12:14:13,129:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:13,129:INFO:Checking exceptions
2025-02-28 12:14:13,129:INFO:Importing libraries
2025-02-28 12:14:13,129:INFO:Copying training dataset
2025-02-28 12:14:13,134:INFO:Defining folds
2025-02-28 12:14:13,134:INFO:Declaring metric variables
2025-02-28 12:14:13,137:INFO:Importing untrained model
2025-02-28 12:14:13,139:INFO:Decision Tree Classifier Imported successfully
2025-02-28 12:14:13,143:INFO:Starting cross validation
2025-02-28 12:14:13,145:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:13,239:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,241:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,246:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,251:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,251:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,254:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,255:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:13,255:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,162:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,164:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,178:INFO:Calculating mean and std
2025-02-28 12:14:15,179:INFO:Creating metrics dataframe
2025-02-28 12:14:15,180:INFO:Uploading results into container
2025-02-28 12:14:15,180:INFO:Uploading model into container now
2025-02-28 12:14:15,180:INFO:_master_model_container: 4
2025-02-28 12:14:15,181:INFO:_display_container: 2
2025-02-28 12:14:15,181:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:14:15,181:INFO:create_model() successfully completed......................................
2025-02-28 12:14:15,290:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:15,290:INFO:Creating metrics dataframe
2025-02-28 12:14:15,296:INFO:Initializing SVM - Linear Kernel
2025-02-28 12:14:15,296:INFO:Total runtime is 0.20270905494689942 minutes
2025-02-28 12:14:15,298:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:15,298:INFO:Initializing create_model()
2025-02-28 12:14:15,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:15,298:INFO:Checking exceptions
2025-02-28 12:14:15,298:INFO:Importing libraries
2025-02-28 12:14:15,298:INFO:Copying training dataset
2025-02-28 12:14:15,301:INFO:Defining folds
2025-02-28 12:14:15,301:INFO:Declaring metric variables
2025-02-28 12:14:15,304:INFO:Importing untrained model
2025-02-28 12:14:15,306:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 12:14:15,311:INFO:Starting cross validation
2025-02-28 12:14:15,312:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:15,467:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,472:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,475:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,476:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,477:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,478:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,478:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,480:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,480:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,481:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,489:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,489:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,491:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,491:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,501:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,501:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,502:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,504:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,504:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,505:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:15,508:INFO:Calculating mean and std
2025-02-28 12:14:15,508:INFO:Creating metrics dataframe
2025-02-28 12:14:15,510:INFO:Uploading results into container
2025-02-28 12:14:15,510:INFO:Uploading model into container now
2025-02-28 12:14:15,511:INFO:_master_model_container: 5
2025-02-28 12:14:15,511:INFO:_display_container: 2
2025-02-28 12:14:15,511:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:14:15,511:INFO:create_model() successfully completed......................................
2025-02-28 12:14:15,619:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:15,620:INFO:Creating metrics dataframe
2025-02-28 12:14:15,624:INFO:Initializing Ridge Classifier
2025-02-28 12:14:15,624:INFO:Total runtime is 0.20817364851633707 minutes
2025-02-28 12:14:15,627:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:15,627:INFO:Initializing create_model()
2025-02-28 12:14:15,627:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:15,627:INFO:Checking exceptions
2025-02-28 12:14:15,627:INFO:Importing libraries
2025-02-28 12:14:15,627:INFO:Copying training dataset
2025-02-28 12:14:15,630:INFO:Defining folds
2025-02-28 12:14:15,630:INFO:Declaring metric variables
2025-02-28 12:14:15,632:INFO:Importing untrained model
2025-02-28 12:14:15,635:INFO:Ridge Classifier Imported successfully
2025-02-28 12:14:15,639:INFO:Starting cross validation
2025-02-28 12:14:15,640:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:15,744:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,745:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,747:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,748:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,748:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,749:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,750:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,751:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,754:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,757:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:15,777:INFO:Calculating mean and std
2025-02-28 12:14:15,778:INFO:Creating metrics dataframe
2025-02-28 12:14:15,779:INFO:Uploading results into container
2025-02-28 12:14:15,779:INFO:Uploading model into container now
2025-02-28 12:14:15,780:INFO:_master_model_container: 6
2025-02-28 12:14:15,780:INFO:_display_container: 2
2025-02-28 12:14:15,780:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 12:14:15,780:INFO:create_model() successfully completed......................................
2025-02-28 12:14:15,889:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:15,890:INFO:Creating metrics dataframe
2025-02-28 12:14:15,894:INFO:Initializing Random Forest Classifier
2025-02-28 12:14:15,894:INFO:Total runtime is 0.2126800020535787 minutes
2025-02-28 12:14:15,896:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:15,896:INFO:Initializing create_model()
2025-02-28 12:14:15,896:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:15,897:INFO:Checking exceptions
2025-02-28 12:14:15,897:INFO:Importing libraries
2025-02-28 12:14:15,897:INFO:Copying training dataset
2025-02-28 12:14:15,900:INFO:Defining folds
2025-02-28 12:14:15,900:INFO:Declaring metric variables
2025-02-28 12:14:15,903:INFO:Importing untrained model
2025-02-28 12:14:15,906:INFO:Random Forest Classifier Imported successfully
2025-02-28 12:14:15,910:INFO:Starting cross validation
2025-02-28 12:14:15,911:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:16,256:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,257:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,284:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,300:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,361:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,370:INFO:Calculating mean and std
2025-02-28 12:14:16,371:INFO:Creating metrics dataframe
2025-02-28 12:14:16,372:INFO:Uploading results into container
2025-02-28 12:14:16,373:INFO:Uploading model into container now
2025-02-28 12:14:16,373:INFO:_master_model_container: 7
2025-02-28 12:14:16,373:INFO:_display_container: 2
2025-02-28 12:14:16,373:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 12:14:16,373:INFO:create_model() successfully completed......................................
2025-02-28 12:14:16,489:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:16,489:INFO:Creating metrics dataframe
2025-02-28 12:14:16,494:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 12:14:16,494:INFO:Total runtime is 0.2226726253827413 minutes
2025-02-28 12:14:16,496:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:16,496:INFO:Initializing create_model()
2025-02-28 12:14:16,496:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:16,496:INFO:Checking exceptions
2025-02-28 12:14:16,496:INFO:Importing libraries
2025-02-28 12:14:16,496:INFO:Copying training dataset
2025-02-28 12:14:16,500:INFO:Defining folds
2025-02-28 12:14:16,500:INFO:Declaring metric variables
2025-02-28 12:14:16,502:INFO:Importing untrained model
2025-02-28 12:14:16,504:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 12:14:16,508:INFO:Starting cross validation
2025-02-28 12:14:16,509:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,596:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,600:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:14:16,613:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,613:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,618:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,619:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,620:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,621:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,622:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,623:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,626:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,628:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:16,631:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:16,647:INFO:Calculating mean and std
2025-02-28 12:14:16,647:INFO:Creating metrics dataframe
2025-02-28 12:14:16,648:INFO:Uploading results into container
2025-02-28 12:14:16,648:INFO:Uploading model into container now
2025-02-28 12:14:16,649:INFO:_master_model_container: 8
2025-02-28 12:14:16,649:INFO:_display_container: 2
2025-02-28 12:14:16,649:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 12:14:16,649:INFO:create_model() successfully completed......................................
2025-02-28 12:14:16,764:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:16,764:INFO:Creating metrics dataframe
2025-02-28 12:14:16,769:INFO:Initializing Ada Boost Classifier
2025-02-28 12:14:16,770:INFO:Total runtime is 0.22727389335632325 minutes
2025-02-28 12:14:16,771:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:16,771:INFO:Initializing create_model()
2025-02-28 12:14:16,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:16,772:INFO:Checking exceptions
2025-02-28 12:14:16,772:INFO:Importing libraries
2025-02-28 12:14:16,772:INFO:Copying training dataset
2025-02-28 12:14:16,775:INFO:Defining folds
2025-02-28 12:14:16,776:INFO:Declaring metric variables
2025-02-28 12:14:16,778:INFO:Importing untrained model
2025-02-28 12:14:16,780:INFO:Ada Boost Classifier Imported successfully
2025-02-28 12:14:16,784:INFO:Starting cross validation
2025-02-28 12:14:16,785:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:16,854:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,856:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,857:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,863:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,863:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,864:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,865:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:16,876:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:14:17,033:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,036:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,037:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,040:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,041:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,041:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,042:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,043:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,044:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,046:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,048:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,051:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,062:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,064:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,066:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,066:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:17,068:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,068:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:17,083:INFO:Calculating mean and std
2025-02-28 12:14:17,084:INFO:Creating metrics dataframe
2025-02-28 12:14:17,085:INFO:Uploading results into container
2025-02-28 12:14:17,085:INFO:Uploading model into container now
2025-02-28 12:14:17,085:INFO:_master_model_container: 9
2025-02-28 12:14:17,086:INFO:_display_container: 2
2025-02-28 12:14:17,086:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 12:14:17,086:INFO:create_model() successfully completed......................................
2025-02-28 12:14:17,201:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:17,201:INFO:Creating metrics dataframe
2025-02-28 12:14:17,206:INFO:Initializing Gradient Boosting Classifier
2025-02-28 12:14:17,206:INFO:Total runtime is 0.23455353180567426 minutes
2025-02-28 12:14:17,208:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:17,208:INFO:Initializing create_model()
2025-02-28 12:14:17,208:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:17,208:INFO:Checking exceptions
2025-02-28 12:14:17,208:INFO:Importing libraries
2025-02-28 12:14:17,208:INFO:Copying training dataset
2025-02-28 12:14:17,212:INFO:Defining folds
2025-02-28 12:14:17,212:INFO:Declaring metric variables
2025-02-28 12:14:17,215:INFO:Importing untrained model
2025-02-28 12:14:17,217:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 12:14:17,222:INFO:Starting cross validation
2025-02-28 12:14:17,223:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:19,280:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,284:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,317:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,320:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,321:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,323:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,323:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,325:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,325:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,328:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,333:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,335:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,357:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,359:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,365:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,367:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,382:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,384:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,390:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,393:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,406:INFO:Calculating mean and std
2025-02-28 12:14:19,407:INFO:Creating metrics dataframe
2025-02-28 12:14:19,408:INFO:Uploading results into container
2025-02-28 12:14:19,408:INFO:Uploading model into container now
2025-02-28 12:14:19,408:INFO:_master_model_container: 10
2025-02-28 12:14:19,408:INFO:_display_container: 2
2025-02-28 12:14:19,409:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:14:19,409:INFO:create_model() successfully completed......................................
2025-02-28 12:14:19,519:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:19,519:INFO:Creating metrics dataframe
2025-02-28 12:14:19,523:INFO:Initializing Linear Discriminant Analysis
2025-02-28 12:14:19,524:INFO:Total runtime is 0.27318072716395064 minutes
2025-02-28 12:14:19,526:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:19,526:INFO:Initializing create_model()
2025-02-28 12:14:19,526:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:19,526:INFO:Checking exceptions
2025-02-28 12:14:19,526:INFO:Importing libraries
2025-02-28 12:14:19,526:INFO:Copying training dataset
2025-02-28 12:14:19,530:INFO:Defining folds
2025-02-28 12:14:19,530:INFO:Declaring metric variables
2025-02-28 12:14:19,533:INFO:Importing untrained model
2025-02-28 12:14:19,535:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 12:14:19,540:INFO:Starting cross validation
2025-02-28 12:14:19,541:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:19,636:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,637:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,638:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,638:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,639:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,639:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,640:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,641:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,646:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,646:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,648:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,649:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,653:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,654:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,655:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,656:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,658:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,659:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:14:19,661:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,662:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:19,674:INFO:Calculating mean and std
2025-02-28 12:14:19,675:INFO:Creating metrics dataframe
2025-02-28 12:14:19,676:INFO:Uploading results into container
2025-02-28 12:14:19,676:INFO:Uploading model into container now
2025-02-28 12:14:19,676:INFO:_master_model_container: 11
2025-02-28 12:14:19,676:INFO:_display_container: 2
2025-02-28 12:14:19,677:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 12:14:19,677:INFO:create_model() successfully completed......................................
2025-02-28 12:14:19,784:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:19,784:INFO:Creating metrics dataframe
2025-02-28 12:14:19,789:INFO:Initializing Extra Trees Classifier
2025-02-28 12:14:19,789:INFO:Total runtime is 0.27759943803151454 minutes
2025-02-28 12:14:19,791:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:19,792:INFO:Initializing create_model()
2025-02-28 12:14:19,792:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:19,792:INFO:Checking exceptions
2025-02-28 12:14:19,792:INFO:Importing libraries
2025-02-28 12:14:19,792:INFO:Copying training dataset
2025-02-28 12:14:19,795:INFO:Defining folds
2025-02-28 12:14:19,795:INFO:Declaring metric variables
2025-02-28 12:14:19,797:INFO:Importing untrained model
2025-02-28 12:14:19,799:INFO:Extra Trees Classifier Imported successfully
2025-02-28 12:14:19,804:INFO:Starting cross validation
2025-02-28 12:14:19,806:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:20,119:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,119:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,121:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,134:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,134:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,135:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,149:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,158:INFO:Calculating mean and std
2025-02-28 12:14:20,159:INFO:Creating metrics dataframe
2025-02-28 12:14:20,160:INFO:Uploading results into container
2025-02-28 12:14:20,160:INFO:Uploading model into container now
2025-02-28 12:14:20,160:INFO:_master_model_container: 12
2025-02-28 12:14:20,160:INFO:_display_container: 2
2025-02-28 12:14:20,161:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 12:14:20,161:INFO:create_model() successfully completed......................................
2025-02-28 12:14:20,270:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:20,270:INFO:Creating metrics dataframe
2025-02-28 12:14:20,276:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:14:20,276:INFO:Total runtime is 0.28570960362752285 minutes
2025-02-28 12:14:20,278:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:20,279:INFO:Initializing create_model()
2025-02-28 12:14:20,279:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:20,279:INFO:Checking exceptions
2025-02-28 12:14:20,279:INFO:Importing libraries
2025-02-28 12:14:20,279:INFO:Copying training dataset
2025-02-28 12:14:20,281:INFO:Defining folds
2025-02-28 12:14:20,281:INFO:Declaring metric variables
2025-02-28 12:14:20,284:INFO:Importing untrained model
2025-02-28 12:14:20,287:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:14:20,291:INFO:Starting cross validation
2025-02-28 12:14:20,292:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:20,910:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,925:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,926:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,932:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,938:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,943:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,965:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,977:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,981:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,986:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:20,998:INFO:Calculating mean and std
2025-02-28 12:14:20,999:INFO:Creating metrics dataframe
2025-02-28 12:14:21,000:INFO:Uploading results into container
2025-02-28 12:14:21,000:INFO:Uploading model into container now
2025-02-28 12:14:21,001:INFO:_master_model_container: 13
2025-02-28 12:14:21,001:INFO:_display_container: 2
2025-02-28 12:14:21,001:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 12:14:21,001:INFO:create_model() successfully completed......................................
2025-02-28 12:14:21,110:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:21,110:INFO:Creating metrics dataframe
2025-02-28 12:14:21,115:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:14:21,116:INFO:Total runtime is 0.2997180938720704 minutes
2025-02-28 12:14:21,118:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:21,118:INFO:Initializing create_model()
2025-02-28 12:14:21,118:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:21,118:INFO:Checking exceptions
2025-02-28 12:14:21,118:INFO:Importing libraries
2025-02-28 12:14:21,118:INFO:Copying training dataset
2025-02-28 12:14:21,122:INFO:Defining folds
2025-02-28 12:14:21,122:INFO:Declaring metric variables
2025-02-28 12:14:21,125:INFO:Importing untrained model
2025-02-28 12:14:21,127:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:14:21,131:INFO:Starting cross validation
2025-02-28 12:14:21,132:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:24,555:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,579:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,626:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,664:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,665:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,713:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,759:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,783:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:24,988:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:25,007:INFO:Calculating mean and std
2025-02-28 12:14:25,008:INFO:Creating metrics dataframe
2025-02-28 12:14:25,009:INFO:Uploading results into container
2025-02-28 12:14:25,009:INFO:Uploading model into container now
2025-02-28 12:14:25,010:INFO:_master_model_container: 14
2025-02-28 12:14:25,010:INFO:_display_container: 2
2025-02-28 12:14:25,010:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:14:25,010:INFO:create_model() successfully completed......................................
2025-02-28 12:14:25,131:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:25,131:INFO:Creating metrics dataframe
2025-02-28 12:14:25,137:INFO:Initializing CatBoost Classifier
2025-02-28 12:14:25,137:INFO:Total runtime is 0.36673409541447965 minutes
2025-02-28 12:14:25,139:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:25,139:INFO:Initializing create_model()
2025-02-28 12:14:25,139:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:25,139:INFO:Checking exceptions
2025-02-28 12:14:25,139:INFO:Importing libraries
2025-02-28 12:14:25,139:INFO:Copying training dataset
2025-02-28 12:14:25,143:INFO:Defining folds
2025-02-28 12:14:25,143:INFO:Declaring metric variables
2025-02-28 12:14:25,146:INFO:Importing untrained model
2025-02-28 12:14:25,148:INFO:CatBoost Classifier Imported successfully
2025-02-28 12:14:25,153:INFO:Starting cross validation
2025-02-28 12:14:25,154:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:32,425:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,430:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,432:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,442:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,448:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,454:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,454:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,456:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,457:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,461:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
1 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pipeline.py", line 278, in fit
    fitted_estimator = self._memory_fit(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\catboost\core.py", line 5245, in fit
    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\catboost\core.py", line 2410, in _fit
    self._train(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\catboost\core.py", line 1790, in _train
    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)
  File "_catboost.pyx", line 5017, in _catboost._CatBoost._train
  File "_catboost.pyx", line 5066, in _catboost._CatBoost._train
_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:26: Can't create train tmp dir: tmp

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-02-28 12:14:32,461:INFO:Calculating mean and std
2025-02-28 12:14:32,462:INFO:Creating metrics dataframe
2025-02-28 12:14:32,462:INFO:Uploading results into container
2025-02-28 12:14:32,464:INFO:Uploading model into container now
2025-02-28 12:14:32,464:INFO:_master_model_container: 15
2025-02-28 12:14:32,464:INFO:_display_container: 2
2025-02-28 12:14:32,464:INFO:<catboost.core.CatBoostClassifier object at 0x0000024E59816C80>
2025-02-28 12:14:32,464:INFO:create_model() successfully completed......................................
2025-02-28 12:14:32,569:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:32,569:INFO:Creating metrics dataframe
2025-02-28 12:14:32,576:INFO:Initializing Dummy Classifier
2025-02-28 12:14:32,576:INFO:Total runtime is 0.49071048498153697 minutes
2025-02-28 12:14:32,578:INFO:SubProcess create_model() called ==================================
2025-02-28 12:14:32,579:INFO:Initializing create_model()
2025-02-28 12:14:32,579:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE8B700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:32,579:INFO:Checking exceptions
2025-02-28 12:14:32,579:INFO:Importing libraries
2025-02-28 12:14:32,579:INFO:Copying training dataset
2025-02-28 12:14:32,581:INFO:Defining folds
2025-02-28 12:14:32,582:INFO:Declaring metric variables
2025-02-28 12:14:32,584:INFO:Importing untrained model
2025-02-28 12:14:32,587:INFO:Dummy Classifier Imported successfully
2025-02-28 12:14:32,591:INFO:Starting cross validation
2025-02-28 12:14:32,592:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:14:32,677:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,685:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,688:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,689:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,696:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,699:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,707:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,710:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,710:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,714:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:14:32,726:INFO:Calculating mean and std
2025-02-28 12:14:32,727:INFO:Creating metrics dataframe
2025-02-28 12:14:32,728:INFO:Uploading results into container
2025-02-28 12:14:32,728:INFO:Uploading model into container now
2025-02-28 12:14:32,728:INFO:_master_model_container: 16
2025-02-28 12:14:32,729:INFO:_display_container: 2
2025-02-28 12:14:32,729:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 12:14:32,729:INFO:create_model() successfully completed......................................
2025-02-28 12:14:32,835:INFO:SubProcess create_model() end ==================================
2025-02-28 12:14:32,835:INFO:Creating metrics dataframe
2025-02-28 12:14:32,842:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:14:32,847:INFO:Initializing create_model()
2025-02-28 12:14:32,847:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:14:32,847:INFO:Checking exceptions
2025-02-28 12:14:32,848:INFO:Importing libraries
2025-02-28 12:14:32,848:INFO:Copying training dataset
2025-02-28 12:14:32,854:INFO:Defining folds
2025-02-28 12:14:32,854:INFO:Declaring metric variables
2025-02-28 12:14:32,854:INFO:Importing untrained model
2025-02-28 12:14:32,854:INFO:Declaring custom model
2025-02-28 12:14:32,854:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:14:32,856:INFO:Cross validation set to False
2025-02-28 12:14:32,856:INFO:Fitting Model
2025-02-28 12:14:32,905:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:14:32,905:INFO:create_model() successfully completed......................................
2025-02-28 12:14:33,026:INFO:_master_model_container: 16
2025-02-28 12:14:33,026:INFO:_display_container: 2
2025-02-28 12:14:33,026:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:14:33,026:INFO:compare_models() successfully completed......................................
2025-02-28 12:17:20,901:INFO:Initializing plot_model()
2025-02-28 12:17:20,901:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, system=True)
2025-02-28 12:17:20,901:INFO:Checking exceptions
2025-02-28 12:17:20,905:INFO:Preloading libraries
2025-02-28 12:17:20,906:INFO:Copying training dataset
2025-02-28 12:17:20,906:INFO:Plot type: confusion_matrix
2025-02-28 12:17:21,114:INFO:Fitting Model
2025-02-28 12:17:21,115:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:17:21,116:INFO:Scoring test/hold-out set
2025-02-28 12:17:21,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\externals\loky\backend\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:
[WinError 2] The system cannot find the file specified
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(

2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\externals\loky\backend\context.py", line 257, in _count_physical_cores
2025-02-28 12:17:21,121:WARNING:    cpu_info = subprocess.run(
2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\subprocess.py", line 503, in run
2025-02-28 12:17:21,121:WARNING:    with Popen(*popenargs, **kwargs) as process:
2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\subprocess.py", line 971, in __init__
2025-02-28 12:17:21,121:WARNING:    self._execute_child(args, executable, preexec_fn, close_fds,
2025-02-28 12:17:21,121:WARNING:  File "c:\Users\dagir\miniconda3\envs\pyca\lib\subprocess.py", line 1456, in _execute_child
2025-02-28 12:17:21,121:WARNING:    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
2025-02-28 12:17:21,491:INFO:Visual Rendered Successfully
2025-02-28 12:17:21,611:INFO:plot_model() successfully completed......................................
2025-02-28 12:17:34,242:INFO:Initializing plot_model()
2025-02-28 12:17:34,242:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, system=True)
2025-02-28 12:17:34,243:INFO:Checking exceptions
2025-02-28 12:17:34,246:INFO:Preloading libraries
2025-02-28 12:17:34,246:INFO:Copying training dataset
2025-02-28 12:17:34,246:INFO:Plot type: auc
2025-02-28 12:17:34,444:INFO:Fitting Model
2025-02-28 12:17:34,444:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:17:34,444:INFO:Scoring test/hold-out set
2025-02-28 12:17:34,773:INFO:Visual Rendered Successfully
2025-02-28 12:17:34,881:INFO:plot_model() successfully completed......................................
2025-02-28 12:17:43,719:INFO:Initializing plot_model()
2025-02-28 12:17:43,720:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E491ED270>, system=True)
2025-02-28 12:17:43,720:INFO:Checking exceptions
2025-02-28 12:19:23,147:INFO:PyCaret RegressionExperiment
2025-02-28 12:19:23,147:INFO:Logging name: reg-default-name
2025-02-28 12:19:23,147:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 12:19:23,147:INFO:version 3.3.2
2025-02-28 12:19:23,147:INFO:Initializing setup()
2025-02-28 12:19:23,147:INFO:self.USI: eab2
2025-02-28 12:19:23,148:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'transform_target_param', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:19:23,148:INFO:Checking environment
2025-02-28 12:19:23,148:INFO:python_version: 3.10.16
2025-02-28 12:19:23,148:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:19:23,148:INFO:machine: AMD64
2025-02-28 12:19:23,148:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:19:23,148:INFO:Memory: svmem(total=34200334336, available=12409036800, percent=63.7, used=21791297536, free=12409036800)
2025-02-28 12:19:23,148:INFO:Physical Core: 24
2025-02-28 12:19:23,148:INFO:Logical Core: 32
2025-02-28 12:19:23,148:INFO:Checking libraries
2025-02-28 12:19:23,148:INFO:System:
2025-02-28 12:19:23,148:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:19:23,148:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:19:23,148:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:19:23,148:INFO:PyCaret required dependencies:
2025-02-28 12:19:23,148:INFO:                 pip: 25.0
2025-02-28 12:19:23,148:INFO:          setuptools: 75.8.0
2025-02-28 12:19:23,148:INFO:             pycaret: 3.3.2
2025-02-28 12:19:23,148:INFO:             IPython: 8.30.0
2025-02-28 12:19:23,148:INFO:          ipywidgets: 8.1.5
2025-02-28 12:19:23,148:INFO:                tqdm: 4.67.1
2025-02-28 12:19:23,149:INFO:               numpy: 1.26.4
2025-02-28 12:19:23,149:INFO:              pandas: 2.1.4
2025-02-28 12:19:23,149:INFO:              jinja2: 3.1.5
2025-02-28 12:19:23,149:INFO:               scipy: 1.11.4
2025-02-28 12:19:23,149:INFO:              joblib: 1.3.2
2025-02-28 12:19:23,149:INFO:             sklearn: 1.4.2
2025-02-28 12:19:23,149:INFO:                pyod: 2.0.3
2025-02-28 12:19:23,149:INFO:            imblearn: 0.13.0
2025-02-28 12:19:23,149:INFO:   category_encoders: 2.7.0
2025-02-28 12:19:23,149:INFO:            lightgbm: 4.5.0
2025-02-28 12:19:23,149:INFO:               numba: 0.61.0
2025-02-28 12:19:23,149:INFO:            requests: 2.32.3
2025-02-28 12:19:23,149:INFO:          matplotlib: 3.7.5
2025-02-28 12:19:23,149:INFO:          scikitplot: 0.3.7
2025-02-28 12:19:23,149:INFO:         yellowbrick: 1.5
2025-02-28 12:19:23,149:INFO:              plotly: 5.24.1
2025-02-28 12:19:23,149:INFO:    plotly-resampler: Not installed
2025-02-28 12:19:23,149:INFO:             kaleido: 0.2.1
2025-02-28 12:19:23,149:INFO:           schemdraw: 0.15
2025-02-28 12:19:23,149:INFO:         statsmodels: 0.14.4
2025-02-28 12:19:23,149:INFO:              sktime: 0.26.0
2025-02-28 12:19:23,149:INFO:               tbats: 1.1.3
2025-02-28 12:19:23,149:INFO:            pmdarima: 2.0.4
2025-02-28 12:19:23,149:INFO:              psutil: 5.9.0
2025-02-28 12:19:23,149:INFO:          markupsafe: 2.1.5
2025-02-28 12:19:23,149:INFO:             pickle5: Not installed
2025-02-28 12:19:23,149:INFO:         cloudpickle: 3.1.1
2025-02-28 12:19:23,149:INFO:         deprecation: 2.1.0
2025-02-28 12:19:23,149:INFO:              xxhash: 3.5.0
2025-02-28 12:19:23,149:INFO:           wurlitzer: Not installed
2025-02-28 12:19:23,149:INFO:PyCaret optional dependencies:
2025-02-28 12:19:23,149:INFO:                shap: 0.44.1
2025-02-28 12:19:23,149:INFO:           interpret: 0.6.9
2025-02-28 12:19:23,149:INFO:                umap: 0.5.7
2025-02-28 12:19:23,149:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:19:23,149:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:19:23,149:INFO:             autoviz: Not installed
2025-02-28 12:19:23,149:INFO:           fairlearn: 0.7.0
2025-02-28 12:19:23,149:INFO:          deepchecks: Not installed
2025-02-28 12:19:23,149:INFO:             xgboost: 2.1.4
2025-02-28 12:19:23,150:INFO:            catboost: 1.2.7
2025-02-28 12:19:23,150:INFO:              kmodes: 0.12.2
2025-02-28 12:19:23,150:INFO:             mlxtend: 0.23.4
2025-02-28 12:19:23,150:INFO:       statsforecast: 1.5.0
2025-02-28 12:19:23,150:INFO:        tune_sklearn: Not installed
2025-02-28 12:19:23,150:INFO:                 ray: Not installed
2025-02-28 12:19:23,150:INFO:            hyperopt: 0.2.7
2025-02-28 12:19:23,150:INFO:              optuna: 4.2.0
2025-02-28 12:19:23,150:INFO:               skopt: 0.10.2
2025-02-28 12:19:23,150:INFO:              mlflow: 2.20.1
2025-02-28 12:19:23,150:INFO:              gradio: 5.15.0
2025-02-28 12:19:23,150:INFO:             fastapi: 0.115.8
2025-02-28 12:19:23,150:INFO:             uvicorn: 0.34.0
2025-02-28 12:19:23,150:INFO:              m2cgen: 0.10.0
2025-02-28 12:19:23,150:INFO:           evidently: 0.4.40
2025-02-28 12:19:23,150:INFO:               fugue: 0.8.7
2025-02-28 12:19:23,150:INFO:           streamlit: Not installed
2025-02-28 12:19:23,150:INFO:             prophet: Not installed
2025-02-28 12:19:23,150:INFO:None
2025-02-28 12:19:23,150:INFO:Set up data.
2025-02-28 12:19:23,156:INFO:Set up folding strategy.
2025-02-28 12:19:23,157:INFO:Set up train/test split.
2025-02-28 12:19:23,160:INFO:Set up index.
2025-02-28 12:19:23,161:INFO:Assigning column types.
2025-02-28 12:19:23,163:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:19:23,163:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,165:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,167:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,193:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,213:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,213:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,214:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,215:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,217:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,219:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,245:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,264:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,265:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,266:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,266:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 12:19:23,268:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,270:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,296:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,315:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,315:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,317:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,319:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,321:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,346:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,365:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,366:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,367:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,367:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 12:19:23,372:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,396:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,415:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,415:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,416:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,420:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,445:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,465:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,465:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,466:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,467:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 12:19:23,497:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,516:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,517:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,518:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,548:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,567:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,567:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,568:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,569:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:19:23,599:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,618:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,619:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,649:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:19:23,668:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,670:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,670:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 12:19:23,719:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,720:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,768:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:23,769:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:23,772:INFO:Preparing preprocessing pipeline...
2025-02-28 12:19:23,772:INFO:Set up simple imputation.
2025-02-28 12:19:23,774:INFO:Set up encoding of ordinal features.
2025-02-28 12:19:23,775:INFO:Set up encoding of categorical features.
2025-02-28 12:19:23,846:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:19:23,854:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score', 'Job_Offers',
                                             'Career_Satisfa...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Student_ID'],
                                    transformer=TargetEncoder(cols=['Student_ID'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:19:23,854:INFO:Creating final display dataframe.
2025-02-28 12:19:24,054:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 20)
4        Transformed data shape        (5000, 31)
5   Transformed train set shape        (3500, 31)
6    Transformed test set shape        (1500, 31)
7              Numeric features                14
8          Categorical features                 5
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              eab2
2025-02-28 12:19:24,109:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:24,110:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:24,160:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:19:24,161:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:19:24,161:INFO:setup() successfully completed in 1.02s...............
2025-02-28 12:19:24,182:INFO:Initializing compare_models()
2025-02-28 12:19:24,182:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 12:19:24,182:INFO:Checking exceptions
2025-02-28 12:19:24,184:INFO:Preparing display monitor
2025-02-28 12:19:24,202:INFO:Initializing Linear Regression
2025-02-28 12:19:24,202:INFO:Total runtime is 0.0 minutes
2025-02-28 12:19:24,205:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:24,206:INFO:Initializing create_model()
2025-02-28 12:19:24,206:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:24,206:INFO:Checking exceptions
2025-02-28 12:19:24,206:INFO:Importing libraries
2025-02-28 12:19:24,206:INFO:Copying training dataset
2025-02-28 12:19:24,209:INFO:Defining folds
2025-02-28 12:19:24,209:INFO:Declaring metric variables
2025-02-28 12:19:24,212:INFO:Importing untrained model
2025-02-28 12:19:24,216:INFO:Linear Regression Imported successfully
2025-02-28 12:19:24,222:INFO:Starting cross validation
2025-02-28 12:19:24,223:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:24,376:INFO:Calculating mean and std
2025-02-28 12:19:24,376:INFO:Creating metrics dataframe
2025-02-28 12:19:24,377:INFO:Uploading results into container
2025-02-28 12:19:24,377:INFO:Uploading model into container now
2025-02-28 12:19:24,378:INFO:_master_model_container: 1
2025-02-28 12:19:24,378:INFO:_display_container: 2
2025-02-28 12:19:24,378:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 12:19:24,378:INFO:create_model() successfully completed......................................
2025-02-28 12:19:24,507:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:24,507:INFO:Creating metrics dataframe
2025-02-28 12:19:24,511:INFO:Initializing Lasso Regression
2025-02-28 12:19:24,511:INFO:Total runtime is 0.005150417486826579 minutes
2025-02-28 12:19:24,513:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:24,513:INFO:Initializing create_model()
2025-02-28 12:19:24,513:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:24,514:INFO:Checking exceptions
2025-02-28 12:19:24,514:INFO:Importing libraries
2025-02-28 12:19:24,514:INFO:Copying training dataset
2025-02-28 12:19:24,517:INFO:Defining folds
2025-02-28 12:19:24,517:INFO:Declaring metric variables
2025-02-28 12:19:24,519:INFO:Importing untrained model
2025-02-28 12:19:24,521:INFO:Lasso Regression Imported successfully
2025-02-28 12:19:24,526:INFO:Starting cross validation
2025-02-28 12:19:24,527:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:24,654:INFO:Calculating mean and std
2025-02-28 12:19:24,654:INFO:Creating metrics dataframe
2025-02-28 12:19:24,655:INFO:Uploading results into container
2025-02-28 12:19:24,655:INFO:Uploading model into container now
2025-02-28 12:19:24,656:INFO:_master_model_container: 2
2025-02-28 12:19:24,656:INFO:_display_container: 2
2025-02-28 12:19:24,656:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 12:19:24,656:INFO:create_model() successfully completed......................................
2025-02-28 12:19:24,784:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:24,784:INFO:Creating metrics dataframe
2025-02-28 12:19:24,788:INFO:Initializing Ridge Regression
2025-02-28 12:19:24,788:INFO:Total runtime is 0.009762648741404215 minutes
2025-02-28 12:19:24,790:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:24,790:INFO:Initializing create_model()
2025-02-28 12:19:24,790:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:24,790:INFO:Checking exceptions
2025-02-28 12:19:24,790:INFO:Importing libraries
2025-02-28 12:19:24,790:INFO:Copying training dataset
2025-02-28 12:19:24,793:INFO:Defining folds
2025-02-28 12:19:24,793:INFO:Declaring metric variables
2025-02-28 12:19:24,795:INFO:Importing untrained model
2025-02-28 12:19:24,797:INFO:Ridge Regression Imported successfully
2025-02-28 12:19:24,801:INFO:Starting cross validation
2025-02-28 12:19:24,803:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:24,937:INFO:Calculating mean and std
2025-02-28 12:19:24,938:INFO:Creating metrics dataframe
2025-02-28 12:19:24,939:INFO:Uploading results into container
2025-02-28 12:19:24,939:INFO:Uploading model into container now
2025-02-28 12:19:24,939:INFO:_master_model_container: 3
2025-02-28 12:19:24,939:INFO:_display_container: 2
2025-02-28 12:19:24,939:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 12:19:24,939:INFO:create_model() successfully completed......................................
2025-02-28 12:19:25,066:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:25,066:INFO:Creating metrics dataframe
2025-02-28 12:19:25,069:INFO:Initializing Elastic Net
2025-02-28 12:19:25,071:INFO:Total runtime is 0.014487226804097492 minutes
2025-02-28 12:19:25,073:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:25,073:INFO:Initializing create_model()
2025-02-28 12:19:25,073:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:25,073:INFO:Checking exceptions
2025-02-28 12:19:25,073:INFO:Importing libraries
2025-02-28 12:19:25,073:INFO:Copying training dataset
2025-02-28 12:19:25,076:INFO:Defining folds
2025-02-28 12:19:25,076:INFO:Declaring metric variables
2025-02-28 12:19:25,078:INFO:Importing untrained model
2025-02-28 12:19:25,081:INFO:Elastic Net Imported successfully
2025-02-28 12:19:25,086:INFO:Starting cross validation
2025-02-28 12:19:25,087:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:25,218:INFO:Calculating mean and std
2025-02-28 12:19:25,219:INFO:Creating metrics dataframe
2025-02-28 12:19:25,220:INFO:Uploading results into container
2025-02-28 12:19:25,220:INFO:Uploading model into container now
2025-02-28 12:19:25,220:INFO:_master_model_container: 4
2025-02-28 12:19:25,220:INFO:_display_container: 2
2025-02-28 12:19:25,221:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 12:19:25,221:INFO:create_model() successfully completed......................................
2025-02-28 12:19:25,345:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:25,345:INFO:Creating metrics dataframe
2025-02-28 12:19:25,349:INFO:Initializing Least Angle Regression
2025-02-28 12:19:25,349:INFO:Total runtime is 0.019125668207804362 minutes
2025-02-28 12:19:25,352:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:25,352:INFO:Initializing create_model()
2025-02-28 12:19:25,352:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:25,352:INFO:Checking exceptions
2025-02-28 12:19:25,352:INFO:Importing libraries
2025-02-28 12:19:25,352:INFO:Copying training dataset
2025-02-28 12:19:25,355:INFO:Defining folds
2025-02-28 12:19:25,355:INFO:Declaring metric variables
2025-02-28 12:19:25,358:INFO:Importing untrained model
2025-02-28 12:19:25,360:INFO:Least Angle Regression Imported successfully
2025-02-28 12:19:25,364:INFO:Starting cross validation
2025-02-28 12:19:25,365:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:27,424:INFO:Calculating mean and std
2025-02-28 12:19:27,425:INFO:Creating metrics dataframe
2025-02-28 12:19:27,426:INFO:Uploading results into container
2025-02-28 12:19:27,426:INFO:Uploading model into container now
2025-02-28 12:19:27,427:INFO:_master_model_container: 5
2025-02-28 12:19:27,427:INFO:_display_container: 2
2025-02-28 12:19:27,427:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 12:19:27,427:INFO:create_model() successfully completed......................................
2025-02-28 12:19:27,559:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:27,559:INFO:Creating metrics dataframe
2025-02-28 12:19:27,562:INFO:Initializing Lasso Least Angle Regression
2025-02-28 12:19:27,562:INFO:Total runtime is 0.0560053547223409 minutes
2025-02-28 12:19:27,565:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:27,565:INFO:Initializing create_model()
2025-02-28 12:19:27,565:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:27,566:INFO:Checking exceptions
2025-02-28 12:19:27,566:INFO:Importing libraries
2025-02-28 12:19:27,566:INFO:Copying training dataset
2025-02-28 12:19:27,569:INFO:Defining folds
2025-02-28 12:19:27,569:INFO:Declaring metric variables
2025-02-28 12:19:27,572:INFO:Importing untrained model
2025-02-28 12:19:27,575:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 12:19:27,579:INFO:Starting cross validation
2025-02-28 12:19:27,581:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:27,721:INFO:Calculating mean and std
2025-02-28 12:19:27,721:INFO:Creating metrics dataframe
2025-02-28 12:19:27,723:INFO:Uploading results into container
2025-02-28 12:19:27,723:INFO:Uploading model into container now
2025-02-28 12:19:27,723:INFO:_master_model_container: 6
2025-02-28 12:19:27,723:INFO:_display_container: 2
2025-02-28 12:19:27,724:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 12:19:27,724:INFO:create_model() successfully completed......................................
2025-02-28 12:19:27,850:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:27,850:INFO:Creating metrics dataframe
2025-02-28 12:19:27,855:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 12:19:27,855:INFO:Total runtime is 0.06088508764902751 minutes
2025-02-28 12:19:27,857:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:27,857:INFO:Initializing create_model()
2025-02-28 12:19:27,858:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:27,858:INFO:Checking exceptions
2025-02-28 12:19:27,858:INFO:Importing libraries
2025-02-28 12:19:27,858:INFO:Copying training dataset
2025-02-28 12:19:27,861:INFO:Defining folds
2025-02-28 12:19:27,861:INFO:Declaring metric variables
2025-02-28 12:19:27,863:INFO:Importing untrained model
2025-02-28 12:19:27,865:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 12:19:27,868:INFO:Starting cross validation
2025-02-28 12:19:27,870:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:27,940:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,943:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,950:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,958:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,960:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,961:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,962:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py:186: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.
  return func(*args, **kwargs)

2025-02-28 12:19:27,984:INFO:Calculating mean and std
2025-02-28 12:19:27,984:INFO:Creating metrics dataframe
2025-02-28 12:19:27,985:INFO:Uploading results into container
2025-02-28 12:19:27,986:INFO:Uploading model into container now
2025-02-28 12:19:27,986:INFO:_master_model_container: 7
2025-02-28 12:19:27,986:INFO:_display_container: 2
2025-02-28 12:19:27,986:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 12:19:27,987:INFO:create_model() successfully completed......................................
2025-02-28 12:19:28,118:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:28,118:INFO:Creating metrics dataframe
2025-02-28 12:19:28,123:INFO:Initializing Bayesian Ridge
2025-02-28 12:19:28,123:INFO:Total runtime is 0.06535278161366781 minutes
2025-02-28 12:19:28,125:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:28,125:INFO:Initializing create_model()
2025-02-28 12:19:28,126:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:28,126:INFO:Checking exceptions
2025-02-28 12:19:28,126:INFO:Importing libraries
2025-02-28 12:19:28,126:INFO:Copying training dataset
2025-02-28 12:19:28,129:INFO:Defining folds
2025-02-28 12:19:28,129:INFO:Declaring metric variables
2025-02-28 12:19:28,131:INFO:Importing untrained model
2025-02-28 12:19:28,134:INFO:Bayesian Ridge Imported successfully
2025-02-28 12:19:28,138:INFO:Starting cross validation
2025-02-28 12:19:28,139:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:28,279:INFO:Calculating mean and std
2025-02-28 12:19:28,280:INFO:Creating metrics dataframe
2025-02-28 12:19:28,281:INFO:Uploading results into container
2025-02-28 12:19:28,281:INFO:Uploading model into container now
2025-02-28 12:19:28,281:INFO:_master_model_container: 8
2025-02-28 12:19:28,281:INFO:_display_container: 2
2025-02-28 12:19:28,281:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 12:19:28,281:INFO:create_model() successfully completed......................................
2025-02-28 12:19:28,406:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:28,406:INFO:Creating metrics dataframe
2025-02-28 12:19:28,410:INFO:Initializing Passive Aggressive Regressor
2025-02-28 12:19:28,410:INFO:Total runtime is 0.0701412041982015 minutes
2025-02-28 12:19:28,412:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:28,412:INFO:Initializing create_model()
2025-02-28 12:19:28,412:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:28,412:INFO:Checking exceptions
2025-02-28 12:19:28,412:INFO:Importing libraries
2025-02-28 12:19:28,412:INFO:Copying training dataset
2025-02-28 12:19:28,415:INFO:Defining folds
2025-02-28 12:19:28,416:INFO:Declaring metric variables
2025-02-28 12:19:28,418:INFO:Importing untrained model
2025-02-28 12:19:28,421:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 12:19:28,425:INFO:Starting cross validation
2025-02-28 12:19:28,426:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:28,561:INFO:Calculating mean and std
2025-02-28 12:19:28,561:INFO:Creating metrics dataframe
2025-02-28 12:19:28,562:INFO:Uploading results into container
2025-02-28 12:19:28,562:INFO:Uploading model into container now
2025-02-28 12:19:28,563:INFO:_master_model_container: 9
2025-02-28 12:19:28,563:INFO:_display_container: 2
2025-02-28 12:19:28,563:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:19:28,563:INFO:create_model() successfully completed......................................
2025-02-28 12:19:28,692:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:28,692:INFO:Creating metrics dataframe
2025-02-28 12:19:28,696:INFO:Initializing Huber Regressor
2025-02-28 12:19:28,696:INFO:Total runtime is 0.07489873568216959 minutes
2025-02-28 12:19:28,698:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:28,699:INFO:Initializing create_model()
2025-02-28 12:19:28,699:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:28,699:INFO:Checking exceptions
2025-02-28 12:19:28,699:INFO:Importing libraries
2025-02-28 12:19:28,699:INFO:Copying training dataset
2025-02-28 12:19:28,702:INFO:Defining folds
2025-02-28 12:19:28,702:INFO:Declaring metric variables
2025-02-28 12:19:28,704:INFO:Importing untrained model
2025-02-28 12:19:28,707:INFO:Huber Regressor Imported successfully
2025-02-28 12:19:28,713:INFO:Starting cross validation
2025-02-28 12:19:28,715:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:28,822:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:19:28,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:19:28,840:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:19:28,874:INFO:Calculating mean and std
2025-02-28 12:19:28,874:INFO:Creating metrics dataframe
2025-02-28 12:19:28,876:INFO:Uploading results into container
2025-02-28 12:19:28,876:INFO:Uploading model into container now
2025-02-28 12:19:28,876:INFO:_master_model_container: 10
2025-02-28 12:19:28,876:INFO:_display_container: 2
2025-02-28 12:19:28,876:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 12:19:28,876:INFO:create_model() successfully completed......................................
2025-02-28 12:19:29,001:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:29,001:INFO:Creating metrics dataframe
2025-02-28 12:19:29,005:INFO:Initializing K Neighbors Regressor
2025-02-28 12:19:29,006:INFO:Total runtime is 0.08007542292277017 minutes
2025-02-28 12:19:29,008:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:29,008:INFO:Initializing create_model()
2025-02-28 12:19:29,008:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:29,008:INFO:Checking exceptions
2025-02-28 12:19:29,008:INFO:Importing libraries
2025-02-28 12:19:29,008:INFO:Copying training dataset
2025-02-28 12:19:29,011:INFO:Defining folds
2025-02-28 12:19:29,012:INFO:Declaring metric variables
2025-02-28 12:19:29,014:INFO:Importing untrained model
2025-02-28 12:19:29,017:INFO:K Neighbors Regressor Imported successfully
2025-02-28 12:19:29,021:INFO:Starting cross validation
2025-02-28 12:19:29,022:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:29,156:INFO:Calculating mean and std
2025-02-28 12:19:29,157:INFO:Creating metrics dataframe
2025-02-28 12:19:29,158:INFO:Uploading results into container
2025-02-28 12:19:29,158:INFO:Uploading model into container now
2025-02-28 12:19:29,158:INFO:_master_model_container: 11
2025-02-28 12:19:29,158:INFO:_display_container: 2
2025-02-28 12:19:29,159:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 12:19:29,159:INFO:create_model() successfully completed......................................
2025-02-28 12:19:29,289:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:29,289:INFO:Creating metrics dataframe
2025-02-28 12:19:29,293:INFO:Initializing Decision Tree Regressor
2025-02-28 12:19:29,293:INFO:Total runtime is 0.08485944271087646 minutes
2025-02-28 12:19:29,295:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:29,296:INFO:Initializing create_model()
2025-02-28 12:19:29,296:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:29,296:INFO:Checking exceptions
2025-02-28 12:19:29,297:INFO:Importing libraries
2025-02-28 12:19:29,297:INFO:Copying training dataset
2025-02-28 12:19:29,300:INFO:Defining folds
2025-02-28 12:19:29,300:INFO:Declaring metric variables
2025-02-28 12:19:29,302:INFO:Importing untrained model
2025-02-28 12:19:29,304:INFO:Decision Tree Regressor Imported successfully
2025-02-28 12:19:29,309:INFO:Starting cross validation
2025-02-28 12:19:29,309:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:29,454:INFO:Calculating mean and std
2025-02-28 12:19:29,454:INFO:Creating metrics dataframe
2025-02-28 12:19:29,456:INFO:Uploading results into container
2025-02-28 12:19:29,456:INFO:Uploading model into container now
2025-02-28 12:19:29,456:INFO:_master_model_container: 12
2025-02-28 12:19:29,456:INFO:_display_container: 2
2025-02-28 12:19:29,456:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:19:29,456:INFO:create_model() successfully completed......................................
2025-02-28 12:19:29,584:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:29,585:INFO:Creating metrics dataframe
2025-02-28 12:19:29,589:INFO:Initializing Random Forest Regressor
2025-02-28 12:19:29,589:INFO:Total runtime is 0.08978945811589559 minutes
2025-02-28 12:19:29,591:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:29,592:INFO:Initializing create_model()
2025-02-28 12:19:29,592:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:29,592:INFO:Checking exceptions
2025-02-28 12:19:29,592:INFO:Importing libraries
2025-02-28 12:19:29,592:INFO:Copying training dataset
2025-02-28 12:19:29,595:INFO:Defining folds
2025-02-28 12:19:29,595:INFO:Declaring metric variables
2025-02-28 12:19:29,598:INFO:Importing untrained model
2025-02-28 12:19:29,601:INFO:Random Forest Regressor Imported successfully
2025-02-28 12:19:29,605:INFO:Starting cross validation
2025-02-28 12:19:29,607:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:30,343:INFO:Calculating mean and std
2025-02-28 12:19:30,344:INFO:Creating metrics dataframe
2025-02-28 12:19:30,345:INFO:Uploading results into container
2025-02-28 12:19:30,345:INFO:Uploading model into container now
2025-02-28 12:19:30,346:INFO:_master_model_container: 13
2025-02-28 12:19:30,346:INFO:_display_container: 2
2025-02-28 12:19:30,346:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 12:19:30,346:INFO:create_model() successfully completed......................................
2025-02-28 12:19:30,469:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:30,469:INFO:Creating metrics dataframe
2025-02-28 12:19:30,474:INFO:Initializing Extra Trees Regressor
2025-02-28 12:19:30,474:INFO:Total runtime is 0.1045389453570048 minutes
2025-02-28 12:19:30,476:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:30,476:INFO:Initializing create_model()
2025-02-28 12:19:30,477:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:30,477:INFO:Checking exceptions
2025-02-28 12:19:30,477:INFO:Importing libraries
2025-02-28 12:19:30,477:INFO:Copying training dataset
2025-02-28 12:19:30,480:INFO:Defining folds
2025-02-28 12:19:30,480:INFO:Declaring metric variables
2025-02-28 12:19:30,482:INFO:Importing untrained model
2025-02-28 12:19:30,484:INFO:Extra Trees Regressor Imported successfully
2025-02-28 12:19:30,489:INFO:Starting cross validation
2025-02-28 12:19:30,490:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:31,007:INFO:Calculating mean and std
2025-02-28 12:19:31,007:INFO:Creating metrics dataframe
2025-02-28 12:19:31,009:INFO:Uploading results into container
2025-02-28 12:19:31,009:INFO:Uploading model into container now
2025-02-28 12:19:31,009:INFO:_master_model_container: 14
2025-02-28 12:19:31,009:INFO:_display_container: 2
2025-02-28 12:19:31,010:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 12:19:31,010:INFO:create_model() successfully completed......................................
2025-02-28 12:19:31,141:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:31,141:INFO:Creating metrics dataframe
2025-02-28 12:19:31,147:INFO:Initializing AdaBoost Regressor
2025-02-28 12:19:31,147:INFO:Total runtime is 0.11575092077255249 minutes
2025-02-28 12:19:31,149:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:31,149:INFO:Initializing create_model()
2025-02-28 12:19:31,149:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:31,149:INFO:Checking exceptions
2025-02-28 12:19:31,149:INFO:Importing libraries
2025-02-28 12:19:31,149:INFO:Copying training dataset
2025-02-28 12:19:31,152:INFO:Defining folds
2025-02-28 12:19:31,153:INFO:Declaring metric variables
2025-02-28 12:19:31,155:INFO:Importing untrained model
2025-02-28 12:19:31,157:INFO:AdaBoost Regressor Imported successfully
2025-02-28 12:19:31,162:INFO:Starting cross validation
2025-02-28 12:19:31,164:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:31,555:INFO:Calculating mean and std
2025-02-28 12:19:31,556:INFO:Creating metrics dataframe
2025-02-28 12:19:31,557:INFO:Uploading results into container
2025-02-28 12:19:31,557:INFO:Uploading model into container now
2025-02-28 12:19:31,557:INFO:_master_model_container: 15
2025-02-28 12:19:31,558:INFO:_display_container: 2
2025-02-28 12:19:31,558:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 12:19:31,558:INFO:create_model() successfully completed......................................
2025-02-28 12:19:31,687:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:31,687:INFO:Creating metrics dataframe
2025-02-28 12:19:31,693:INFO:Initializing Gradient Boosting Regressor
2025-02-28 12:19:31,693:INFO:Total runtime is 0.12485628922780355 minutes
2025-02-28 12:19:31,695:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:31,695:INFO:Initializing create_model()
2025-02-28 12:19:31,695:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:31,695:INFO:Checking exceptions
2025-02-28 12:19:31,695:INFO:Importing libraries
2025-02-28 12:19:31,695:INFO:Copying training dataset
2025-02-28 12:19:31,699:INFO:Defining folds
2025-02-28 12:19:31,699:INFO:Declaring metric variables
2025-02-28 12:19:31,702:INFO:Importing untrained model
2025-02-28 12:19:31,705:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 12:19:31,712:INFO:Starting cross validation
2025-02-28 12:19:31,713:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:32,319:INFO:Calculating mean and std
2025-02-28 12:19:32,319:INFO:Creating metrics dataframe
2025-02-28 12:19:32,321:INFO:Uploading results into container
2025-02-28 12:19:32,321:INFO:Uploading model into container now
2025-02-28 12:19:32,321:INFO:_master_model_container: 16
2025-02-28 12:19:32,321:INFO:_display_container: 2
2025-02-28 12:19:32,321:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:19:32,321:INFO:create_model() successfully completed......................................
2025-02-28 12:19:32,447:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:32,447:INFO:Creating metrics dataframe
2025-02-28 12:19:32,453:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:19:32,453:INFO:Total runtime is 0.13752080202102662 minutes
2025-02-28 12:19:32,455:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:32,455:INFO:Initializing create_model()
2025-02-28 12:19:32,455:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:32,455:INFO:Checking exceptions
2025-02-28 12:19:32,455:INFO:Importing libraries
2025-02-28 12:19:32,455:INFO:Copying training dataset
2025-02-28 12:19:32,459:INFO:Defining folds
2025-02-28 12:19:32,459:INFO:Declaring metric variables
2025-02-28 12:19:32,461:INFO:Importing untrained model
2025-02-28 12:19:32,464:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:19:32,469:INFO:Starting cross validation
2025-02-28 12:19:32,470:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:33,052:INFO:Calculating mean and std
2025-02-28 12:19:33,052:INFO:Creating metrics dataframe
2025-02-28 12:19:33,054:INFO:Uploading results into container
2025-02-28 12:19:33,054:INFO:Uploading model into container now
2025-02-28 12:19:33,054:INFO:_master_model_container: 17
2025-02-28 12:19:33,054:INFO:_display_container: 2
2025-02-28 12:19:33,055:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:19:33,055:INFO:create_model() successfully completed......................................
2025-02-28 12:19:33,185:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:33,185:INFO:Creating metrics dataframe
2025-02-28 12:19:33,191:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:19:33,191:INFO:Total runtime is 0.14982364575068158 minutes
2025-02-28 12:19:33,193:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:33,193:INFO:Initializing create_model()
2025-02-28 12:19:33,193:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:33,194:INFO:Checking exceptions
2025-02-28 12:19:33,194:INFO:Importing libraries
2025-02-28 12:19:33,194:INFO:Copying training dataset
2025-02-28 12:19:33,197:INFO:Defining folds
2025-02-28 12:19:33,197:INFO:Declaring metric variables
2025-02-28 12:19:33,199:INFO:Importing untrained model
2025-02-28 12:19:33,202:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:19:33,207:INFO:Starting cross validation
2025-02-28 12:19:33,208:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:34,647:INFO:Calculating mean and std
2025-02-28 12:19:34,648:INFO:Creating metrics dataframe
2025-02-28 12:19:34,649:INFO:Uploading results into container
2025-02-28 12:19:34,650:INFO:Uploading model into container now
2025-02-28 12:19:34,650:INFO:_master_model_container: 18
2025-02-28 12:19:34,650:INFO:_display_container: 2
2025-02-28 12:19:34,651:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:19:34,651:INFO:create_model() successfully completed......................................
2025-02-28 12:19:34,792:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:34,792:INFO:Creating metrics dataframe
2025-02-28 12:19:34,798:INFO:Initializing CatBoost Regressor
2025-02-28 12:19:34,798:INFO:Total runtime is 0.1765997131665548 minutes
2025-02-28 12:19:34,800:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:34,800:INFO:Initializing create_model()
2025-02-28 12:19:34,800:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:34,801:INFO:Checking exceptions
2025-02-28 12:19:34,801:INFO:Importing libraries
2025-02-28 12:19:34,801:INFO:Copying training dataset
2025-02-28 12:19:34,803:INFO:Defining folds
2025-02-28 12:19:34,803:INFO:Declaring metric variables
2025-02-28 12:19:34,806:INFO:Importing untrained model
2025-02-28 12:19:34,808:INFO:CatBoost Regressor Imported successfully
2025-02-28 12:19:34,811:INFO:Starting cross validation
2025-02-28 12:19:34,813:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:37,832:INFO:Calculating mean and std
2025-02-28 12:19:37,833:INFO:Creating metrics dataframe
2025-02-28 12:19:37,834:INFO:Uploading results into container
2025-02-28 12:19:37,834:INFO:Uploading model into container now
2025-02-28 12:19:37,834:INFO:_master_model_container: 19
2025-02-28 12:19:37,834:INFO:_display_container: 2
2025-02-28 12:19:37,835:INFO:<catboost.core.CatBoostRegressor object at 0x0000024E5BBD15A0>
2025-02-28 12:19:37,835:INFO:create_model() successfully completed......................................
2025-02-28 12:19:37,958:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:37,958:INFO:Creating metrics dataframe
2025-02-28 12:19:37,963:INFO:Initializing Dummy Regressor
2025-02-28 12:19:37,963:INFO:Total runtime is 0.2293560028076172 minutes
2025-02-28 12:19:37,966:INFO:SubProcess create_model() called ==================================
2025-02-28 12:19:37,966:INFO:Initializing create_model()
2025-02-28 12:19:37,966:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E80F2A680>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:37,966:INFO:Checking exceptions
2025-02-28 12:19:37,966:INFO:Importing libraries
2025-02-28 12:19:37,966:INFO:Copying training dataset
2025-02-28 12:19:37,970:INFO:Defining folds
2025-02-28 12:19:37,970:INFO:Declaring metric variables
2025-02-28 12:19:37,973:INFO:Importing untrained model
2025-02-28 12:19:37,975:INFO:Dummy Regressor Imported successfully
2025-02-28 12:19:37,979:INFO:Starting cross validation
2025-02-28 12:19:37,980:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:19:38,114:INFO:Calculating mean and std
2025-02-28 12:19:38,115:INFO:Creating metrics dataframe
2025-02-28 12:19:38,116:INFO:Uploading results into container
2025-02-28 12:19:38,116:INFO:Uploading model into container now
2025-02-28 12:19:38,116:INFO:_master_model_container: 20
2025-02-28 12:19:38,116:INFO:_display_container: 2
2025-02-28 12:19:38,116:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:19:38,117:INFO:create_model() successfully completed......................................
2025-02-28 12:19:38,243:INFO:SubProcess create_model() end ==================================
2025-02-28 12:19:38,243:INFO:Creating metrics dataframe
2025-02-28 12:19:38,249:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:19:38,256:INFO:Initializing create_model()
2025-02-28 12:19:38,256:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:19:38,256:INFO:Checking exceptions
2025-02-28 12:19:38,257:INFO:Importing libraries
2025-02-28 12:19:38,257:INFO:Copying training dataset
2025-02-28 12:19:38,260:INFO:Defining folds
2025-02-28 12:19:38,260:INFO:Declaring metric variables
2025-02-28 12:19:38,260:INFO:Importing untrained model
2025-02-28 12:19:38,260:INFO:Declaring custom model
2025-02-28 12:19:38,260:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:19:38,261:INFO:Cross validation set to False
2025-02-28 12:19:38,261:INFO:Fitting Model
2025-02-28 12:19:38,500:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:19:38,501:INFO:create_model() successfully completed......................................
2025-02-28 12:19:38,656:INFO:_master_model_container: 20
2025-02-28 12:19:38,656:INFO:_display_container: 2
2025-02-28 12:19:38,656:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:19:38,656:INFO:compare_models() successfully completed......................................
2025-02-28 12:20:33,517:INFO:Initializing create_model()
2025-02-28 12:20:33,517:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:20:33,517:INFO:Checking exceptions
2025-02-28 12:20:33,525:INFO:Importing libraries
2025-02-28 12:20:33,525:INFO:Copying training dataset
2025-02-28 12:20:33,528:INFO:Defining folds
2025-02-28 12:20:33,528:INFO:Declaring metric variables
2025-02-28 12:20:33,530:INFO:Importing untrained model
2025-02-28 12:20:33,533:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:20:33,539:INFO:Starting cross validation
2025-02-28 12:20:33,540:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:20:34,172:INFO:Calculating mean and std
2025-02-28 12:20:34,172:INFO:Creating metrics dataframe
2025-02-28 12:20:34,175:INFO:Finalizing model
2025-02-28 12:20:34,286:INFO:Uploading results into container
2025-02-28 12:20:34,287:INFO:Uploading model into container now
2025-02-28 12:20:34,293:INFO:_master_model_container: 21
2025-02-28 12:20:34,293:INFO:_display_container: 3
2025-02-28 12:20:34,293:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:20:34,294:INFO:create_model() successfully completed......................................
2025-02-28 12:20:53,870:INFO:Initializing plot_model()
2025-02-28 12:20:53,870:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, system=True)
2025-02-28 12:20:53,870:INFO:Checking exceptions
2025-02-28 12:20:53,873:INFO:Preloading libraries
2025-02-28 12:20:53,878:INFO:Copying training dataset
2025-02-28 12:20:53,878:INFO:Plot type: residuals
2025-02-28 12:20:54,129:INFO:Fitting Model
2025-02-28 12:20:54,160:INFO:Scoring test/hold-out set
2025-02-28 12:20:54,434:INFO:Visual Rendered Successfully
2025-02-28 12:20:54,575:INFO:plot_model() successfully completed......................................
2025-02-28 12:21:02,113:INFO:Initializing plot_model()
2025-02-28 12:21:02,113:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, system=True)
2025-02-28 12:21:02,113:INFO:Checking exceptions
2025-02-28 12:21:02,116:INFO:Preloading libraries
2025-02-28 12:21:02,121:INFO:Copying training dataset
2025-02-28 12:21:02,121:INFO:Plot type: error
2025-02-28 12:21:02,342:INFO:Fitting Model
2025-02-28 12:21:02,342:INFO:Scoring test/hold-out set
2025-02-28 12:21:02,486:INFO:Visual Rendered Successfully
2025-02-28 12:21:02,617:INFO:plot_model() successfully completed......................................
2025-02-28 12:21:10,033:INFO:Initializing plot_model()
2025-02-28 12:21:10,033:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E80F2A3E0>, system=True)
2025-02-28 12:21:10,033:INFO:Checking exceptions
2025-02-28 12:21:10,036:INFO:Preloading libraries
2025-02-28 12:21:10,041:INFO:Copying training dataset
2025-02-28 12:21:10,041:INFO:Plot type: feature
2025-02-28 12:21:10,041:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 12:21:10,189:INFO:Visual Rendered Successfully
2025-02-28 12:21:10,322:INFO:plot_model() successfully completed......................................
2025-02-28 12:23:59,991:INFO:PyCaret ClassificationExperiment
2025-02-28 12:23:59,991:INFO:Logging name: clf-default-name
2025-02-28 12:23:59,991:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 12:23:59,991:INFO:version 3.3.2
2025-02-28 12:23:59,991:INFO:Initializing setup()
2025-02-28 12:23:59,991:INFO:self.USI: c87e
2025-02-28 12:23:59,991:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fix_imbalance', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'is_multiclass', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:23:59,991:INFO:Checking environment
2025-02-28 12:23:59,991:INFO:python_version: 3.10.16
2025-02-28 12:23:59,991:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:23:59,991:INFO:machine: AMD64
2025-02-28 12:23:59,991:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:23:59,991:INFO:Memory: svmem(total=34200334336, available=8954920960, percent=73.8, used=25245413376, free=8954920960)
2025-02-28 12:23:59,991:INFO:Physical Core: 24
2025-02-28 12:23:59,992:INFO:Logical Core: 32
2025-02-28 12:23:59,992:INFO:Checking libraries
2025-02-28 12:23:59,992:INFO:System:
2025-02-28 12:23:59,992:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:23:59,992:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:23:59,992:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:23:59,992:INFO:PyCaret required dependencies:
2025-02-28 12:23:59,992:INFO:                 pip: 25.0
2025-02-28 12:23:59,992:INFO:          setuptools: 75.8.0
2025-02-28 12:23:59,992:INFO:             pycaret: 3.3.2
2025-02-28 12:23:59,992:INFO:             IPython: 8.30.0
2025-02-28 12:23:59,992:INFO:          ipywidgets: 8.1.5
2025-02-28 12:23:59,992:INFO:                tqdm: 4.67.1
2025-02-28 12:23:59,992:INFO:               numpy: 1.26.4
2025-02-28 12:23:59,992:INFO:              pandas: 2.1.4
2025-02-28 12:23:59,992:INFO:              jinja2: 3.1.5
2025-02-28 12:23:59,992:INFO:               scipy: 1.11.4
2025-02-28 12:23:59,992:INFO:              joblib: 1.3.2
2025-02-28 12:23:59,992:INFO:             sklearn: 1.4.2
2025-02-28 12:23:59,992:INFO:                pyod: 2.0.3
2025-02-28 12:23:59,992:INFO:            imblearn: 0.13.0
2025-02-28 12:23:59,992:INFO:   category_encoders: 2.7.0
2025-02-28 12:23:59,992:INFO:            lightgbm: 4.5.0
2025-02-28 12:23:59,992:INFO:               numba: 0.61.0
2025-02-28 12:23:59,992:INFO:            requests: 2.32.3
2025-02-28 12:23:59,992:INFO:          matplotlib: 3.7.5
2025-02-28 12:23:59,992:INFO:          scikitplot: 0.3.7
2025-02-28 12:23:59,992:INFO:         yellowbrick: 1.5
2025-02-28 12:23:59,993:INFO:              plotly: 5.24.1
2025-02-28 12:23:59,993:INFO:    plotly-resampler: Not installed
2025-02-28 12:23:59,993:INFO:             kaleido: 0.2.1
2025-02-28 12:23:59,993:INFO:           schemdraw: 0.15
2025-02-28 12:23:59,993:INFO:         statsmodels: 0.14.4
2025-02-28 12:23:59,993:INFO:              sktime: 0.26.0
2025-02-28 12:23:59,993:INFO:               tbats: 1.1.3
2025-02-28 12:23:59,993:INFO:            pmdarima: 2.0.4
2025-02-28 12:23:59,993:INFO:              psutil: 5.9.0
2025-02-28 12:23:59,993:INFO:          markupsafe: 2.1.5
2025-02-28 12:23:59,993:INFO:             pickle5: Not installed
2025-02-28 12:23:59,993:INFO:         cloudpickle: 3.1.1
2025-02-28 12:23:59,993:INFO:         deprecation: 2.1.0
2025-02-28 12:23:59,993:INFO:              xxhash: 3.5.0
2025-02-28 12:23:59,993:INFO:           wurlitzer: Not installed
2025-02-28 12:23:59,993:INFO:PyCaret optional dependencies:
2025-02-28 12:23:59,993:INFO:                shap: 0.44.1
2025-02-28 12:23:59,993:INFO:           interpret: 0.6.9
2025-02-28 12:23:59,993:INFO:                umap: 0.5.7
2025-02-28 12:23:59,993:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:23:59,993:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:23:59,993:INFO:             autoviz: Not installed
2025-02-28 12:23:59,993:INFO:           fairlearn: 0.7.0
2025-02-28 12:23:59,994:INFO:          deepchecks: Not installed
2025-02-28 12:23:59,994:INFO:             xgboost: 2.1.4
2025-02-28 12:23:59,994:INFO:            catboost: 1.2.7
2025-02-28 12:23:59,994:INFO:              kmodes: 0.12.2
2025-02-28 12:23:59,994:INFO:             mlxtend: 0.23.4
2025-02-28 12:23:59,994:INFO:       statsforecast: 1.5.0
2025-02-28 12:23:59,994:INFO:        tune_sklearn: Not installed
2025-02-28 12:23:59,994:INFO:                 ray: Not installed
2025-02-28 12:23:59,994:INFO:            hyperopt: 0.2.7
2025-02-28 12:23:59,994:INFO:              optuna: 4.2.0
2025-02-28 12:23:59,994:INFO:               skopt: 0.10.2
2025-02-28 12:23:59,994:INFO:              mlflow: 2.20.1
2025-02-28 12:23:59,994:INFO:              gradio: 5.15.0
2025-02-28 12:23:59,994:INFO:             fastapi: 0.115.8
2025-02-28 12:23:59,994:INFO:             uvicorn: 0.34.0
2025-02-28 12:23:59,994:INFO:              m2cgen: 0.10.0
2025-02-28 12:23:59,994:INFO:           evidently: 0.4.40
2025-02-28 12:23:59,994:INFO:               fugue: 0.8.7
2025-02-28 12:23:59,994:INFO:           streamlit: Not installed
2025-02-28 12:23:59,994:INFO:             prophet: Not installed
2025-02-28 12:23:59,994:INFO:None
2025-02-28 12:23:59,994:INFO:Set up data.
2025-02-28 12:24:00,000:INFO:Set up folding strategy.
2025-02-28 12:24:00,000:INFO:Set up train/test split.
2025-02-28 12:24:00,003:INFO:Set up index.
2025-02-28 12:24:00,003:INFO:Assigning column types.
2025-02-28 12:24:00,006:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:24:00,026:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,027:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,039:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,040:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,060:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,060:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,072:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,072:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,074:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:24:00,094:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,106:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,107:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,127:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:00,138:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,140:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,140:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 12:24:00,172:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,173:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,208:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,209:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,210:INFO:Preparing preprocessing pipeline...
2025-02-28 12:24:00,211:INFO:Set up simple imputation.
2025-02-28 12:24:00,212:INFO:Set up encoding of ordinal features.
2025-02-28 12:24:00,215:INFO:Set up encoding of categorical features.
2025-02-28 12:24:00,261:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:24:00,268:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score',
                                             'Starting_Salary',
                                             'Career_Sa...
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study',
                                             'Current_Job_Level'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study',
                                                                    'Current_Job_Level'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:24:00,268:INFO:Creating final display dataframe.
2025-02-28 12:24:00,413:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 19)
4        Transformed data shape        (5000, 30)
5   Transformed train set shape        (3500, 30)
6    Transformed test set shape        (1500, 30)
7              Numeric features                14
8          Categorical features                 4
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              c87e
2025-02-28 12:24:00,451:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,452:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,484:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:00,485:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:00,486:INFO:setup() successfully completed in 0.5s...............
2025-02-28 12:24:00,488:INFO:PyCaret RegressionExperiment
2025-02-28 12:24:00,488:INFO:Logging name: reg-default-name
2025-02-28 12:24:00,488:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 12:24:00,488:INFO:version 3.3.2
2025-02-28 12:24:00,488:INFO:Initializing setup()
2025-02-28 12:24:00,488:INFO:self.USI: b86f
2025-02-28 12:24:00,488:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'transform_target_param', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:24:00,488:INFO:Checking environment
2025-02-28 12:24:00,488:INFO:python_version: 3.10.16
2025-02-28 12:24:00,488:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:24:00,488:INFO:machine: AMD64
2025-02-28 12:24:00,488:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:24:00,488:INFO:Memory: svmem(total=34200334336, available=8937410560, percent=73.9, used=25262923776, free=8937410560)
2025-02-28 12:24:00,488:INFO:Physical Core: 24
2025-02-28 12:24:00,488:INFO:Logical Core: 32
2025-02-28 12:24:00,488:INFO:Checking libraries
2025-02-28 12:24:00,489:INFO:System:
2025-02-28 12:24:00,489:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:24:00,489:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:24:00,489:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:24:00,489:INFO:PyCaret required dependencies:
2025-02-28 12:24:00,489:INFO:                 pip: 25.0
2025-02-28 12:24:00,489:INFO:          setuptools: 75.8.0
2025-02-28 12:24:00,489:INFO:             pycaret: 3.3.2
2025-02-28 12:24:00,489:INFO:             IPython: 8.30.0
2025-02-28 12:24:00,489:INFO:          ipywidgets: 8.1.5
2025-02-28 12:24:00,489:INFO:                tqdm: 4.67.1
2025-02-28 12:24:00,489:INFO:               numpy: 1.26.4
2025-02-28 12:24:00,489:INFO:              pandas: 2.1.4
2025-02-28 12:24:00,489:INFO:              jinja2: 3.1.5
2025-02-28 12:24:00,489:INFO:               scipy: 1.11.4
2025-02-28 12:24:00,489:INFO:              joblib: 1.3.2
2025-02-28 12:24:00,489:INFO:             sklearn: 1.4.2
2025-02-28 12:24:00,489:INFO:                pyod: 2.0.3
2025-02-28 12:24:00,489:INFO:            imblearn: 0.13.0
2025-02-28 12:24:00,489:INFO:   category_encoders: 2.7.0
2025-02-28 12:24:00,489:INFO:            lightgbm: 4.5.0
2025-02-28 12:24:00,489:INFO:               numba: 0.61.0
2025-02-28 12:24:00,489:INFO:            requests: 2.32.3
2025-02-28 12:24:00,489:INFO:          matplotlib: 3.7.5
2025-02-28 12:24:00,489:INFO:          scikitplot: 0.3.7
2025-02-28 12:24:00,490:INFO:         yellowbrick: 1.5
2025-02-28 12:24:00,490:INFO:              plotly: 5.24.1
2025-02-28 12:24:00,490:INFO:    plotly-resampler: Not installed
2025-02-28 12:24:00,490:INFO:             kaleido: 0.2.1
2025-02-28 12:24:00,490:INFO:           schemdraw: 0.15
2025-02-28 12:24:00,490:INFO:         statsmodels: 0.14.4
2025-02-28 12:24:00,490:INFO:              sktime: 0.26.0
2025-02-28 12:24:00,490:INFO:               tbats: 1.1.3
2025-02-28 12:24:00,490:INFO:            pmdarima: 2.0.4
2025-02-28 12:24:00,490:INFO:              psutil: 5.9.0
2025-02-28 12:24:00,490:INFO:          markupsafe: 2.1.5
2025-02-28 12:24:00,490:INFO:             pickle5: Not installed
2025-02-28 12:24:00,490:INFO:         cloudpickle: 3.1.1
2025-02-28 12:24:00,490:INFO:         deprecation: 2.1.0
2025-02-28 12:24:00,490:INFO:              xxhash: 3.5.0
2025-02-28 12:24:00,490:INFO:           wurlitzer: Not installed
2025-02-28 12:24:00,490:INFO:PyCaret optional dependencies:
2025-02-28 12:24:00,490:INFO:                shap: 0.44.1
2025-02-28 12:24:00,490:INFO:           interpret: 0.6.9
2025-02-28 12:24:00,490:INFO:                umap: 0.5.7
2025-02-28 12:24:00,490:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:24:00,490:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:24:00,490:INFO:             autoviz: Not installed
2025-02-28 12:24:00,490:INFO:           fairlearn: 0.7.0
2025-02-28 12:24:00,490:INFO:          deepchecks: Not installed
2025-02-28 12:24:00,491:INFO:             xgboost: 2.1.4
2025-02-28 12:24:00,491:INFO:            catboost: 1.2.7
2025-02-28 12:24:00,491:INFO:              kmodes: 0.12.2
2025-02-28 12:24:00,491:INFO:             mlxtend: 0.23.4
2025-02-28 12:24:00,491:INFO:       statsforecast: 1.5.0
2025-02-28 12:24:00,491:INFO:        tune_sklearn: Not installed
2025-02-28 12:24:00,491:INFO:                 ray: Not installed
2025-02-28 12:24:00,491:INFO:            hyperopt: 0.2.7
2025-02-28 12:24:00,491:INFO:              optuna: 4.2.0
2025-02-28 12:24:00,491:INFO:               skopt: 0.10.2
2025-02-28 12:24:00,491:INFO:              mlflow: 2.20.1
2025-02-28 12:24:00,491:INFO:              gradio: 5.15.0
2025-02-28 12:24:00,491:INFO:             fastapi: 0.115.8
2025-02-28 12:24:00,491:INFO:             uvicorn: 0.34.0
2025-02-28 12:24:00,491:INFO:              m2cgen: 0.10.0
2025-02-28 12:24:00,491:INFO:           evidently: 0.4.40
2025-02-28 12:24:00,491:INFO:               fugue: 0.8.7
2025-02-28 12:24:00,491:INFO:           streamlit: Not installed
2025-02-28 12:24:00,491:INFO:             prophet: Not installed
2025-02-28 12:24:00,491:INFO:None
2025-02-28 12:24:00,491:INFO:Set up data.
2025-02-28 12:24:26,378:INFO:PyCaret ClassificationExperiment
2025-02-28 12:24:26,378:INFO:Logging name: clf-default-name
2025-02-28 12:24:26,378:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 12:24:26,378:INFO:version 3.3.2
2025-02-28 12:24:26,378:INFO:Initializing setup()
2025-02-28 12:24:26,378:INFO:self.USI: cd25
2025-02-28 12:24:26,378:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fix_imbalance', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'is_multiclass', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:24:26,378:INFO:Checking environment
2025-02-28 12:24:26,378:INFO:python_version: 3.10.16
2025-02-28 12:24:26,378:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:24:26,378:INFO:machine: AMD64
2025-02-28 12:24:26,379:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:24:26,379:INFO:Memory: svmem(total=34200334336, available=9020604416, percent=73.6, used=25179729920, free=9020604416)
2025-02-28 12:24:26,379:INFO:Physical Core: 24
2025-02-28 12:24:26,379:INFO:Logical Core: 32
2025-02-28 12:24:26,379:INFO:Checking libraries
2025-02-28 12:24:26,379:INFO:System:
2025-02-28 12:24:26,379:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:24:26,379:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:24:26,379:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:24:26,379:INFO:PyCaret required dependencies:
2025-02-28 12:24:26,379:INFO:                 pip: 25.0
2025-02-28 12:24:26,379:INFO:          setuptools: 75.8.0
2025-02-28 12:24:26,379:INFO:             pycaret: 3.3.2
2025-02-28 12:24:26,380:INFO:             IPython: 8.30.0
2025-02-28 12:24:26,380:INFO:          ipywidgets: 8.1.5
2025-02-28 12:24:26,380:INFO:                tqdm: 4.67.1
2025-02-28 12:24:26,380:INFO:               numpy: 1.26.4
2025-02-28 12:24:26,380:INFO:              pandas: 2.1.4
2025-02-28 12:24:26,380:INFO:              jinja2: 3.1.5
2025-02-28 12:24:26,380:INFO:               scipy: 1.11.4
2025-02-28 12:24:26,380:INFO:              joblib: 1.3.2
2025-02-28 12:24:26,381:INFO:             sklearn: 1.4.2
2025-02-28 12:24:26,382:INFO:                pyod: 2.0.3
2025-02-28 12:24:26,382:INFO:            imblearn: 0.13.0
2025-02-28 12:24:26,382:INFO:   category_encoders: 2.7.0
2025-02-28 12:24:26,382:INFO:            lightgbm: 4.5.0
2025-02-28 12:24:26,382:INFO:               numba: 0.61.0
2025-02-28 12:24:26,382:INFO:            requests: 2.32.3
2025-02-28 12:24:26,382:INFO:          matplotlib: 3.7.5
2025-02-28 12:24:26,382:INFO:          scikitplot: 0.3.7
2025-02-28 12:24:26,382:INFO:         yellowbrick: 1.5
2025-02-28 12:24:26,382:INFO:              plotly: 5.24.1
2025-02-28 12:24:26,382:INFO:    plotly-resampler: Not installed
2025-02-28 12:24:26,382:INFO:             kaleido: 0.2.1
2025-02-28 12:24:26,382:INFO:           schemdraw: 0.15
2025-02-28 12:24:26,382:INFO:         statsmodels: 0.14.4
2025-02-28 12:24:26,382:INFO:              sktime: 0.26.0
2025-02-28 12:24:26,382:INFO:               tbats: 1.1.3
2025-02-28 12:24:26,382:INFO:            pmdarima: 2.0.4
2025-02-28 12:24:26,382:INFO:              psutil: 5.9.0
2025-02-28 12:24:26,382:INFO:          markupsafe: 2.1.5
2025-02-28 12:24:26,383:INFO:             pickle5: Not installed
2025-02-28 12:24:26,383:INFO:         cloudpickle: 3.1.1
2025-02-28 12:24:26,383:INFO:         deprecation: 2.1.0
2025-02-28 12:24:26,383:INFO:              xxhash: 3.5.0
2025-02-28 12:24:26,383:INFO:           wurlitzer: Not installed
2025-02-28 12:24:26,383:INFO:PyCaret optional dependencies:
2025-02-28 12:24:26,383:INFO:                shap: 0.44.1
2025-02-28 12:24:26,383:INFO:           interpret: 0.6.9
2025-02-28 12:24:26,383:INFO:                umap: 0.5.7
2025-02-28 12:24:26,383:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:24:26,383:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:24:26,383:INFO:             autoviz: Not installed
2025-02-28 12:24:26,383:INFO:           fairlearn: 0.7.0
2025-02-28 12:24:26,383:INFO:          deepchecks: Not installed
2025-02-28 12:24:26,383:INFO:             xgboost: 2.1.4
2025-02-28 12:24:26,383:INFO:            catboost: 1.2.7
2025-02-28 12:24:26,383:INFO:              kmodes: 0.12.2
2025-02-28 12:24:26,383:INFO:             mlxtend: 0.23.4
2025-02-28 12:24:26,383:INFO:       statsforecast: 1.5.0
2025-02-28 12:24:26,383:INFO:        tune_sklearn: Not installed
2025-02-28 12:24:26,383:INFO:                 ray: Not installed
2025-02-28 12:24:26,383:INFO:            hyperopt: 0.2.7
2025-02-28 12:24:26,383:INFO:              optuna: 4.2.0
2025-02-28 12:24:26,383:INFO:               skopt: 0.10.2
2025-02-28 12:24:26,383:INFO:              mlflow: 2.20.1
2025-02-28 12:24:26,383:INFO:              gradio: 5.15.0
2025-02-28 12:24:26,384:INFO:             fastapi: 0.115.8
2025-02-28 12:24:26,384:INFO:             uvicorn: 0.34.0
2025-02-28 12:24:26,384:INFO:              m2cgen: 0.10.0
2025-02-28 12:24:26,384:INFO:           evidently: 0.4.40
2025-02-28 12:24:26,384:INFO:               fugue: 0.8.7
2025-02-28 12:24:26,384:INFO:           streamlit: Not installed
2025-02-28 12:24:26,384:INFO:             prophet: Not installed
2025-02-28 12:24:26,384:INFO:None
2025-02-28 12:24:26,384:INFO:Set up data.
2025-02-28 12:24:26,389:INFO:Set up folding strategy.
2025-02-28 12:24:26,389:INFO:Set up train/test split.
2025-02-28 12:24:26,393:INFO:Set up index.
2025-02-28 12:24:26,394:INFO:Assigning column types.
2025-02-28 12:24:26,397:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:24:26,416:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,416:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,429:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,430:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,451:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,451:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,464:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,465:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,465:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:24:26,485:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,496:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,498:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,519:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:24:26,531:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,532:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,533:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 12:24:26,564:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,565:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,597:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,598:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,599:INFO:Preparing preprocessing pipeline...
2025-02-28 12:24:26,600:INFO:Set up simple imputation.
2025-02-28 12:24:26,601:INFO:Set up encoding of ordinal features.
2025-02-28 12:24:26,603:INFO:Set up encoding of categorical features.
2025-02-28 12:24:26,644:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:24:26,652:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score',
                                             'Starting_Salary',
                                             'Career_Sa...
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study',
                                             'Current_Job_Level'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study',
                                                                    'Current_Job_Level'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:24:26,652:INFO:Creating final display dataframe.
2025-02-28 12:24:26,791:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 19)
4        Transformed data shape        (5000, 30)
5   Transformed train set shape        (3500, 30)
6    Transformed test set shape        (1500, 30)
7              Numeric features                14
8          Categorical features                 4
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              cd25
2025-02-28 12:24:26,827:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,829:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,860:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:24:26,861:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:24:26,862:INFO:setup() successfully completed in 0.49s...............
2025-02-28 12:24:26,888:INFO:Initializing compare_models()
2025-02-28 12:24:26,888:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 12:24:26,888:INFO:Checking exceptions
2025-02-28 12:24:26,891:INFO:Preparing display monitor
2025-02-28 12:24:26,904:INFO:Initializing Logistic Regression
2025-02-28 12:24:26,904:INFO:Total runtime is 0.0 minutes
2025-02-28 12:24:26,905:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:26,906:INFO:Initializing create_model()
2025-02-28 12:24:26,906:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:26,906:INFO:Checking exceptions
2025-02-28 12:24:26,906:INFO:Importing libraries
2025-02-28 12:24:26,906:INFO:Copying training dataset
2025-02-28 12:24:26,909:INFO:Defining folds
2025-02-28 12:24:26,909:INFO:Declaring metric variables
2025-02-28 12:24:26,911:INFO:Importing untrained model
2025-02-28 12:24:26,913:INFO:Logistic Regression Imported successfully
2025-02-28 12:24:26,917:INFO:Starting cross validation
2025-02-28 12:24:26,918:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:27,419:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,431:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,435:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,449:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,452:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,456:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,462:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,469:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,470:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,473:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,476:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,476:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,476:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,477:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:24:27,480:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,485:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,492:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,492:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,493:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,493:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:27,503:INFO:Calculating mean and std
2025-02-28 12:24:27,503:INFO:Creating metrics dataframe
2025-02-28 12:24:27,504:INFO:Uploading results into container
2025-02-28 12:24:27,504:INFO:Uploading model into container now
2025-02-28 12:24:27,504:INFO:_master_model_container: 1
2025-02-28 12:24:27,504:INFO:_display_container: 2
2025-02-28 12:24:27,505:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 12:24:27,505:INFO:create_model() successfully completed......................................
2025-02-28 12:24:27,655:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:27,655:INFO:Creating metrics dataframe
2025-02-28 12:24:27,658:INFO:Initializing K Neighbors Classifier
2025-02-28 12:24:27,658:INFO:Total runtime is 0.012566765149434408 minutes
2025-02-28 12:24:27,660:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:27,661:INFO:Initializing create_model()
2025-02-28 12:24:27,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:27,661:INFO:Checking exceptions
2025-02-28 12:24:27,661:INFO:Importing libraries
2025-02-28 12:24:27,661:INFO:Copying training dataset
2025-02-28 12:24:27,664:INFO:Defining folds
2025-02-28 12:24:27,664:INFO:Declaring metric variables
2025-02-28 12:24:27,667:INFO:Importing untrained model
2025-02-28 12:24:27,669:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:24:27,672:INFO:Starting cross validation
2025-02-28 12:24:27,673:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:27,815:INFO:Calculating mean and std
2025-02-28 12:24:27,815:INFO:Creating metrics dataframe
2025-02-28 12:24:27,816:INFO:Uploading results into container
2025-02-28 12:24:27,817:INFO:Uploading model into container now
2025-02-28 12:24:27,817:INFO:_master_model_container: 2
2025-02-28 12:24:27,817:INFO:_display_container: 2
2025-02-28 12:24:27,817:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:24:27,817:INFO:create_model() successfully completed......................................
2025-02-28 12:24:27,955:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:27,955:INFO:Creating metrics dataframe
2025-02-28 12:24:27,959:INFO:Initializing Naive Bayes
2025-02-28 12:24:27,959:INFO:Total runtime is 0.017585055033365885 minutes
2025-02-28 12:24:27,962:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:27,962:INFO:Initializing create_model()
2025-02-28 12:24:27,962:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:27,962:INFO:Checking exceptions
2025-02-28 12:24:27,962:INFO:Importing libraries
2025-02-28 12:24:27,962:INFO:Copying training dataset
2025-02-28 12:24:27,966:INFO:Defining folds
2025-02-28 12:24:27,966:INFO:Declaring metric variables
2025-02-28 12:24:27,968:INFO:Importing untrained model
2025-02-28 12:24:27,970:INFO:Naive Bayes Imported successfully
2025-02-28 12:24:27,975:INFO:Starting cross validation
2025-02-28 12:24:27,976:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:28,079:INFO:Calculating mean and std
2025-02-28 12:24:28,080:INFO:Creating metrics dataframe
2025-02-28 12:24:28,081:INFO:Uploading results into container
2025-02-28 12:24:28,081:INFO:Uploading model into container now
2025-02-28 12:24:28,081:INFO:_master_model_container: 3
2025-02-28 12:24:28,081:INFO:_display_container: 2
2025-02-28 12:24:28,081:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 12:24:28,082:INFO:create_model() successfully completed......................................
2025-02-28 12:24:28,218:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:28,218:INFO:Creating metrics dataframe
2025-02-28 12:24:28,222:INFO:Initializing Decision Tree Classifier
2025-02-28 12:24:28,222:INFO:Total runtime is 0.021964287757873534 minutes
2025-02-28 12:24:28,225:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:28,225:INFO:Initializing create_model()
2025-02-28 12:24:28,225:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:28,225:INFO:Checking exceptions
2025-02-28 12:24:28,225:INFO:Importing libraries
2025-02-28 12:24:28,225:INFO:Copying training dataset
2025-02-28 12:24:28,228:INFO:Defining folds
2025-02-28 12:24:28,228:INFO:Declaring metric variables
2025-02-28 12:24:28,230:INFO:Importing untrained model
2025-02-28 12:24:28,233:INFO:Decision Tree Classifier Imported successfully
2025-02-28 12:24:28,238:INFO:Starting cross validation
2025-02-28 12:24:28,239:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:28,373:INFO:Calculating mean and std
2025-02-28 12:24:28,373:INFO:Creating metrics dataframe
2025-02-28 12:24:28,375:INFO:Uploading results into container
2025-02-28 12:24:28,375:INFO:Uploading model into container now
2025-02-28 12:24:28,375:INFO:_master_model_container: 4
2025-02-28 12:24:28,375:INFO:_display_container: 2
2025-02-28 12:24:28,376:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:24:28,376:INFO:create_model() successfully completed......................................
2025-02-28 12:24:28,511:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:28,511:INFO:Creating metrics dataframe
2025-02-28 12:24:28,516:INFO:Initializing SVM - Linear Kernel
2025-02-28 12:24:28,516:INFO:Total runtime is 0.026861933867136638 minutes
2025-02-28 12:24:28,519:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:28,519:INFO:Initializing create_model()
2025-02-28 12:24:28,519:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:28,519:INFO:Checking exceptions
2025-02-28 12:24:28,519:INFO:Importing libraries
2025-02-28 12:24:28,519:INFO:Copying training dataset
2025-02-28 12:24:28,522:INFO:Defining folds
2025-02-28 12:24:28,523:INFO:Declaring metric variables
2025-02-28 12:24:28,525:INFO:Importing untrained model
2025-02-28 12:24:28,527:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 12:24:28,533:INFO:Starting cross validation
2025-02-28 12:24:28,534:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:28,666:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,668:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,669:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,671:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,680:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,681:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,681:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,683:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,683:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,683:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,692:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,693:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,694:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,694:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,707:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,707:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,709:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,709:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,709:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:28,718:INFO:Calculating mean and std
2025-02-28 12:24:28,719:INFO:Creating metrics dataframe
2025-02-28 12:24:28,720:INFO:Uploading results into container
2025-02-28 12:24:28,720:INFO:Uploading model into container now
2025-02-28 12:24:28,720:INFO:_master_model_container: 5
2025-02-28 12:24:28,720:INFO:_display_container: 2
2025-02-28 12:24:28,721:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:24:28,721:INFO:create_model() successfully completed......................................
2025-02-28 12:24:28,857:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:28,857:INFO:Creating metrics dataframe
2025-02-28 12:24:28,861:INFO:Initializing Ridge Classifier
2025-02-28 12:24:28,861:INFO:Total runtime is 0.03261667092641195 minutes
2025-02-28 12:24:28,863:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:28,863:INFO:Initializing create_model()
2025-02-28 12:24:28,864:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:28,864:INFO:Checking exceptions
2025-02-28 12:24:28,864:INFO:Importing libraries
2025-02-28 12:24:28,864:INFO:Copying training dataset
2025-02-28 12:24:28,867:INFO:Defining folds
2025-02-28 12:24:28,868:INFO:Declaring metric variables
2025-02-28 12:24:28,870:INFO:Importing untrained model
2025-02-28 12:24:28,872:INFO:Ridge Classifier Imported successfully
2025-02-28 12:24:28,876:INFO:Starting cross validation
2025-02-28 12:24:28,877:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:28,953:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,953:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,953:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,955:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,966:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,968:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,971:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,972:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,973:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,977:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:28,997:INFO:Calculating mean and std
2025-02-28 12:24:28,998:INFO:Creating metrics dataframe
2025-02-28 12:24:28,999:INFO:Uploading results into container
2025-02-28 12:24:28,999:INFO:Uploading model into container now
2025-02-28 12:24:29,000:INFO:_master_model_container: 6
2025-02-28 12:24:29,000:INFO:_display_container: 2
2025-02-28 12:24:29,000:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 12:24:29,000:INFO:create_model() successfully completed......................................
2025-02-28 12:24:29,137:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:29,137:INFO:Creating metrics dataframe
2025-02-28 12:24:29,142:INFO:Initializing Random Forest Classifier
2025-02-28 12:24:29,143:INFO:Total runtime is 0.037310230731964114 minutes
2025-02-28 12:24:29,144:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:29,145:INFO:Initializing create_model()
2025-02-28 12:24:29,145:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:29,145:INFO:Checking exceptions
2025-02-28 12:24:29,145:INFO:Importing libraries
2025-02-28 12:24:29,145:INFO:Copying training dataset
2025-02-28 12:24:29,148:INFO:Defining folds
2025-02-28 12:24:29,148:INFO:Declaring metric variables
2025-02-28 12:24:29,151:INFO:Importing untrained model
2025-02-28 12:24:29,154:INFO:Random Forest Classifier Imported successfully
2025-02-28 12:24:29,158:INFO:Starting cross validation
2025-02-28 12:24:29,159:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:29,651:INFO:Calculating mean and std
2025-02-28 12:24:29,652:INFO:Creating metrics dataframe
2025-02-28 12:24:29,653:INFO:Uploading results into container
2025-02-28 12:24:29,653:INFO:Uploading model into container now
2025-02-28 12:24:29,654:INFO:_master_model_container: 7
2025-02-28 12:24:29,654:INFO:_display_container: 2
2025-02-28 12:24:29,654:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 12:24:29,654:INFO:create_model() successfully completed......................................
2025-02-28 12:24:29,789:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:29,790:INFO:Creating metrics dataframe
2025-02-28 12:24:29,795:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 12:24:29,795:INFO:Total runtime is 0.048177178700764975 minutes
2025-02-28 12:24:29,798:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:29,798:INFO:Initializing create_model()
2025-02-28 12:24:29,798:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:29,798:INFO:Checking exceptions
2025-02-28 12:24:29,798:INFO:Importing libraries
2025-02-28 12:24:29,798:INFO:Copying training dataset
2025-02-28 12:24:29,802:INFO:Defining folds
2025-02-28 12:24:29,802:INFO:Declaring metric variables
2025-02-28 12:24:29,804:INFO:Importing untrained model
2025-02-28 12:24:29,806:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 12:24:29,811:INFO:Starting cross validation
2025-02-28 12:24:29,812:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:29,861:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,862:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,865:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,869:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,870:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,872:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,872:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,879:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,880:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,880:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,881:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,885:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,886:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:24:29,889:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,889:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,891:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,898:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,899:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,907:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:29,918:INFO:Calculating mean and std
2025-02-28 12:24:29,918:INFO:Creating metrics dataframe
2025-02-28 12:24:29,920:INFO:Uploading results into container
2025-02-28 12:24:29,920:INFO:Uploading model into container now
2025-02-28 12:24:29,920:INFO:_master_model_container: 8
2025-02-28 12:24:29,920:INFO:_display_container: 2
2025-02-28 12:24:29,920:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 12:24:29,920:INFO:create_model() successfully completed......................................
2025-02-28 12:24:30,054:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:30,054:INFO:Creating metrics dataframe
2025-02-28 12:24:30,060:INFO:Initializing Ada Boost Classifier
2025-02-28 12:24:30,060:INFO:Total runtime is 0.05258921384811401 minutes
2025-02-28 12:24:30,062:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:30,062:INFO:Initializing create_model()
2025-02-28 12:24:30,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:30,063:INFO:Checking exceptions
2025-02-28 12:24:30,063:INFO:Importing libraries
2025-02-28 12:24:30,063:INFO:Copying training dataset
2025-02-28 12:24:30,065:INFO:Defining folds
2025-02-28 12:24:30,066:INFO:Declaring metric variables
2025-02-28 12:24:30,068:INFO:Importing untrained model
2025-02-28 12:24:30,070:INFO:Ada Boost Classifier Imported successfully
2025-02-28 12:24:30,074:INFO:Starting cross validation
2025-02-28 12:24:30,076:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:30,125:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,128:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,131:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,131:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,132:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,133:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,139:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,140:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,142:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:24:30,304:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,305:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,305:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,309:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,312:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,314:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,316:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,318:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,321:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,329:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:30,338:INFO:Calculating mean and std
2025-02-28 12:24:30,339:INFO:Creating metrics dataframe
2025-02-28 12:24:30,340:INFO:Uploading results into container
2025-02-28 12:24:30,340:INFO:Uploading model into container now
2025-02-28 12:24:30,340:INFO:_master_model_container: 9
2025-02-28 12:24:30,340:INFO:_display_container: 2
2025-02-28 12:24:30,341:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 12:24:30,341:INFO:create_model() successfully completed......................................
2025-02-28 12:24:30,471:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:30,472:INFO:Creating metrics dataframe
2025-02-28 12:24:30,476:INFO:Initializing Gradient Boosting Classifier
2025-02-28 12:24:30,476:INFO:Total runtime is 0.059534160296122234 minutes
2025-02-28 12:24:30,479:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:30,479:INFO:Initializing create_model()
2025-02-28 12:24:30,479:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:30,479:INFO:Checking exceptions
2025-02-28 12:24:30,479:INFO:Importing libraries
2025-02-28 12:24:30,479:INFO:Copying training dataset
2025-02-28 12:24:30,483:INFO:Defining folds
2025-02-28 12:24:30,483:INFO:Declaring metric variables
2025-02-28 12:24:30,486:INFO:Importing untrained model
2025-02-28 12:24:30,488:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 12:24:30,492:INFO:Starting cross validation
2025-02-28 12:24:30,493:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:33,261:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,262:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,278:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,279:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,302:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,302:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,309:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,329:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,362:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,369:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,380:INFO:Calculating mean and std
2025-02-28 12:24:33,381:INFO:Creating metrics dataframe
2025-02-28 12:24:33,382:INFO:Uploading results into container
2025-02-28 12:24:33,382:INFO:Uploading model into container now
2025-02-28 12:24:33,382:INFO:_master_model_container: 10
2025-02-28 12:24:33,382:INFO:_display_container: 2
2025-02-28 12:24:33,383:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:24:33,383:INFO:create_model() successfully completed......................................
2025-02-28 12:24:33,526:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:33,526:INFO:Creating metrics dataframe
2025-02-28 12:24:33,531:INFO:Initializing Linear Discriminant Analysis
2025-02-28 12:24:33,532:INFO:Total runtime is 0.11046508153279622 minutes
2025-02-28 12:24:33,534:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:33,534:INFO:Initializing create_model()
2025-02-28 12:24:33,534:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:33,534:INFO:Checking exceptions
2025-02-28 12:24:33,534:INFO:Importing libraries
2025-02-28 12:24:33,534:INFO:Copying training dataset
2025-02-28 12:24:33,538:INFO:Defining folds
2025-02-28 12:24:33,538:INFO:Declaring metric variables
2025-02-28 12:24:33,541:INFO:Importing untrained model
2025-02-28 12:24:33,543:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 12:24:33,547:INFO:Starting cross validation
2025-02-28 12:24:33,548:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:33,616:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,625:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,630:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,634:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,637:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,640:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,643:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,643:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,681:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:24:33,691:INFO:Calculating mean and std
2025-02-28 12:24:33,692:INFO:Creating metrics dataframe
2025-02-28 12:24:33,692:INFO:Uploading results into container
2025-02-28 12:24:33,694:INFO:Uploading model into container now
2025-02-28 12:24:33,694:INFO:_master_model_container: 11
2025-02-28 12:24:33,694:INFO:_display_container: 2
2025-02-28 12:24:33,694:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 12:24:33,694:INFO:create_model() successfully completed......................................
2025-02-28 12:24:33,835:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:33,835:INFO:Creating metrics dataframe
2025-02-28 12:24:33,840:INFO:Initializing Extra Trees Classifier
2025-02-28 12:24:33,840:INFO:Total runtime is 0.1156006654103597 minutes
2025-02-28 12:24:33,842:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:33,842:INFO:Initializing create_model()
2025-02-28 12:24:33,842:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:33,842:INFO:Checking exceptions
2025-02-28 12:24:33,842:INFO:Importing libraries
2025-02-28 12:24:33,842:INFO:Copying training dataset
2025-02-28 12:24:33,845:INFO:Defining folds
2025-02-28 12:24:33,845:INFO:Declaring metric variables
2025-02-28 12:24:33,848:INFO:Importing untrained model
2025-02-28 12:24:33,850:INFO:Extra Trees Classifier Imported successfully
2025-02-28 12:24:33,854:INFO:Starting cross validation
2025-02-28 12:24:33,856:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:34,267:INFO:Calculating mean and std
2025-02-28 12:24:34,268:INFO:Creating metrics dataframe
2025-02-28 12:24:34,269:INFO:Uploading results into container
2025-02-28 12:24:34,269:INFO:Uploading model into container now
2025-02-28 12:24:34,270:INFO:_master_model_container: 12
2025-02-28 12:24:34,270:INFO:_display_container: 2
2025-02-28 12:24:34,270:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 12:24:34,270:INFO:create_model() successfully completed......................................
2025-02-28 12:24:34,406:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:34,406:INFO:Creating metrics dataframe
2025-02-28 12:24:34,411:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:24:34,411:INFO:Total runtime is 0.1251200238863627 minutes
2025-02-28 12:24:34,414:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:34,415:INFO:Initializing create_model()
2025-02-28 12:24:34,415:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:34,415:INFO:Checking exceptions
2025-02-28 12:24:34,415:INFO:Importing libraries
2025-02-28 12:24:34,415:INFO:Copying training dataset
2025-02-28 12:24:34,418:INFO:Defining folds
2025-02-28 12:24:34,418:INFO:Declaring metric variables
2025-02-28 12:24:34,420:INFO:Importing untrained model
2025-02-28 12:24:34,423:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:24:34,427:INFO:Starting cross validation
2025-02-28 12:24:34,428:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:35,157:INFO:Calculating mean and std
2025-02-28 12:24:35,157:INFO:Creating metrics dataframe
2025-02-28 12:24:35,159:INFO:Uploading results into container
2025-02-28 12:24:35,159:INFO:Uploading model into container now
2025-02-28 12:24:35,159:INFO:_master_model_container: 13
2025-02-28 12:24:35,160:INFO:_display_container: 2
2025-02-28 12:24:35,160:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 12:24:35,160:INFO:create_model() successfully completed......................................
2025-02-28 12:24:35,296:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:35,296:INFO:Creating metrics dataframe
2025-02-28 12:24:35,302:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:24:35,302:INFO:Total runtime is 0.13995962540308635 minutes
2025-02-28 12:24:35,304:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:35,304:INFO:Initializing create_model()
2025-02-28 12:24:35,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:35,304:INFO:Checking exceptions
2025-02-28 12:24:35,304:INFO:Importing libraries
2025-02-28 12:24:35,304:INFO:Copying training dataset
2025-02-28 12:24:35,307:INFO:Defining folds
2025-02-28 12:24:35,307:INFO:Declaring metric variables
2025-02-28 12:24:35,310:INFO:Importing untrained model
2025-02-28 12:24:35,312:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:24:35,317:INFO:Starting cross validation
2025-02-28 12:24:35,318:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:43,006:INFO:Calculating mean and std
2025-02-28 12:24:43,007:INFO:Creating metrics dataframe
2025-02-28 12:24:43,008:INFO:Uploading results into container
2025-02-28 12:24:43,009:INFO:Uploading model into container now
2025-02-28 12:24:43,009:INFO:_master_model_container: 14
2025-02-28 12:24:43,009:INFO:_display_container: 2
2025-02-28 12:24:43,009:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:24:43,010:INFO:create_model() successfully completed......................................
2025-02-28 12:24:43,163:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:43,163:INFO:Creating metrics dataframe
2025-02-28 12:24:43,169:INFO:Initializing CatBoost Classifier
2025-02-28 12:24:43,169:INFO:Total runtime is 0.2710877617200216 minutes
2025-02-28 12:24:43,171:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:43,171:INFO:Initializing create_model()
2025-02-28 12:24:43,171:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:43,171:INFO:Checking exceptions
2025-02-28 12:24:43,171:INFO:Importing libraries
2025-02-28 12:24:43,171:INFO:Copying training dataset
2025-02-28 12:24:43,175:INFO:Defining folds
2025-02-28 12:24:43,175:INFO:Declaring metric variables
2025-02-28 12:24:43,177:INFO:Importing untrained model
2025-02-28 12:24:43,179:INFO:CatBoost Classifier Imported successfully
2025-02-28 12:24:43,183:INFO:Starting cross validation
2025-02-28 12:24:43,185:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:50,792:INFO:Calculating mean and std
2025-02-28 12:24:50,793:INFO:Creating metrics dataframe
2025-02-28 12:24:50,794:INFO:Uploading results into container
2025-02-28 12:24:50,794:INFO:Uploading model into container now
2025-02-28 12:24:50,794:INFO:_master_model_container: 15
2025-02-28 12:24:50,794:INFO:_display_container: 2
2025-02-28 12:24:50,794:INFO:<catboost.core.CatBoostClassifier object at 0x0000024E516CBE80>
2025-02-28 12:24:50,794:INFO:create_model() successfully completed......................................
2025-02-28 12:24:50,935:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:50,935:INFO:Creating metrics dataframe
2025-02-28 12:24:50,941:INFO:Initializing Dummy Classifier
2025-02-28 12:24:50,941:INFO:Total runtime is 0.4006140947341919 minutes
2025-02-28 12:24:50,943:INFO:SubProcess create_model() called ==================================
2025-02-28 12:24:50,943:INFO:Initializing create_model()
2025-02-28 12:24:50,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E5AE4B580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:50,943:INFO:Checking exceptions
2025-02-28 12:24:50,944:INFO:Importing libraries
2025-02-28 12:24:50,944:INFO:Copying training dataset
2025-02-28 12:24:50,947:INFO:Defining folds
2025-02-28 12:24:50,947:INFO:Declaring metric variables
2025-02-28 12:24:50,949:INFO:Importing untrained model
2025-02-28 12:24:50,952:INFO:Dummy Classifier Imported successfully
2025-02-28 12:24:50,956:INFO:Starting cross validation
2025-02-28 12:24:50,957:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:24:51,033:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,033:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,046:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,047:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,051:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,052:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,056:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,062:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:24:51,071:INFO:Calculating mean and std
2025-02-28 12:24:51,072:INFO:Creating metrics dataframe
2025-02-28 12:24:51,073:INFO:Uploading results into container
2025-02-28 12:24:51,073:INFO:Uploading model into container now
2025-02-28 12:24:51,074:INFO:_master_model_container: 16
2025-02-28 12:24:51,074:INFO:_display_container: 2
2025-02-28 12:24:51,074:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 12:24:51,074:INFO:create_model() successfully completed......................................
2025-02-28 12:24:51,215:INFO:SubProcess create_model() end ==================================
2025-02-28 12:24:51,215:INFO:Creating metrics dataframe
2025-02-28 12:24:51,221:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:24:51,227:INFO:Initializing create_model()
2025-02-28 12:24:51,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:24:51,227:INFO:Checking exceptions
2025-02-28 12:24:51,228:INFO:Importing libraries
2025-02-28 12:24:51,228:INFO:Copying training dataset
2025-02-28 12:24:51,231:INFO:Defining folds
2025-02-28 12:24:51,231:INFO:Declaring metric variables
2025-02-28 12:24:51,231:INFO:Importing untrained model
2025-02-28 12:24:51,231:INFO:Declaring custom model
2025-02-28 12:24:51,232:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:24:51,232:INFO:Cross validation set to False
2025-02-28 12:24:51,233:INFO:Fitting Model
2025-02-28 12:24:51,268:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:24:51,268:INFO:create_model() successfully completed......................................
2025-02-28 12:24:51,424:INFO:_master_model_container: 16
2025-02-28 12:24:51,424:INFO:_display_container: 2
2025-02-28 12:24:51,424:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:24:51,424:INFO:compare_models() successfully completed......................................
2025-02-28 12:24:51,445:INFO:Initializing plot_model()
2025-02-28 12:24:51,445:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, system=True)
2025-02-28 12:24:51,445:INFO:Checking exceptions
2025-02-28 12:24:51,448:INFO:Preloading libraries
2025-02-28 12:24:51,449:INFO:Copying training dataset
2025-02-28 12:24:51,449:INFO:Plot type: confusion_matrix
2025-02-28 12:24:51,593:INFO:Fitting Model
2025-02-28 12:24:51,593:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:24:51,593:INFO:Scoring test/hold-out set
2025-02-28 12:24:51,939:INFO:Visual Rendered Successfully
2025-02-28 12:24:52,076:INFO:plot_model() successfully completed......................................
2025-02-28 12:24:52,092:INFO:Initializing plot_model()
2025-02-28 12:24:52,092:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, system=True)
2025-02-28 12:24:52,092:INFO:Checking exceptions
2025-02-28 12:24:52,096:INFO:Preloading libraries
2025-02-28 12:24:52,096:INFO:Copying training dataset
2025-02-28 12:24:52,096:INFO:Plot type: auc
2025-02-28 12:24:52,241:INFO:Fitting Model
2025-02-28 12:24:52,241:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names
  warnings.warn(

2025-02-28 12:24:52,241:INFO:Scoring test/hold-out set
2025-02-28 12:24:52,555:INFO:Visual Rendered Successfully
2025-02-28 12:24:52,693:INFO:plot_model() successfully completed......................................
2025-02-28 12:24:52,710:INFO:Initializing plot_model()
2025-02-28 12:24:52,710:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024E5AE489A0>, system=True)
2025-02-28 12:24:52,710:INFO:Checking exceptions
2025-02-28 12:25:17,277:INFO:PyCaret RegressionExperiment
2025-02-28 12:25:17,278:INFO:Logging name: reg-default-name
2025-02-28 12:25:17,278:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 12:25:17,278:INFO:version 3.3.2
2025-02-28 12:25:17,278:INFO:Initializing setup()
2025-02-28 12:25:17,278:INFO:self.USI: 8123
2025-02-28 12:25:17,278:INFO:self._variable_keys: {'idx', 'X_test', 'y_train', 'X_train', 'X', 'y', 'y_test', 'exp_id', 'fold_groups_param', '_ml_usecase', 'fold_shuffle_param', 'USI', 'transform_target_param', 'n_jobs_param', 'log_plots_param', '_available_plots', 'html_param', 'target_param', 'memory', 'exp_name_log', 'seed', 'pipeline', 'gpu_param', 'fold_generator', 'gpu_n_jobs_param', 'logging_param', 'data'}
2025-02-28 12:25:17,278:INFO:Checking environment
2025-02-28 12:25:17,278:INFO:python_version: 3.10.16
2025-02-28 12:25:17,278:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:25:17,278:INFO:machine: AMD64
2025-02-28 12:25:17,278:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:25:17,278:INFO:Memory: svmem(total=34200334336, available=7993126912, percent=76.6, used=26207207424, free=7993126912)
2025-02-28 12:25:17,278:INFO:Physical Core: 24
2025-02-28 12:25:17,278:INFO:Logical Core: 32
2025-02-28 12:25:17,279:INFO:Checking libraries
2025-02-28 12:25:17,279:INFO:System:
2025-02-28 12:25:17,279:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:25:17,279:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:25:17,279:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:25:17,279:INFO:PyCaret required dependencies:
2025-02-28 12:25:17,279:INFO:                 pip: 25.0
2025-02-28 12:25:17,279:INFO:          setuptools: 75.8.0
2025-02-28 12:25:17,279:INFO:             pycaret: 3.3.2
2025-02-28 12:25:17,279:INFO:             IPython: 8.30.0
2025-02-28 12:25:17,279:INFO:          ipywidgets: 8.1.5
2025-02-28 12:25:17,279:INFO:                tqdm: 4.67.1
2025-02-28 12:25:17,279:INFO:               numpy: 1.26.4
2025-02-28 12:25:17,279:INFO:              pandas: 2.1.4
2025-02-28 12:25:17,279:INFO:              jinja2: 3.1.5
2025-02-28 12:25:17,280:INFO:               scipy: 1.11.4
2025-02-28 12:25:17,280:INFO:              joblib: 1.3.2
2025-02-28 12:25:17,280:INFO:             sklearn: 1.4.2
2025-02-28 12:25:17,280:INFO:                pyod: 2.0.3
2025-02-28 12:25:17,280:INFO:            imblearn: 0.13.0
2025-02-28 12:25:17,280:INFO:   category_encoders: 2.7.0
2025-02-28 12:25:17,280:INFO:            lightgbm: 4.5.0
2025-02-28 12:25:17,280:INFO:               numba: 0.61.0
2025-02-28 12:25:17,280:INFO:            requests: 2.32.3
2025-02-28 12:25:17,280:INFO:          matplotlib: 3.7.5
2025-02-28 12:25:17,280:INFO:          scikitplot: 0.3.7
2025-02-28 12:25:17,280:INFO:         yellowbrick: 1.5
2025-02-28 12:25:17,280:INFO:              plotly: 5.24.1
2025-02-28 12:25:17,280:INFO:    plotly-resampler: Not installed
2025-02-28 12:25:17,280:INFO:             kaleido: 0.2.1
2025-02-28 12:25:17,280:INFO:           schemdraw: 0.15
2025-02-28 12:25:17,280:INFO:         statsmodels: 0.14.4
2025-02-28 12:25:17,280:INFO:              sktime: 0.26.0
2025-02-28 12:25:17,280:INFO:               tbats: 1.1.3
2025-02-28 12:25:17,280:INFO:            pmdarima: 2.0.4
2025-02-28 12:25:17,280:INFO:              psutil: 5.9.0
2025-02-28 12:25:17,280:INFO:          markupsafe: 2.1.5
2025-02-28 12:25:17,280:INFO:             pickle5: Not installed
2025-02-28 12:25:17,280:INFO:         cloudpickle: 3.1.1
2025-02-28 12:25:17,280:INFO:         deprecation: 2.1.0
2025-02-28 12:25:17,280:INFO:              xxhash: 3.5.0
2025-02-28 12:25:17,280:INFO:           wurlitzer: Not installed
2025-02-28 12:25:17,281:INFO:PyCaret optional dependencies:
2025-02-28 12:25:17,281:INFO:                shap: 0.44.1
2025-02-28 12:25:17,281:INFO:           interpret: 0.6.9
2025-02-28 12:25:17,281:INFO:                umap: 0.5.7
2025-02-28 12:25:17,281:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:25:17,281:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:25:17,281:INFO:             autoviz: Not installed
2025-02-28 12:25:17,281:INFO:           fairlearn: 0.7.0
2025-02-28 12:25:17,281:INFO:          deepchecks: Not installed
2025-02-28 12:25:17,281:INFO:             xgboost: 2.1.4
2025-02-28 12:25:17,281:INFO:            catboost: 1.2.7
2025-02-28 12:25:17,281:INFO:              kmodes: 0.12.2
2025-02-28 12:25:17,281:INFO:             mlxtend: 0.23.4
2025-02-28 12:25:17,281:INFO:       statsforecast: 1.5.0
2025-02-28 12:25:17,281:INFO:        tune_sklearn: Not installed
2025-02-28 12:25:17,281:INFO:                 ray: Not installed
2025-02-28 12:25:17,281:INFO:            hyperopt: 0.2.7
2025-02-28 12:25:17,281:INFO:              optuna: 4.2.0
2025-02-28 12:25:17,281:INFO:               skopt: 0.10.2
2025-02-28 12:25:17,281:INFO:              mlflow: 2.20.1
2025-02-28 12:25:17,281:INFO:              gradio: 5.15.0
2025-02-28 12:25:17,281:INFO:             fastapi: 0.115.8
2025-02-28 12:25:17,281:INFO:             uvicorn: 0.34.0
2025-02-28 12:25:17,281:INFO:              m2cgen: 0.10.0
2025-02-28 12:25:17,281:INFO:           evidently: 0.4.40
2025-02-28 12:25:17,281:INFO:               fugue: 0.8.7
2025-02-28 12:25:17,281:INFO:           streamlit: Not installed
2025-02-28 12:25:17,281:INFO:             prophet: Not installed
2025-02-28 12:25:17,282:INFO:None
2025-02-28 12:25:17,282:INFO:Set up data.
2025-02-28 12:25:17,287:INFO:Set up folding strategy.
2025-02-28 12:25:17,287:INFO:Set up train/test split.
2025-02-28 12:25:17,290:INFO:Set up index.
2025-02-28 12:25:17,290:INFO:Assigning column types.
2025-02-28 12:25:17,292:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:25:17,292:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,294:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,296:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,321:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,340:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,340:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,341:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,341:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,343:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,346:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,371:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,389:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,389:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,390:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,390:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 12:25:17,392:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,395:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,419:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,438:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,438:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,439:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,441:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,442:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,469:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,489:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,489:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,490:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,491:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 12:25:17,494:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,520:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,538:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,538:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,539:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,544:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,569:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,589:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,589:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,590:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,590:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 12:25:17,620:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,639:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,639:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,640:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,669:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,688:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,688:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,689:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,690:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:25:17,720:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,738:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,740:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,769:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:25:17,788:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,790:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,790:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 12:25:17,839:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,841:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,890:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:17,891:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:17,892:INFO:Preparing preprocessing pipeline...
2025-02-28 12:25:17,892:INFO:Set up simple imputation.
2025-02-28 12:25:17,894:INFO:Set up encoding of ordinal features.
2025-02-28 12:25:17,895:INFO:Set up encoding of categorical features.
2025-02-28 12:25:17,939:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:25:17,947:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score', 'Job_Offers',
                                             'Career_Satisfa...
dtype: int64}],
                                                               return_df=True,
                                                               verbose=0))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study',
                                             'Current_Job_Level'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study',
                                                                    'Current_Job_Level'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:25:17,947:INFO:Creating final display dataframe.
2025-02-28 12:25:18,087:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 19)
4        Transformed data shape        (5000, 30)
5   Transformed train set shape        (3500, 30)
6    Transformed test set shape        (1500, 30)
7              Numeric features                14
8          Categorical features                 4
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              8123
2025-02-28 12:25:18,144:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:18,145:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:18,192:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:25:18,193:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:25:18,194:INFO:setup() successfully completed in 0.92s...............
2025-02-28 12:25:18,204:INFO:Initializing compare_models()
2025-02-28 12:25:18,204:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 12:25:18,204:INFO:Checking exceptions
2025-02-28 12:25:18,206:INFO:Preparing display monitor
2025-02-28 12:25:18,219:INFO:Initializing Linear Regression
2025-02-28 12:25:18,220:INFO:Total runtime is 1.6661485036214194e-05 minutes
2025-02-28 12:25:18,222:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:18,222:INFO:Initializing create_model()
2025-02-28 12:25:18,222:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:18,222:INFO:Checking exceptions
2025-02-28 12:25:18,222:INFO:Importing libraries
2025-02-28 12:25:18,222:INFO:Copying training dataset
2025-02-28 12:25:18,224:INFO:Defining folds
2025-02-28 12:25:18,225:INFO:Declaring metric variables
2025-02-28 12:25:18,227:INFO:Importing untrained model
2025-02-28 12:25:18,229:INFO:Linear Regression Imported successfully
2025-02-28 12:25:18,233:INFO:Starting cross validation
2025-02-28 12:25:18,235:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:18,401:INFO:Calculating mean and std
2025-02-28 12:25:18,401:INFO:Creating metrics dataframe
2025-02-28 12:25:18,402:INFO:Uploading results into container
2025-02-28 12:25:18,402:INFO:Uploading model into container now
2025-02-28 12:25:18,402:INFO:_master_model_container: 1
2025-02-28 12:25:18,402:INFO:_display_container: 2
2025-02-28 12:25:18,403:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 12:25:18,403:INFO:create_model() successfully completed......................................
2025-02-28 12:25:18,546:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:18,546:INFO:Creating metrics dataframe
2025-02-28 12:25:18,549:INFO:Initializing Lasso Regression
2025-02-28 12:25:18,549:INFO:Total runtime is 0.005511474609375001 minutes
2025-02-28 12:25:18,552:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:18,552:INFO:Initializing create_model()
2025-02-28 12:25:18,552:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:18,552:INFO:Checking exceptions
2025-02-28 12:25:18,552:INFO:Importing libraries
2025-02-28 12:25:18,552:INFO:Copying training dataset
2025-02-28 12:25:18,556:INFO:Defining folds
2025-02-28 12:25:18,556:INFO:Declaring metric variables
2025-02-28 12:25:18,558:INFO:Importing untrained model
2025-02-28 12:25:18,560:INFO:Lasso Regression Imported successfully
2025-02-28 12:25:18,564:INFO:Starting cross validation
2025-02-28 12:25:18,565:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:18,680:INFO:Calculating mean and std
2025-02-28 12:25:18,680:INFO:Creating metrics dataframe
2025-02-28 12:25:18,681:INFO:Uploading results into container
2025-02-28 12:25:18,681:INFO:Uploading model into container now
2025-02-28 12:25:18,682:INFO:_master_model_container: 2
2025-02-28 12:25:18,682:INFO:_display_container: 2
2025-02-28 12:25:18,682:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 12:25:18,682:INFO:create_model() successfully completed......................................
2025-02-28 12:25:18,822:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:18,822:INFO:Creating metrics dataframe
2025-02-28 12:25:18,827:INFO:Initializing Ridge Regression
2025-02-28 12:25:18,827:INFO:Total runtime is 0.010136560599009196 minutes
2025-02-28 12:25:18,829:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:18,829:INFO:Initializing create_model()
2025-02-28 12:25:18,830:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:18,830:INFO:Checking exceptions
2025-02-28 12:25:18,830:INFO:Importing libraries
2025-02-28 12:25:18,830:INFO:Copying training dataset
2025-02-28 12:25:18,832:INFO:Defining folds
2025-02-28 12:25:18,833:INFO:Declaring metric variables
2025-02-28 12:25:18,834:INFO:Importing untrained model
2025-02-28 12:25:18,836:INFO:Ridge Regression Imported successfully
2025-02-28 12:25:18,840:INFO:Starting cross validation
2025-02-28 12:25:18,841:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:18,946:INFO:Calculating mean and std
2025-02-28 12:25:18,946:INFO:Creating metrics dataframe
2025-02-28 12:25:18,947:INFO:Uploading results into container
2025-02-28 12:25:18,948:INFO:Uploading model into container now
2025-02-28 12:25:18,948:INFO:_master_model_container: 3
2025-02-28 12:25:18,948:INFO:_display_container: 2
2025-02-28 12:25:18,948:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 12:25:18,948:INFO:create_model() successfully completed......................................
2025-02-28 12:25:19,095:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:19,095:INFO:Creating metrics dataframe
2025-02-28 12:25:19,100:INFO:Initializing Elastic Net
2025-02-28 12:25:19,100:INFO:Total runtime is 0.014685277144114176 minutes
2025-02-28 12:25:19,103:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:19,103:INFO:Initializing create_model()
2025-02-28 12:25:19,103:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:19,103:INFO:Checking exceptions
2025-02-28 12:25:19,103:INFO:Importing libraries
2025-02-28 12:25:19,103:INFO:Copying training dataset
2025-02-28 12:25:19,105:INFO:Defining folds
2025-02-28 12:25:19,106:INFO:Declaring metric variables
2025-02-28 12:25:19,108:INFO:Importing untrained model
2025-02-28 12:25:19,111:INFO:Elastic Net Imported successfully
2025-02-28 12:25:19,115:INFO:Starting cross validation
2025-02-28 12:25:19,116:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:19,226:INFO:Calculating mean and std
2025-02-28 12:25:19,226:INFO:Creating metrics dataframe
2025-02-28 12:25:19,228:INFO:Uploading results into container
2025-02-28 12:25:19,228:INFO:Uploading model into container now
2025-02-28 12:25:19,228:INFO:_master_model_container: 4
2025-02-28 12:25:19,228:INFO:_display_container: 2
2025-02-28 12:25:19,228:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 12:25:19,228:INFO:create_model() successfully completed......................................
2025-02-28 12:25:19,370:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:19,370:INFO:Creating metrics dataframe
2025-02-28 12:25:19,374:INFO:Initializing Least Angle Regression
2025-02-28 12:25:19,374:INFO:Total runtime is 0.019255654017130534 minutes
2025-02-28 12:25:19,376:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:19,377:INFO:Initializing create_model()
2025-02-28 12:25:19,377:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:19,377:INFO:Checking exceptions
2025-02-28 12:25:19,377:INFO:Importing libraries
2025-02-28 12:25:19,377:INFO:Copying training dataset
2025-02-28 12:25:19,379:INFO:Defining folds
2025-02-28 12:25:19,379:INFO:Declaring metric variables
2025-02-28 12:25:19,382:INFO:Importing untrained model
2025-02-28 12:25:19,385:INFO:Least Angle Regression Imported successfully
2025-02-28 12:25:19,389:INFO:Starting cross validation
2025-02-28 12:25:19,390:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:19,491:INFO:Calculating mean and std
2025-02-28 12:25:19,492:INFO:Creating metrics dataframe
2025-02-28 12:25:19,493:INFO:Uploading results into container
2025-02-28 12:25:19,493:INFO:Uploading model into container now
2025-02-28 12:25:19,493:INFO:_master_model_container: 5
2025-02-28 12:25:19,493:INFO:_display_container: 2
2025-02-28 12:25:19,493:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 12:25:19,493:INFO:create_model() successfully completed......................................
2025-02-28 12:25:19,634:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:19,634:INFO:Creating metrics dataframe
2025-02-28 12:25:19,638:INFO:Initializing Lasso Least Angle Regression
2025-02-28 12:25:19,638:INFO:Total runtime is 0.02365705172220866 minutes
2025-02-28 12:25:19,640:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:19,640:INFO:Initializing create_model()
2025-02-28 12:25:19,640:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:19,641:INFO:Checking exceptions
2025-02-28 12:25:19,641:INFO:Importing libraries
2025-02-28 12:25:19,641:INFO:Copying training dataset
2025-02-28 12:25:19,644:INFO:Defining folds
2025-02-28 12:25:19,644:INFO:Declaring metric variables
2025-02-28 12:25:19,646:INFO:Importing untrained model
2025-02-28 12:25:19,649:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 12:25:19,653:INFO:Starting cross validation
2025-02-28 12:25:19,654:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:19,757:INFO:Calculating mean and std
2025-02-28 12:25:19,758:INFO:Creating metrics dataframe
2025-02-28 12:25:19,759:INFO:Uploading results into container
2025-02-28 12:25:19,759:INFO:Uploading model into container now
2025-02-28 12:25:19,759:INFO:_master_model_container: 6
2025-02-28 12:25:19,759:INFO:_display_container: 2
2025-02-28 12:25:19,760:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 12:25:19,760:INFO:create_model() successfully completed......................................
2025-02-28 12:25:19,896:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:19,896:INFO:Creating metrics dataframe
2025-02-28 12:25:19,900:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 12:25:19,900:INFO:Total runtime is 0.028023290634155273 minutes
2025-02-28 12:25:19,903:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:19,903:INFO:Initializing create_model()
2025-02-28 12:25:19,903:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:19,903:INFO:Checking exceptions
2025-02-28 12:25:19,903:INFO:Importing libraries
2025-02-28 12:25:19,903:INFO:Copying training dataset
2025-02-28 12:25:19,906:INFO:Defining folds
2025-02-28 12:25:19,906:INFO:Declaring metric variables
2025-02-28 12:25:19,908:INFO:Importing untrained model
2025-02-28 12:25:19,910:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 12:25:19,915:INFO:Starting cross validation
2025-02-28 12:25:19,917:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:20,022:INFO:Calculating mean and std
2025-02-28 12:25:20,023:INFO:Creating metrics dataframe
2025-02-28 12:25:20,024:INFO:Uploading results into container
2025-02-28 12:25:20,025:INFO:Uploading model into container now
2025-02-28 12:25:20,025:INFO:_master_model_container: 7
2025-02-28 12:25:20,025:INFO:_display_container: 2
2025-02-28 12:25:20,025:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 12:25:20,025:INFO:create_model() successfully completed......................................
2025-02-28 12:25:20,161:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:20,162:INFO:Creating metrics dataframe
2025-02-28 12:25:20,166:INFO:Initializing Bayesian Ridge
2025-02-28 12:25:20,166:INFO:Total runtime is 0.03245712916056315 minutes
2025-02-28 12:25:20,168:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:20,168:INFO:Initializing create_model()
2025-02-28 12:25:20,168:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:20,168:INFO:Checking exceptions
2025-02-28 12:25:20,168:INFO:Importing libraries
2025-02-28 12:25:20,168:INFO:Copying training dataset
2025-02-28 12:25:20,172:INFO:Defining folds
2025-02-28 12:25:20,172:INFO:Declaring metric variables
2025-02-28 12:25:20,174:INFO:Importing untrained model
2025-02-28 12:25:20,177:INFO:Bayesian Ridge Imported successfully
2025-02-28 12:25:20,182:INFO:Starting cross validation
2025-02-28 12:25:20,183:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:20,288:INFO:Calculating mean and std
2025-02-28 12:25:20,289:INFO:Creating metrics dataframe
2025-02-28 12:25:20,290:INFO:Uploading results into container
2025-02-28 12:25:20,290:INFO:Uploading model into container now
2025-02-28 12:25:20,290:INFO:_master_model_container: 8
2025-02-28 12:25:20,290:INFO:_display_container: 2
2025-02-28 12:25:20,291:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 12:25:20,291:INFO:create_model() successfully completed......................................
2025-02-28 12:25:20,432:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:20,432:INFO:Creating metrics dataframe
2025-02-28 12:25:20,436:INFO:Initializing Passive Aggressive Regressor
2025-02-28 12:25:20,436:INFO:Total runtime is 0.03695876598358154 minutes
2025-02-28 12:25:20,439:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:20,439:INFO:Initializing create_model()
2025-02-28 12:25:20,439:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:20,439:INFO:Checking exceptions
2025-02-28 12:25:20,439:INFO:Importing libraries
2025-02-28 12:25:20,439:INFO:Copying training dataset
2025-02-28 12:25:20,442:INFO:Defining folds
2025-02-28 12:25:20,442:INFO:Declaring metric variables
2025-02-28 12:25:20,445:INFO:Importing untrained model
2025-02-28 12:25:20,447:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 12:25:20,451:INFO:Starting cross validation
2025-02-28 12:25:20,452:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:20,568:INFO:Calculating mean and std
2025-02-28 12:25:20,569:INFO:Creating metrics dataframe
2025-02-28 12:25:20,570:INFO:Uploading results into container
2025-02-28 12:25:20,570:INFO:Uploading model into container now
2025-02-28 12:25:20,570:INFO:_master_model_container: 9
2025-02-28 12:25:20,570:INFO:_display_container: 2
2025-02-28 12:25:20,571:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:25:20,571:INFO:create_model() successfully completed......................................
2025-02-28 12:25:20,710:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:20,710:INFO:Creating metrics dataframe
2025-02-28 12:25:20,714:INFO:Initializing Huber Regressor
2025-02-28 12:25:20,714:INFO:Total runtime is 0.0415929635365804 minutes
2025-02-28 12:25:20,716:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:20,716:INFO:Initializing create_model()
2025-02-28 12:25:20,716:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:20,716:INFO:Checking exceptions
2025-02-28 12:25:20,716:INFO:Importing libraries
2025-02-28 12:25:20,717:INFO:Copying training dataset
2025-02-28 12:25:20,720:INFO:Defining folds
2025-02-28 12:25:20,720:INFO:Declaring metric variables
2025-02-28 12:25:20,723:INFO:Importing untrained model
2025-02-28 12:25:20,725:INFO:Huber Regressor Imported successfully
2025-02-28 12:25:20,729:INFO:Starting cross validation
2025-02-28 12:25:20,730:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:20,803:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,803:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,809:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,809:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,820:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,826:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,833:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,840:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,845:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:25:20,862:INFO:Calculating mean and std
2025-02-28 12:25:20,862:INFO:Creating metrics dataframe
2025-02-28 12:25:20,864:INFO:Uploading results into container
2025-02-28 12:25:20,864:INFO:Uploading model into container now
2025-02-28 12:25:20,864:INFO:_master_model_container: 10
2025-02-28 12:25:20,864:INFO:_display_container: 2
2025-02-28 12:25:20,864:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 12:25:20,864:INFO:create_model() successfully completed......................................
2025-02-28 12:25:21,005:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:21,006:INFO:Creating metrics dataframe
2025-02-28 12:25:21,010:INFO:Initializing K Neighbors Regressor
2025-02-28 12:25:21,010:INFO:Total runtime is 0.046526400248209636 minutes
2025-02-28 12:25:21,012:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:21,013:INFO:Initializing create_model()
2025-02-28 12:25:21,013:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:21,013:INFO:Checking exceptions
2025-02-28 12:25:21,013:INFO:Importing libraries
2025-02-28 12:25:21,013:INFO:Copying training dataset
2025-02-28 12:25:21,017:INFO:Defining folds
2025-02-28 12:25:21,017:INFO:Declaring metric variables
2025-02-28 12:25:21,020:INFO:Importing untrained model
2025-02-28 12:25:21,022:INFO:K Neighbors Regressor Imported successfully
2025-02-28 12:25:21,026:INFO:Starting cross validation
2025-02-28 12:25:21,027:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:21,127:INFO:Calculating mean and std
2025-02-28 12:25:21,127:INFO:Creating metrics dataframe
2025-02-28 12:25:21,129:INFO:Uploading results into container
2025-02-28 12:25:21,129:INFO:Uploading model into container now
2025-02-28 12:25:21,129:INFO:_master_model_container: 11
2025-02-28 12:25:21,130:INFO:_display_container: 2
2025-02-28 12:25:21,130:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 12:25:21,130:INFO:create_model() successfully completed......................................
2025-02-28 12:25:21,268:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:21,268:INFO:Creating metrics dataframe
2025-02-28 12:25:21,272:INFO:Initializing Decision Tree Regressor
2025-02-28 12:25:21,272:INFO:Total runtime is 0.050894276301066084 minutes
2025-02-28 12:25:21,276:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:21,276:INFO:Initializing create_model()
2025-02-28 12:25:21,276:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:21,276:INFO:Checking exceptions
2025-02-28 12:25:21,276:INFO:Importing libraries
2025-02-28 12:25:21,276:INFO:Copying training dataset
2025-02-28 12:25:21,279:INFO:Defining folds
2025-02-28 12:25:21,279:INFO:Declaring metric variables
2025-02-28 12:25:21,282:INFO:Importing untrained model
2025-02-28 12:25:21,284:INFO:Decision Tree Regressor Imported successfully
2025-02-28 12:25:21,288:INFO:Starting cross validation
2025-02-28 12:25:21,289:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:21,408:INFO:Calculating mean and std
2025-02-28 12:25:21,409:INFO:Creating metrics dataframe
2025-02-28 12:25:21,410:INFO:Uploading results into container
2025-02-28 12:25:21,410:INFO:Uploading model into container now
2025-02-28 12:25:21,410:INFO:_master_model_container: 12
2025-02-28 12:25:21,410:INFO:_display_container: 2
2025-02-28 12:25:21,410:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:25:21,411:INFO:create_model() successfully completed......................................
2025-02-28 12:25:21,554:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:21,555:INFO:Creating metrics dataframe
2025-02-28 12:25:21,559:INFO:Initializing Random Forest Regressor
2025-02-28 12:25:21,559:INFO:Total runtime is 0.05567668676376343 minutes
2025-02-28 12:25:21,562:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:21,562:INFO:Initializing create_model()
2025-02-28 12:25:21,562:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:21,562:INFO:Checking exceptions
2025-02-28 12:25:21,562:INFO:Importing libraries
2025-02-28 12:25:21,562:INFO:Copying training dataset
2025-02-28 12:25:21,566:INFO:Defining folds
2025-02-28 12:25:21,567:INFO:Declaring metric variables
2025-02-28 12:25:21,569:INFO:Importing untrained model
2025-02-28 12:25:21,571:INFO:Random Forest Regressor Imported successfully
2025-02-28 12:25:21,575:INFO:Starting cross validation
2025-02-28 12:25:21,576:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:22,628:INFO:Calculating mean and std
2025-02-28 12:25:22,628:INFO:Creating metrics dataframe
2025-02-28 12:25:22,630:INFO:Uploading results into container
2025-02-28 12:25:22,630:INFO:Uploading model into container now
2025-02-28 12:25:22,630:INFO:_master_model_container: 13
2025-02-28 12:25:22,630:INFO:_display_container: 2
2025-02-28 12:25:22,631:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 12:25:22,631:INFO:create_model() successfully completed......................................
2025-02-28 12:25:22,773:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:22,773:INFO:Creating metrics dataframe
2025-02-28 12:25:22,779:INFO:Initializing Extra Trees Regressor
2025-02-28 12:25:22,779:INFO:Total runtime is 0.07600015799204508 minutes
2025-02-28 12:25:22,781:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:22,781:INFO:Initializing create_model()
2025-02-28 12:25:22,781:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:22,781:INFO:Checking exceptions
2025-02-28 12:25:22,781:INFO:Importing libraries
2025-02-28 12:25:22,781:INFO:Copying training dataset
2025-02-28 12:25:22,786:INFO:Defining folds
2025-02-28 12:25:22,786:INFO:Declaring metric variables
2025-02-28 12:25:22,789:INFO:Importing untrained model
2025-02-28 12:25:22,791:INFO:Extra Trees Regressor Imported successfully
2025-02-28 12:25:22,795:INFO:Starting cross validation
2025-02-28 12:25:22,796:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:23,577:INFO:Calculating mean and std
2025-02-28 12:25:23,578:INFO:Creating metrics dataframe
2025-02-28 12:25:23,579:INFO:Uploading results into container
2025-02-28 12:25:23,580:INFO:Uploading model into container now
2025-02-28 12:25:23,580:INFO:_master_model_container: 14
2025-02-28 12:25:23,580:INFO:_display_container: 2
2025-02-28 12:25:23,580:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 12:25:23,580:INFO:create_model() successfully completed......................................
2025-02-28 12:25:23,724:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:23,724:INFO:Creating metrics dataframe
2025-02-28 12:25:23,729:INFO:Initializing AdaBoost Regressor
2025-02-28 12:25:23,729:INFO:Total runtime is 0.09183123111724853 minutes
2025-02-28 12:25:23,731:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:23,732:INFO:Initializing create_model()
2025-02-28 12:25:23,732:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:23,732:INFO:Checking exceptions
2025-02-28 12:25:23,732:INFO:Importing libraries
2025-02-28 12:25:23,732:INFO:Copying training dataset
2025-02-28 12:25:23,735:INFO:Defining folds
2025-02-28 12:25:23,735:INFO:Declaring metric variables
2025-02-28 12:25:23,738:INFO:Importing untrained model
2025-02-28 12:25:23,740:INFO:AdaBoost Regressor Imported successfully
2025-02-28 12:25:23,745:INFO:Starting cross validation
2025-02-28 12:25:23,746:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:23,936:INFO:Calculating mean and std
2025-02-28 12:25:23,937:INFO:Creating metrics dataframe
2025-02-28 12:25:23,938:INFO:Uploading results into container
2025-02-28 12:25:23,938:INFO:Uploading model into container now
2025-02-28 12:25:23,938:INFO:_master_model_container: 15
2025-02-28 12:25:23,938:INFO:_display_container: 2
2025-02-28 12:25:23,938:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 12:25:23,939:INFO:create_model() successfully completed......................................
2025-02-28 12:25:24,077:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:24,077:INFO:Creating metrics dataframe
2025-02-28 12:25:24,083:INFO:Initializing Gradient Boosting Regressor
2025-02-28 12:25:24,083:INFO:Total runtime is 0.0977335770924886 minutes
2025-02-28 12:25:24,085:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:24,085:INFO:Initializing create_model()
2025-02-28 12:25:24,085:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:24,085:INFO:Checking exceptions
2025-02-28 12:25:24,085:INFO:Importing libraries
2025-02-28 12:25:24,085:INFO:Copying training dataset
2025-02-28 12:25:24,089:INFO:Defining folds
2025-02-28 12:25:24,089:INFO:Declaring metric variables
2025-02-28 12:25:24,091:INFO:Importing untrained model
2025-02-28 12:25:24,093:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 12:25:24,099:INFO:Starting cross validation
2025-02-28 12:25:24,100:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:24,613:INFO:Calculating mean and std
2025-02-28 12:25:24,613:INFO:Creating metrics dataframe
2025-02-28 12:25:24,614:INFO:Uploading results into container
2025-02-28 12:25:24,615:INFO:Uploading model into container now
2025-02-28 12:25:24,615:INFO:_master_model_container: 16
2025-02-28 12:25:24,615:INFO:_display_container: 2
2025-02-28 12:25:24,615:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:25:24,615:INFO:create_model() successfully completed......................................
2025-02-28 12:25:24,754:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:24,754:INFO:Creating metrics dataframe
2025-02-28 12:25:24,759:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:25:24,759:INFO:Total runtime is 0.1090067187945048 minutes
2025-02-28 12:25:24,762:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:24,762:INFO:Initializing create_model()
2025-02-28 12:25:24,762:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:24,762:INFO:Checking exceptions
2025-02-28 12:25:24,762:INFO:Importing libraries
2025-02-28 12:25:24,762:INFO:Copying training dataset
2025-02-28 12:25:24,766:INFO:Defining folds
2025-02-28 12:25:24,766:INFO:Declaring metric variables
2025-02-28 12:25:24,769:INFO:Importing untrained model
2025-02-28 12:25:24,771:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:25:24,776:INFO:Starting cross validation
2025-02-28 12:25:24,777:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:25,096:INFO:Calculating mean and std
2025-02-28 12:25:25,096:INFO:Creating metrics dataframe
2025-02-28 12:25:25,098:INFO:Uploading results into container
2025-02-28 12:25:25,098:INFO:Uploading model into container now
2025-02-28 12:25:25,098:INFO:_master_model_container: 17
2025-02-28 12:25:25,098:INFO:_display_container: 2
2025-02-28 12:25:25,099:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:25:25,099:INFO:create_model() successfully completed......................................
2025-02-28 12:25:25,241:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:25,241:INFO:Creating metrics dataframe
2025-02-28 12:25:25,248:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:25:25,248:INFO:Total runtime is 0.11714901129404703 minutes
2025-02-28 12:25:25,250:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:25,250:INFO:Initializing create_model()
2025-02-28 12:25:25,250:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:25,250:INFO:Checking exceptions
2025-02-28 12:25:25,250:INFO:Importing libraries
2025-02-28 12:25:25,250:INFO:Copying training dataset
2025-02-28 12:25:25,253:INFO:Defining folds
2025-02-28 12:25:25,253:INFO:Declaring metric variables
2025-02-28 12:25:25,255:INFO:Importing untrained model
2025-02-28 12:25:25,258:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:25:25,262:INFO:Starting cross validation
2025-02-28 12:25:25,263:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:26,725:INFO:Calculating mean and std
2025-02-28 12:25:26,726:INFO:Creating metrics dataframe
2025-02-28 12:25:26,727:INFO:Uploading results into container
2025-02-28 12:25:26,727:INFO:Uploading model into container now
2025-02-28 12:25:26,728:INFO:_master_model_container: 18
2025-02-28 12:25:26,728:INFO:_display_container: 2
2025-02-28 12:25:26,728:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:25:26,728:INFO:create_model() successfully completed......................................
2025-02-28 12:25:26,888:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:26,888:INFO:Creating metrics dataframe
2025-02-28 12:25:26,894:INFO:Initializing CatBoost Regressor
2025-02-28 12:25:26,894:INFO:Total runtime is 0.1445919076601664 minutes
2025-02-28 12:25:26,896:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:26,896:INFO:Initializing create_model()
2025-02-28 12:25:26,896:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:26,896:INFO:Checking exceptions
2025-02-28 12:25:26,896:INFO:Importing libraries
2025-02-28 12:25:26,896:INFO:Copying training dataset
2025-02-28 12:25:26,900:INFO:Defining folds
2025-02-28 12:25:26,900:INFO:Declaring metric variables
2025-02-28 12:25:26,902:INFO:Importing untrained model
2025-02-28 12:25:26,905:INFO:CatBoost Regressor Imported successfully
2025-02-28 12:25:26,909:INFO:Starting cross validation
2025-02-28 12:25:26,910:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:29,724:INFO:Calculating mean and std
2025-02-28 12:25:29,724:INFO:Creating metrics dataframe
2025-02-28 12:25:29,727:INFO:Uploading results into container
2025-02-28 12:25:29,727:INFO:Uploading model into container now
2025-02-28 12:25:29,727:INFO:_master_model_container: 19
2025-02-28 12:25:29,727:INFO:_display_container: 2
2025-02-28 12:25:29,727:INFO:<catboost.core.CatBoostRegressor object at 0x0000024E5B9DF370>
2025-02-28 12:25:29,727:INFO:create_model() successfully completed......................................
2025-02-28 12:25:29,868:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:29,869:INFO:Creating metrics dataframe
2025-02-28 12:25:29,874:INFO:Initializing Dummy Regressor
2025-02-28 12:25:29,875:INFO:Total runtime is 0.1942762017250061 minutes
2025-02-28 12:25:29,877:INFO:SubProcess create_model() called ==================================
2025-02-28 12:25:29,877:INFO:Initializing create_model()
2025-02-28 12:25:29,877:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024E539ED720>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:29,877:INFO:Checking exceptions
2025-02-28 12:25:29,877:INFO:Importing libraries
2025-02-28 12:25:29,877:INFO:Copying training dataset
2025-02-28 12:25:29,880:INFO:Defining folds
2025-02-28 12:25:29,880:INFO:Declaring metric variables
2025-02-28 12:25:29,883:INFO:Importing untrained model
2025-02-28 12:25:29,885:INFO:Dummy Regressor Imported successfully
2025-02-28 12:25:29,889:INFO:Starting cross validation
2025-02-28 12:25:29,890:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:29,991:INFO:Calculating mean and std
2025-02-28 12:25:29,992:INFO:Creating metrics dataframe
2025-02-28 12:25:29,993:INFO:Uploading results into container
2025-02-28 12:25:29,993:INFO:Uploading model into container now
2025-02-28 12:25:29,993:INFO:_master_model_container: 20
2025-02-28 12:25:29,993:INFO:_display_container: 2
2025-02-28 12:25:29,994:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:25:29,994:INFO:create_model() successfully completed......................................
2025-02-28 12:25:30,134:INFO:SubProcess create_model() end ==================================
2025-02-28 12:25:30,134:INFO:Creating metrics dataframe
2025-02-28 12:25:30,140:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:25:30,145:INFO:Initializing create_model()
2025-02-28 12:25:30,145:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:30,145:INFO:Checking exceptions
2025-02-28 12:25:30,146:INFO:Importing libraries
2025-02-28 12:25:30,146:INFO:Copying training dataset
2025-02-28 12:25:30,149:INFO:Defining folds
2025-02-28 12:25:30,149:INFO:Declaring metric variables
2025-02-28 12:25:30,149:INFO:Importing untrained model
2025-02-28 12:25:30,149:INFO:Declaring custom model
2025-02-28 12:25:30,149:INFO:Dummy Regressor Imported successfully
2025-02-28 12:25:30,150:INFO:Cross validation set to False
2025-02-28 12:25:30,150:INFO:Fitting Model
2025-02-28 12:25:30,184:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:25:30,184:INFO:create_model() successfully completed......................................
2025-02-28 12:25:30,344:INFO:_master_model_container: 20
2025-02-28 12:25:30,344:INFO:_display_container: 2
2025-02-28 12:25:30,344:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:25:30,344:INFO:compare_models() successfully completed......................................
2025-02-28 12:25:30,371:INFO:Initializing create_model()
2025-02-28 12:25:30,371:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:25:30,371:INFO:Checking exceptions
2025-02-28 12:25:30,379:INFO:Importing libraries
2025-02-28 12:25:30,379:INFO:Copying training dataset
2025-02-28 12:25:30,383:INFO:Defining folds
2025-02-28 12:25:30,383:INFO:Declaring metric variables
2025-02-28 12:25:30,385:INFO:Importing untrained model
2025-02-28 12:25:30,388:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:25:30,392:INFO:Starting cross validation
2025-02-28 12:25:30,394:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:25:30,658:INFO:Calculating mean and std
2025-02-28 12:25:30,658:INFO:Creating metrics dataframe
2025-02-28 12:25:30,661:INFO:Finalizing model
2025-02-28 12:25:30,786:INFO:Uploading results into container
2025-02-28 12:25:30,787:INFO:Uploading model into container now
2025-02-28 12:25:30,792:INFO:_master_model_container: 21
2025-02-28 12:25:30,794:INFO:_display_container: 3
2025-02-28 12:25:30,794:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:25:30,794:INFO:create_model() successfully completed......................................
2025-02-28 12:25:31,036:INFO:Initializing plot_model()
2025-02-28 12:25:31,036:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, system=True)
2025-02-28 12:25:31,036:INFO:Checking exceptions
2025-02-28 12:25:31,039:INFO:Preloading libraries
2025-02-28 12:25:31,047:INFO:Copying training dataset
2025-02-28 12:25:31,047:INFO:Plot type: residuals
2025-02-28 12:25:31,221:INFO:Fitting Model
2025-02-28 12:25:31,253:INFO:Scoring test/hold-out set
2025-02-28 12:25:31,471:INFO:Visual Rendered Successfully
2025-02-28 12:25:31,619:INFO:plot_model() successfully completed......................................
2025-02-28 12:25:31,674:INFO:Initializing plot_model()
2025-02-28 12:25:31,674:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, system=True)
2025-02-28 12:25:31,674:INFO:Checking exceptions
2025-02-28 12:25:31,678:INFO:Preloading libraries
2025-02-28 12:25:31,685:INFO:Copying training dataset
2025-02-28 12:25:31,685:INFO:Plot type: error
2025-02-28 12:25:31,846:INFO:Fitting Model
2025-02-28 12:25:31,846:INFO:Scoring test/hold-out set
2025-02-28 12:25:31,992:INFO:Visual Rendered Successfully
2025-02-28 12:25:32,133:INFO:plot_model() successfully completed......................................
2025-02-28 12:25:32,171:INFO:Initializing plot_model()
2025-02-28 12:25:32,171:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000024E5B9B0400>, system=True)
2025-02-28 12:25:32,171:INFO:Checking exceptions
2025-02-28 12:25:32,174:INFO:Preloading libraries
2025-02-28 12:25:32,180:INFO:Copying training dataset
2025-02-28 12:25:32,180:INFO:Plot type: feature
2025-02-28 12:25:32,181:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 12:25:32,312:INFO:Visual Rendered Successfully
2025-02-28 12:25:32,456:INFO:plot_model() successfully completed......................................
2025-02-28 12:42:41,103:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:42:41,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:42:41,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:42:41,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 12:42:42,488:INFO:PyCaret ClassificationExperiment
2025-02-28 12:42:42,488:INFO:Logging name: clf-default-name
2025-02-28 12:42:42,488:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 12:42:42,488:INFO:version 3.3.2
2025-02-28 12:42:42,488:INFO:Initializing setup()
2025-02-28 12:42:42,488:INFO:self.USI: e0e8
2025-02-28 12:42:42,488:INFO:self._variable_keys: {'gpu_param', 'target_param', 'exp_id', 'data', 'USI', 'gpu_n_jobs_param', 'fix_imbalance', '_ml_usecase', 'y_train', 'pipeline', '_available_plots', 'seed', 'X_train', 'fold_generator', 'is_multiclass', 'logging_param', 'X', 'fold_shuffle_param', 'html_param', 'idx', 'exp_name_log', 'memory', 'y', 'fold_groups_param', 'log_plots_param', 'y_test', 'X_test', 'n_jobs_param'}
2025-02-28 12:42:42,488:INFO:Checking environment
2025-02-28 12:42:42,488:INFO:python_version: 3.10.16
2025-02-28 12:42:42,488:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:42:42,488:INFO:machine: AMD64
2025-02-28 12:42:42,488:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:42:42,488:INFO:Memory: svmem(total=34200334336, available=18888056832, percent=44.8, used=15312277504, free=18888056832)
2025-02-28 12:42:42,488:INFO:Physical Core: 24
2025-02-28 12:42:42,488:INFO:Logical Core: 32
2025-02-28 12:42:42,488:INFO:Checking libraries
2025-02-28 12:42:42,488:INFO:System:
2025-02-28 12:42:42,488:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:42:42,488:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:42:42,488:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:42:42,488:INFO:PyCaret required dependencies:
2025-02-28 12:42:43,017:INFO:                 pip: 25.0
2025-02-28 12:42:43,017:INFO:          setuptools: 75.8.0
2025-02-28 12:42:43,017:INFO:             pycaret: 3.3.2
2025-02-28 12:42:43,017:INFO:             IPython: 8.30.0
2025-02-28 12:42:43,017:INFO:          ipywidgets: 8.1.5
2025-02-28 12:42:43,017:INFO:                tqdm: 4.67.1
2025-02-28 12:42:43,017:INFO:               numpy: 1.26.4
2025-02-28 12:42:43,017:INFO:              pandas: 2.1.4
2025-02-28 12:42:43,017:INFO:              jinja2: 3.1.5
2025-02-28 12:42:43,017:INFO:               scipy: 1.11.4
2025-02-28 12:42:43,017:INFO:              joblib: 1.3.2
2025-02-28 12:42:43,017:INFO:             sklearn: 1.4.2
2025-02-28 12:42:43,017:INFO:                pyod: 2.0.3
2025-02-28 12:42:43,017:INFO:            imblearn: 0.13.0
2025-02-28 12:42:43,017:INFO:   category_encoders: 2.7.0
2025-02-28 12:42:43,017:INFO:            lightgbm: 4.5.0
2025-02-28 12:42:43,017:INFO:               numba: 0.61.0
2025-02-28 12:42:43,017:INFO:            requests: 2.32.3
2025-02-28 12:42:43,017:INFO:          matplotlib: 3.7.5
2025-02-28 12:42:43,017:INFO:          scikitplot: 0.3.7
2025-02-28 12:42:43,017:INFO:         yellowbrick: 1.5
2025-02-28 12:42:43,017:INFO:              plotly: 5.24.1
2025-02-28 12:42:43,017:INFO:    plotly-resampler: Not installed
2025-02-28 12:42:43,017:INFO:             kaleido: 0.2.1
2025-02-28 12:42:43,017:INFO:           schemdraw: 0.15
2025-02-28 12:42:43,017:INFO:         statsmodels: 0.14.4
2025-02-28 12:42:43,017:INFO:              sktime: 0.26.0
2025-02-28 12:42:43,017:INFO:               tbats: 1.1.3
2025-02-28 12:42:43,018:INFO:            pmdarima: 2.0.4
2025-02-28 12:42:43,018:INFO:              psutil: 5.9.0
2025-02-28 12:42:43,018:INFO:          markupsafe: 2.1.5
2025-02-28 12:42:43,018:INFO:             pickle5: Not installed
2025-02-28 12:42:43,018:INFO:         cloudpickle: 3.1.1
2025-02-28 12:42:43,018:INFO:         deprecation: 2.1.0
2025-02-28 12:42:43,018:INFO:              xxhash: 3.5.0
2025-02-28 12:42:43,018:INFO:           wurlitzer: Not installed
2025-02-28 12:42:43,018:INFO:PyCaret optional dependencies:
2025-02-28 12:42:44,785:INFO:                shap: 0.44.1
2025-02-28 12:42:44,785:INFO:           interpret: 0.6.9
2025-02-28 12:42:44,785:INFO:                umap: 0.5.7
2025-02-28 12:42:44,785:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:42:44,785:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:42:44,785:INFO:             autoviz: Not installed
2025-02-28 12:42:44,785:INFO:           fairlearn: 0.7.0
2025-02-28 12:42:44,785:INFO:          deepchecks: Not installed
2025-02-28 12:42:44,785:INFO:             xgboost: 2.1.4
2025-02-28 12:42:44,785:INFO:            catboost: 1.2.7
2025-02-28 12:42:44,785:INFO:              kmodes: 0.12.2
2025-02-28 12:42:44,785:INFO:             mlxtend: 0.23.4
2025-02-28 12:42:44,785:INFO:       statsforecast: 1.5.0
2025-02-28 12:42:44,785:INFO:        tune_sklearn: Not installed
2025-02-28 12:42:44,785:INFO:                 ray: Not installed
2025-02-28 12:42:44,785:INFO:            hyperopt: 0.2.7
2025-02-28 12:42:44,785:INFO:              optuna: 4.2.0
2025-02-28 12:42:44,785:INFO:               skopt: 0.10.2
2025-02-28 12:42:44,785:INFO:              mlflow: 2.20.1
2025-02-28 12:42:44,785:INFO:              gradio: 5.15.0
2025-02-28 12:42:44,785:INFO:             fastapi: 0.115.8
2025-02-28 12:42:44,785:INFO:             uvicorn: 0.34.0
2025-02-28 12:42:44,785:INFO:              m2cgen: 0.10.0
2025-02-28 12:42:44,785:INFO:           evidently: 0.4.40
2025-02-28 12:42:44,785:INFO:               fugue: 0.8.7
2025-02-28 12:42:44,785:INFO:           streamlit: Not installed
2025-02-28 12:42:44,785:INFO:             prophet: Not installed
2025-02-28 12:42:44,785:INFO:None
2025-02-28 12:42:44,785:INFO:Set up data.
2025-02-28 12:42:44,788:INFO:Set up folding strategy.
2025-02-28 12:42:44,788:INFO:Set up train/test split.
2025-02-28 12:42:44,792:INFO:Set up index.
2025-02-28 12:42:44,793:INFO:Assigning column types.
2025-02-28 12:42:44,795:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:42:44,814:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,815:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,833:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,834:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:44,865:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,866:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,879:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,880:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:44,880:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:42:44,901:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,914:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,915:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:44,937:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 12:42:44,948:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,949:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:44,950:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 12:42:44,982:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:44,984:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:45,016:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:45,018:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:45,019:INFO:Preparing preprocessing pipeline...
2025-02-28 12:42:45,019:INFO:Set up simple imputation.
2025-02-28 12:42:45,021:INFO:Set up encoding of categorical features.
2025-02-28 12:42:45,062:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:42:45,065:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:42:45,065:INFO:Creating final display dataframe.
2025-02-28 12:42:45,168:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              e0e8
2025-02-28 12:42:45,206:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:45,208:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:45,241:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:42:45,243:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:42:45,243:INFO:setup() successfully completed in 2.76s...............
2025-02-28 12:42:45,267:INFO:Initializing compare_models()
2025-02-28 12:42:45,267:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 12:42:45,267:INFO:Checking exceptions
2025-02-28 12:42:45,271:INFO:Preparing display monitor
2025-02-28 12:42:45,287:INFO:Initializing Logistic Regression
2025-02-28 12:42:45,287:INFO:Total runtime is 0.0 minutes
2025-02-28 12:42:45,291:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:45,292:INFO:Initializing create_model()
2025-02-28 12:42:45,292:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:45,292:INFO:Checking exceptions
2025-02-28 12:42:45,292:INFO:Importing libraries
2025-02-28 12:42:45,292:INFO:Copying training dataset
2025-02-28 12:42:45,294:INFO:Defining folds
2025-02-28 12:42:45,295:INFO:Declaring metric variables
2025-02-28 12:42:45,297:INFO:Importing untrained model
2025-02-28 12:42:45,299:INFO:Logistic Regression Imported successfully
2025-02-28 12:42:45,303:INFO:Starting cross validation
2025-02-28 12:42:45,304:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:48,749:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,752:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,758:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,767:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,771:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,772:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,773:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,787:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,788:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,791:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,802:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,804:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,812:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,815:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,821:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,837:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,840:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,854:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,861:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 12:42:48,873:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:48,894:INFO:Calculating mean and std
2025-02-28 12:42:48,895:INFO:Creating metrics dataframe
2025-02-28 12:42:48,895:INFO:Uploading results into container
2025-02-28 12:42:48,897:INFO:Uploading model into container now
2025-02-28 12:42:48,897:INFO:_master_model_container: 1
2025-02-28 12:42:48,897:INFO:_display_container: 2
2025-02-28 12:42:48,898:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 12:42:48,898:INFO:create_model() successfully completed......................................
2025-02-28 12:42:48,973:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:48,973:INFO:Creating metrics dataframe
2025-02-28 12:42:48,977:INFO:Initializing K Neighbors Classifier
2025-02-28 12:42:48,977:INFO:Total runtime is 0.06151277621587117 minutes
2025-02-28 12:42:48,979:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:48,980:INFO:Initializing create_model()
2025-02-28 12:42:48,980:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:48,980:INFO:Checking exceptions
2025-02-28 12:42:48,980:INFO:Importing libraries
2025-02-28 12:42:48,980:INFO:Copying training dataset
2025-02-28 12:42:48,982:INFO:Defining folds
2025-02-28 12:42:48,982:INFO:Declaring metric variables
2025-02-28 12:42:48,984:INFO:Importing untrained model
2025-02-28 12:42:48,986:INFO:K Neighbors Classifier Imported successfully
2025-02-28 12:42:48,990:INFO:Starting cross validation
2025-02-28 12:42:48,992:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:51,761:INFO:Calculating mean and std
2025-02-28 12:42:51,762:INFO:Creating metrics dataframe
2025-02-28 12:42:51,764:INFO:Uploading results into container
2025-02-28 12:42:51,765:INFO:Uploading model into container now
2025-02-28 12:42:51,765:INFO:_master_model_container: 2
2025-02-28 12:42:51,765:INFO:_display_container: 2
2025-02-28 12:42:51,765:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 12:42:51,766:INFO:create_model() successfully completed......................................
2025-02-28 12:42:51,855:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:51,855:INFO:Creating metrics dataframe
2025-02-28 12:42:51,859:INFO:Initializing Naive Bayes
2025-02-28 12:42:51,859:INFO:Total runtime is 0.10954153935114543 minutes
2025-02-28 12:42:51,861:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:51,861:INFO:Initializing create_model()
2025-02-28 12:42:51,861:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:51,861:INFO:Checking exceptions
2025-02-28 12:42:51,862:INFO:Importing libraries
2025-02-28 12:42:51,862:INFO:Copying training dataset
2025-02-28 12:42:51,864:INFO:Defining folds
2025-02-28 12:42:51,864:INFO:Declaring metric variables
2025-02-28 12:42:51,866:INFO:Importing untrained model
2025-02-28 12:42:51,869:INFO:Naive Bayes Imported successfully
2025-02-28 12:42:51,872:INFO:Starting cross validation
2025-02-28 12:42:51,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:54,660:INFO:Calculating mean and std
2025-02-28 12:42:54,661:INFO:Creating metrics dataframe
2025-02-28 12:42:54,663:INFO:Uploading results into container
2025-02-28 12:42:54,664:INFO:Uploading model into container now
2025-02-28 12:42:54,664:INFO:_master_model_container: 3
2025-02-28 12:42:54,664:INFO:_display_container: 2
2025-02-28 12:42:54,665:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 12:42:54,665:INFO:create_model() successfully completed......................................
2025-02-28 12:42:54,745:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:54,745:INFO:Creating metrics dataframe
2025-02-28 12:42:54,748:INFO:Initializing Decision Tree Classifier
2025-02-28 12:42:54,748:INFO:Total runtime is 0.15769506692886354 minutes
2025-02-28 12:42:54,750:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:54,751:INFO:Initializing create_model()
2025-02-28 12:42:54,751:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:54,751:INFO:Checking exceptions
2025-02-28 12:42:54,751:INFO:Importing libraries
2025-02-28 12:42:54,751:INFO:Copying training dataset
2025-02-28 12:42:54,753:INFO:Defining folds
2025-02-28 12:42:54,753:INFO:Declaring metric variables
2025-02-28 12:42:54,755:INFO:Importing untrained model
2025-02-28 12:42:54,758:INFO:Decision Tree Classifier Imported successfully
2025-02-28 12:42:54,763:INFO:Starting cross validation
2025-02-28 12:42:54,763:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:56,785:INFO:Calculating mean and std
2025-02-28 12:42:56,785:INFO:Creating metrics dataframe
2025-02-28 12:42:56,787:INFO:Uploading results into container
2025-02-28 12:42:56,788:INFO:Uploading model into container now
2025-02-28 12:42:56,788:INFO:_master_model_container: 4
2025-02-28 12:42:56,788:INFO:_display_container: 2
2025-02-28 12:42:56,788:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:42:56,788:INFO:create_model() successfully completed......................................
2025-02-28 12:42:56,870:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:56,870:INFO:Creating metrics dataframe
2025-02-28 12:42:56,874:INFO:Initializing SVM - Linear Kernel
2025-02-28 12:42:56,874:INFO:Total runtime is 0.1931254545847575 minutes
2025-02-28 12:42:56,877:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:56,877:INFO:Initializing create_model()
2025-02-28 12:42:56,877:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:56,877:INFO:Checking exceptions
2025-02-28 12:42:56,877:INFO:Importing libraries
2025-02-28 12:42:56,877:INFO:Copying training dataset
2025-02-28 12:42:56,880:INFO:Defining folds
2025-02-28 12:42:56,880:INFO:Declaring metric variables
2025-02-28 12:42:56,882:INFO:Importing untrained model
2025-02-28 12:42:56,885:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 12:42:56,889:INFO:Starting cross validation
2025-02-28 12:42:56,890:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:57,007:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,008:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,010:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,010:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,010:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,011:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,015:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,015:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,021:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,023:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,024:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,025:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,035:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,035:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,037:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,037:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,050:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,053:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,053:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,066:INFO:Calculating mean and std
2025-02-28 12:42:57,067:INFO:Creating metrics dataframe
2025-02-28 12:42:57,068:INFO:Uploading results into container
2025-02-28 12:42:57,068:INFO:Uploading model into container now
2025-02-28 12:42:57,069:INFO:_master_model_container: 5
2025-02-28 12:42:57,069:INFO:_display_container: 2
2025-02-28 12:42:57,069:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:42:57,069:INFO:create_model() successfully completed......................................
2025-02-28 12:42:57,145:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:57,146:INFO:Creating metrics dataframe
2025-02-28 12:42:57,149:INFO:Initializing Ridge Classifier
2025-02-28 12:42:57,150:INFO:Total runtime is 0.19771718978881836 minutes
2025-02-28 12:42:57,152:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:57,152:INFO:Initializing create_model()
2025-02-28 12:42:57,152:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:57,152:INFO:Checking exceptions
2025-02-28 12:42:57,152:INFO:Importing libraries
2025-02-28 12:42:57,152:INFO:Copying training dataset
2025-02-28 12:42:57,154:INFO:Defining folds
2025-02-28 12:42:57,155:INFO:Declaring metric variables
2025-02-28 12:42:57,157:INFO:Importing untrained model
2025-02-28 12:42:57,158:INFO:Ridge Classifier Imported successfully
2025-02-28 12:42:57,163:INFO:Starting cross validation
2025-02-28 12:42:57,164:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:57,211:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,212:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,216:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,216:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,219:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,220:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,222:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,226:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,229:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,231:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,238:INFO:Calculating mean and std
2025-02-28 12:42:57,238:INFO:Creating metrics dataframe
2025-02-28 12:42:57,240:INFO:Uploading results into container
2025-02-28 12:42:57,240:INFO:Uploading model into container now
2025-02-28 12:42:57,240:INFO:_master_model_container: 6
2025-02-28 12:42:57,240:INFO:_display_container: 2
2025-02-28 12:42:57,240:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 12:42:57,241:INFO:create_model() successfully completed......................................
2025-02-28 12:42:57,320:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:57,320:INFO:Creating metrics dataframe
2025-02-28 12:42:57,325:INFO:Initializing Random Forest Classifier
2025-02-28 12:42:57,325:INFO:Total runtime is 0.20063705841700236 minutes
2025-02-28 12:42:57,326:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:57,326:INFO:Initializing create_model()
2025-02-28 12:42:57,326:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:57,327:INFO:Checking exceptions
2025-02-28 12:42:57,327:INFO:Importing libraries
2025-02-28 12:42:57,327:INFO:Copying training dataset
2025-02-28 12:42:57,329:INFO:Defining folds
2025-02-28 12:42:57,329:INFO:Declaring metric variables
2025-02-28 12:42:57,331:INFO:Importing untrained model
2025-02-28 12:42:57,333:INFO:Random Forest Classifier Imported successfully
2025-02-28 12:42:57,337:INFO:Starting cross validation
2025-02-28 12:42:57,337:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:57,721:INFO:Calculating mean and std
2025-02-28 12:42:57,722:INFO:Creating metrics dataframe
2025-02-28 12:42:57,723:INFO:Uploading results into container
2025-02-28 12:42:57,723:INFO:Uploading model into container now
2025-02-28 12:42:57,723:INFO:_master_model_container: 7
2025-02-28 12:42:57,723:INFO:_display_container: 2
2025-02-28 12:42:57,724:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 12:42:57,724:INFO:create_model() successfully completed......................................
2025-02-28 12:42:57,799:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:57,799:INFO:Creating metrics dataframe
2025-02-28 12:42:57,804:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 12:42:57,804:INFO:Total runtime is 0.2086216648419698 minutes
2025-02-28 12:42:57,805:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:57,805:INFO:Initializing create_model()
2025-02-28 12:42:57,807:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:57,807:INFO:Checking exceptions
2025-02-28 12:42:57,807:INFO:Importing libraries
2025-02-28 12:42:57,807:INFO:Copying training dataset
2025-02-28 12:42:57,809:INFO:Defining folds
2025-02-28 12:42:57,809:INFO:Declaring metric variables
2025-02-28 12:42:57,811:INFO:Importing untrained model
2025-02-28 12:42:57,813:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 12:42:57,817:INFO:Starting cross validation
2025-02-28 12:42:57,819:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:57,852:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,854:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,858:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,859:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,861:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,863:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,864:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,865:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,865:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,868:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,870:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,873:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 12:42:57,873:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,874:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,874:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,876:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,876:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

at the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,877:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,877:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,878:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,880:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,883:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,885:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,885:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:57,889:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:42:57,907:INFO:Calculating mean and std
2025-02-28 12:42:57,908:INFO:Creating metrics dataframe
2025-02-28 12:42:57,909:INFO:Uploading results into container
2025-02-28 12:42:57,909:INFO:Uploading model into container now
2025-02-28 12:42:57,909:INFO:_master_model_container: 8
2025-02-28 12:42:57,909:INFO:_display_container: 2
2025-02-28 12:42:57,910:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 12:42:57,910:INFO:create_model() successfully completed......................................
2025-02-28 12:42:57,986:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:57,986:INFO:Creating metrics dataframe
2025-02-28 12:42:57,990:INFO:Initializing Ada Boost Classifier
2025-02-28 12:42:57,991:INFO:Total runtime is 0.2117407520612081 minutes
2025-02-28 12:42:57,993:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:57,993:INFO:Initializing create_model()
2025-02-28 12:42:57,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:57,993:INFO:Checking exceptions
2025-02-28 12:42:57,993:INFO:Importing libraries
2025-02-28 12:42:57,993:INFO:Copying training dataset
2025-02-28 12:42:57,995:INFO:Defining folds
2025-02-28 12:42:57,995:INFO:Declaring metric variables
2025-02-28 12:42:57,998:INFO:Importing untrained model
2025-02-28 12:42:58,000:INFO:Ada Boost Classifier Imported successfully
2025-02-28 12:42:58,004:INFO:Starting cross validation
2025-02-28 12:42:58,005:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:42:58,037:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,039:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,040:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,042:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,043:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,045:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,046:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,054:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,055:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,055:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 12:42:58,175:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,183:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,187:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,195:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,200:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,200:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,201:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,202:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,207:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,208:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:42:58,218:INFO:Calculating mean and std
2025-02-28 12:42:58,218:INFO:Creating metrics dataframe
2025-02-28 12:42:58,219:INFO:Uploading results into container
2025-02-28 12:42:58,220:INFO:Uploading model into container now
2025-02-28 12:42:58,220:INFO:_master_model_container: 9
2025-02-28 12:42:58,220:INFO:_display_container: 2
2025-02-28 12:42:58,220:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 12:42:58,221:INFO:create_model() successfully completed......................................
2025-02-28 12:42:58,294:INFO:SubProcess create_model() end ==================================
2025-02-28 12:42:58,294:INFO:Creating metrics dataframe
2025-02-28 12:42:58,300:INFO:Initializing Gradient Boosting Classifier
2025-02-28 12:42:58,300:INFO:Total runtime is 0.21688273350397747 minutes
2025-02-28 12:42:58,302:INFO:SubProcess create_model() called ==================================
2025-02-28 12:42:58,302:INFO:Initializing create_model()
2025-02-28 12:42:58,302:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:42:58,302:INFO:Checking exceptions
2025-02-28 12:42:58,302:INFO:Importing libraries
2025-02-28 12:42:58,302:INFO:Copying training dataset
2025-02-28 12:42:58,305:INFO:Defining folds
2025-02-28 12:42:58,305:INFO:Declaring metric variables
2025-02-28 12:42:58,307:INFO:Importing untrained model
2025-02-28 12:42:58,309:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 12:42:58,313:INFO:Starting cross validation
2025-02-28 12:42:58,314:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:00,356:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,434:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,444:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,459:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,472:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,485:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,512:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,527:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,530:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,531:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,549:INFO:Calculating mean and std
2025-02-28 12:43:00,549:INFO:Creating metrics dataframe
2025-02-28 12:43:00,551:INFO:Uploading results into container
2025-02-28 12:43:00,551:INFO:Uploading model into container now
2025-02-28 12:43:00,551:INFO:_master_model_container: 10
2025-02-28 12:43:00,551:INFO:_display_container: 2
2025-02-28 12:43:00,552:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:43:00,552:INFO:create_model() successfully completed......................................
2025-02-28 12:43:00,625:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:00,627:INFO:Creating metrics dataframe
2025-02-28 12:43:00,632:INFO:Initializing Linear Discriminant Analysis
2025-02-28 12:43:00,632:INFO:Total runtime is 0.25574666261672974 minutes
2025-02-28 12:43:00,633:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:00,633:INFO:Initializing create_model()
2025-02-28 12:43:00,633:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:00,634:INFO:Checking exceptions
2025-02-28 12:43:00,634:INFO:Importing libraries
2025-02-28 12:43:00,634:INFO:Copying training dataset
2025-02-28 12:43:00,637:INFO:Defining folds
2025-02-28 12:43:00,637:INFO:Declaring metric variables
2025-02-28 12:43:00,638:INFO:Importing untrained model
2025-02-28 12:43:00,641:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 12:43:00,645:INFO:Starting cross validation
2025-02-28 12:43:00,646:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:00,694:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,695:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,698:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,699:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,704:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,705:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,706:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,708:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,710:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,711:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 12:43:00,722:INFO:Calculating mean and std
2025-02-28 12:43:00,723:INFO:Creating metrics dataframe
2025-02-28 12:43:00,724:INFO:Uploading results into container
2025-02-28 12:43:00,724:INFO:Uploading model into container now
2025-02-28 12:43:00,724:INFO:_master_model_container: 11
2025-02-28 12:43:00,724:INFO:_display_container: 2
2025-02-28 12:43:00,725:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 12:43:00,725:INFO:create_model() successfully completed......................................
2025-02-28 12:43:00,798:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:00,798:INFO:Creating metrics dataframe
2025-02-28 12:43:00,803:INFO:Initializing Extra Trees Classifier
2025-02-28 12:43:00,803:INFO:Total runtime is 0.25859919786453245 minutes
2025-02-28 12:43:00,805:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:00,805:INFO:Initializing create_model()
2025-02-28 12:43:00,805:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:00,805:INFO:Checking exceptions
2025-02-28 12:43:00,805:INFO:Importing libraries
2025-02-28 12:43:00,805:INFO:Copying training dataset
2025-02-28 12:43:00,808:INFO:Defining folds
2025-02-28 12:43:00,808:INFO:Declaring metric variables
2025-02-28 12:43:00,810:INFO:Importing untrained model
2025-02-28 12:43:00,812:INFO:Extra Trees Classifier Imported successfully
2025-02-28 12:43:00,816:INFO:Starting cross validation
2025-02-28 12:43:00,817:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:01,175:INFO:Calculating mean and std
2025-02-28 12:43:01,175:INFO:Creating metrics dataframe
2025-02-28 12:43:01,177:INFO:Uploading results into container
2025-02-28 12:43:01,177:INFO:Uploading model into container now
2025-02-28 12:43:01,178:INFO:_master_model_container: 12
2025-02-28 12:43:01,178:INFO:_display_container: 2
2025-02-28 12:43:01,178:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 12:43:01,178:INFO:create_model() successfully completed......................................
2025-02-28 12:43:01,254:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:01,254:INFO:Creating metrics dataframe
2025-02-28 12:43:01,259:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:43:01,259:INFO:Total runtime is 0.26621123552322384 minutes
2025-02-28 12:43:01,261:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:01,262:INFO:Initializing create_model()
2025-02-28 12:43:01,262:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:01,262:INFO:Checking exceptions
2025-02-28 12:43:01,262:INFO:Importing libraries
2025-02-28 12:43:01,262:INFO:Copying training dataset
2025-02-28 12:43:01,264:INFO:Defining folds
2025-02-28 12:43:01,264:INFO:Declaring metric variables
2025-02-28 12:43:01,266:INFO:Importing untrained model
2025-02-28 12:43:01,269:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:43:01,273:INFO:Starting cross validation
2025-02-28 12:43:01,274:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:02,001:INFO:Calculating mean and std
2025-02-28 12:43:02,002:INFO:Creating metrics dataframe
2025-02-28 12:43:02,003:INFO:Uploading results into container
2025-02-28 12:43:02,003:INFO:Uploading model into container now
2025-02-28 12:43:02,004:INFO:_master_model_container: 13
2025-02-28 12:43:02,004:INFO:_display_container: 2
2025-02-28 12:43:02,004:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 12:43:02,004:INFO:create_model() successfully completed......................................
2025-02-28 12:43:02,079:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:02,079:INFO:Creating metrics dataframe
2025-02-28 12:43:02,084:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:43:02,084:INFO:Total runtime is 0.27995494604110716 minutes
2025-02-28 12:43:02,087:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:02,087:INFO:Initializing create_model()
2025-02-28 12:43:02,087:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:02,087:INFO:Checking exceptions
2025-02-28 12:43:02,087:INFO:Importing libraries
2025-02-28 12:43:02,087:INFO:Copying training dataset
2025-02-28 12:43:02,090:INFO:Defining folds
2025-02-28 12:43:02,090:INFO:Declaring metric variables
2025-02-28 12:43:02,092:INFO:Importing untrained model
2025-02-28 12:43:02,094:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:43:02,097:INFO:Starting cross validation
2025-02-28 12:43:02,098:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:10,135:INFO:Calculating mean and std
2025-02-28 12:43:10,136:INFO:Creating metrics dataframe
2025-02-28 12:43:10,137:INFO:Uploading results into container
2025-02-28 12:43:10,137:INFO:Uploading model into container now
2025-02-28 12:43:10,138:INFO:_master_model_container: 14
2025-02-28 12:43:10,138:INFO:_display_container: 2
2025-02-28 12:43:10,138:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:43:10,138:INFO:create_model() successfully completed......................................
2025-02-28 12:43:10,232:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:10,232:INFO:Creating metrics dataframe
2025-02-28 12:43:10,237:INFO:Initializing CatBoost Classifier
2025-02-28 12:43:10,237:INFO:Total runtime is 0.4158382574717203 minutes
2025-02-28 12:43:10,239:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:10,239:INFO:Initializing create_model()
2025-02-28 12:43:10,239:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:10,239:INFO:Checking exceptions
2025-02-28 12:43:10,239:INFO:Importing libraries
2025-02-28 12:43:10,239:INFO:Copying training dataset
2025-02-28 12:43:10,242:INFO:Defining folds
2025-02-28 12:43:10,242:INFO:Declaring metric variables
2025-02-28 12:43:10,245:INFO:Importing untrained model
2025-02-28 12:43:10,247:INFO:CatBoost Classifier Imported successfully
2025-02-28 12:43:10,250:INFO:Starting cross validation
2025-02-28 12:43:10,251:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:17,197:INFO:Calculating mean and std
2025-02-28 12:43:17,197:INFO:Creating metrics dataframe
2025-02-28 12:43:17,199:INFO:Uploading results into container
2025-02-28 12:43:17,199:INFO:Uploading model into container now
2025-02-28 12:43:17,199:INFO:_master_model_container: 15
2025-02-28 12:43:17,200:INFO:_display_container: 2
2025-02-28 12:43:17,200:INFO:<catboost.core.CatBoostClassifier object at 0x000002188D05E200>
2025-02-28 12:43:17,200:INFO:create_model() successfully completed......................................
2025-02-28 12:43:17,277:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:17,277:INFO:Creating metrics dataframe
2025-02-28 12:43:17,282:INFO:Initializing Dummy Classifier
2025-02-28 12:43:17,283:INFO:Total runtime is 0.5332764546076456 minutes
2025-02-28 12:43:17,285:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:17,285:INFO:Initializing create_model()
2025-02-28 12:43:17,285:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021880E53490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:17,285:INFO:Checking exceptions
2025-02-28 12:43:17,285:INFO:Importing libraries
2025-02-28 12:43:17,285:INFO:Copying training dataset
2025-02-28 12:43:17,288:INFO:Defining folds
2025-02-28 12:43:17,288:INFO:Declaring metric variables
2025-02-28 12:43:17,290:INFO:Importing untrained model
2025-02-28 12:43:17,292:INFO:Dummy Classifier Imported successfully
2025-02-28 12:43:17,296:INFO:Starting cross validation
2025-02-28 12:43:17,297:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:17,344:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,345:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,349:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,350:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,350:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,351:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,353:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,354:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,363:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,365:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 12:43:17,368:INFO:Calculating mean and std
2025-02-28 12:43:17,368:INFO:Creating metrics dataframe
2025-02-28 12:43:17,370:INFO:Uploading results into container
2025-02-28 12:43:17,370:INFO:Uploading model into container now
2025-02-28 12:43:17,370:INFO:_master_model_container: 16
2025-02-28 12:43:17,370:INFO:_display_container: 2
2025-02-28 12:43:17,371:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 12:43:17,371:INFO:create_model() successfully completed......................................
2025-02-28 12:43:17,448:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:17,448:INFO:Creating metrics dataframe
2025-02-28 12:43:17,454:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:43:17,460:INFO:Initializing create_model()
2025-02-28 12:43:17,460:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:17,460:INFO:Checking exceptions
2025-02-28 12:43:17,461:INFO:Importing libraries
2025-02-28 12:43:17,461:INFO:Copying training dataset
2025-02-28 12:43:17,463:INFO:Defining folds
2025-02-28 12:43:17,463:INFO:Declaring metric variables
2025-02-28 12:43:17,463:INFO:Importing untrained model
2025-02-28 12:43:17,463:INFO:Declaring custom model
2025-02-28 12:43:17,464:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:43:17,464:INFO:Cross validation set to False
2025-02-28 12:43:17,464:INFO:Fitting Model
2025-02-28 12:43:17,493:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 12:43:17,493:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000185 seconds.
2025-02-28 12:43:17,494:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-28 12:43:17,494:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 12:43:17,494:INFO:[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 20
2025-02-28 12:43:17,494:INFO:[LightGBM] [Info] Start training from score -1.773639
2025-02-28 12:43:17,494:INFO:[LightGBM] [Info] Start training from score -1.804411
2025-02-28 12:43:17,495:INFO:[LightGBM] [Info] Start training from score -1.765257
2025-02-28 12:43:17,495:INFO:[LightGBM] [Info] Start training from score -1.800944
2025-02-28 12:43:17,495:INFO:[LightGBM] [Info] Start training from score -1.827239
2025-02-28 12:43:17,495:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 12:43:17,857:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:43:17,857:INFO:create_model() successfully completed......................................
2025-02-28 12:43:17,993:INFO:_master_model_container: 16
2025-02-28 12:43:17,993:INFO:_display_container: 2
2025-02-28 12:43:17,994:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:43:17,994:INFO:compare_models() successfully completed......................................
2025-02-28 12:43:18,014:INFO:Initializing plot_model()
2025-02-28 12:43:18,014:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, system=True)
2025-02-28 12:43:18,014:INFO:Checking exceptions
2025-02-28 12:43:18,016:INFO:Preloading libraries
2025-02-28 12:43:18,061:INFO:Copying training dataset
2025-02-28 12:43:18,061:INFO:Plot type: confusion_matrix
2025-02-28 12:43:18,183:INFO:Fitting Model
2025-02-28 12:43:18,184:INFO:Scoring test/hold-out set
2025-02-28 12:43:18,319:INFO:Visual Rendered Successfully
2025-02-28 12:43:18,397:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:18,418:INFO:Initializing plot_model()
2025-02-28 12:43:18,418:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021880E53B80>, system=True)
2025-02-28 12:43:18,418:INFO:Checking exceptions
2025-02-28 12:43:18,421:INFO:Preloading libraries
2025-02-28 12:43:18,464:INFO:Copying training dataset
2025-02-28 12:43:18,464:INFO:Plot type: auc
2025-02-28 12:43:18,589:INFO:Fitting Model
2025-02-28 12:43:18,589:INFO:Scoring test/hold-out set
2025-02-28 12:43:18,733:INFO:Visual Rendered Successfully
2025-02-28 12:43:18,812:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:18,845:INFO:PyCaret RegressionExperiment
2025-02-28 12:43:18,845:INFO:Logging name: reg-default-name
2025-02-28 12:43:18,845:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 12:43:18,845:INFO:version 3.3.2
2025-02-28 12:43:18,845:INFO:Initializing setup()
2025-02-28 12:43:18,845:INFO:self.USI: 9222
2025-02-28 12:43:18,845:INFO:self._variable_keys: {'transform_target_param', 'gpu_param', 'target_param', 'exp_id', 'data', 'USI', 'gpu_n_jobs_param', '_ml_usecase', 'y_train', 'pipeline', '_available_plots', 'seed', 'X_train', 'fold_generator', 'logging_param', 'X', 'fold_shuffle_param', 'html_param', 'idx', 'exp_name_log', 'memory', 'y', 'fold_groups_param', 'log_plots_param', 'y_test', 'X_test', 'n_jobs_param'}
2025-02-28 12:43:18,845:INFO:Checking environment
2025-02-28 12:43:18,846:INFO:python_version: 3.10.16
2025-02-28 12:43:18,846:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 12:43:18,846:INFO:machine: AMD64
2025-02-28 12:43:18,846:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 12:43:18,846:INFO:Memory: svmem(total=34200334336, available=11794411520, percent=65.5, used=22405922816, free=11794411520)
2025-02-28 12:43:18,846:INFO:Physical Core: 24
2025-02-28 12:43:18,846:INFO:Logical Core: 32
2025-02-28 12:43:18,846:INFO:Checking libraries
2025-02-28 12:43:18,846:INFO:System:
2025-02-28 12:43:18,846:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 12:43:18,846:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 12:43:18,846:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 12:43:18,846:INFO:PyCaret required dependencies:
2025-02-28 12:43:18,846:INFO:                 pip: 25.0
2025-02-28 12:43:18,846:INFO:          setuptools: 75.8.0
2025-02-28 12:43:18,846:INFO:             pycaret: 3.3.2
2025-02-28 12:43:18,846:INFO:             IPython: 8.30.0
2025-02-28 12:43:18,846:INFO:          ipywidgets: 8.1.5
2025-02-28 12:43:18,846:INFO:                tqdm: 4.67.1
2025-02-28 12:43:18,846:INFO:               numpy: 1.26.4
2025-02-28 12:43:18,846:INFO:              pandas: 2.1.4
2025-02-28 12:43:18,847:INFO:              jinja2: 3.1.5
2025-02-28 12:43:18,847:INFO:               scipy: 1.11.4
2025-02-28 12:43:18,847:INFO:              joblib: 1.3.2
2025-02-28 12:43:18,847:INFO:             sklearn: 1.4.2
2025-02-28 12:43:18,847:INFO:                pyod: 2.0.3
2025-02-28 12:43:18,847:INFO:            imblearn: 0.13.0
2025-02-28 12:43:18,847:INFO:   category_encoders: 2.7.0
2025-02-28 12:43:18,847:INFO:            lightgbm: 4.5.0
2025-02-28 12:43:18,847:INFO:               numba: 0.61.0
2025-02-28 12:43:18,847:INFO:            requests: 2.32.3
2025-02-28 12:43:18,847:INFO:          matplotlib: 3.7.5
2025-02-28 12:43:18,847:INFO:          scikitplot: 0.3.7
2025-02-28 12:43:18,847:INFO:         yellowbrick: 1.5
2025-02-28 12:43:18,847:INFO:              plotly: 5.24.1
2025-02-28 12:43:18,847:INFO:    plotly-resampler: Not installed
2025-02-28 12:43:18,847:INFO:             kaleido: 0.2.1
2025-02-28 12:43:18,847:INFO:           schemdraw: 0.15
2025-02-28 12:43:18,847:INFO:         statsmodels: 0.14.4
2025-02-28 12:43:18,847:INFO:              sktime: 0.26.0
2025-02-28 12:43:18,847:INFO:               tbats: 1.1.3
2025-02-28 12:43:18,847:INFO:            pmdarima: 2.0.4
2025-02-28 12:43:18,847:INFO:              psutil: 5.9.0
2025-02-28 12:43:18,847:INFO:          markupsafe: 2.1.5
2025-02-28 12:43:18,847:INFO:             pickle5: Not installed
2025-02-28 12:43:18,847:INFO:         cloudpickle: 3.1.1
2025-02-28 12:43:18,847:INFO:         deprecation: 2.1.0
2025-02-28 12:43:18,847:INFO:              xxhash: 3.5.0
2025-02-28 12:43:18,847:INFO:           wurlitzer: Not installed
2025-02-28 12:43:18,847:INFO:PyCaret optional dependencies:
2025-02-28 12:43:18,847:INFO:                shap: 0.44.1
2025-02-28 12:43:18,848:INFO:           interpret: 0.6.9
2025-02-28 12:43:18,848:INFO:                umap: 0.5.7
2025-02-28 12:43:18,848:INFO:     ydata_profiling: 4.12.2
2025-02-28 12:43:18,848:INFO:  explainerdashboard: 0.4.8
2025-02-28 12:43:18,848:INFO:             autoviz: Not installed
2025-02-28 12:43:18,848:INFO:           fairlearn: 0.7.0
2025-02-28 12:43:18,848:INFO:          deepchecks: Not installed
2025-02-28 12:43:18,848:INFO:             xgboost: 2.1.4
2025-02-28 12:43:18,848:INFO:            catboost: 1.2.7
2025-02-28 12:43:18,848:INFO:              kmodes: 0.12.2
2025-02-28 12:43:18,848:INFO:             mlxtend: 0.23.4
2025-02-28 12:43:18,848:INFO:       statsforecast: 1.5.0
2025-02-28 12:43:18,848:INFO:        tune_sklearn: Not installed
2025-02-28 12:43:18,848:INFO:                 ray: Not installed
2025-02-28 12:43:18,848:INFO:            hyperopt: 0.2.7
2025-02-28 12:43:18,848:INFO:              optuna: 4.2.0
2025-02-28 12:43:18,848:INFO:               skopt: 0.10.2
2025-02-28 12:43:18,848:INFO:              mlflow: 2.20.1
2025-02-28 12:43:18,848:INFO:              gradio: 5.15.0
2025-02-28 12:43:18,848:INFO:             fastapi: 0.115.8
2025-02-28 12:43:18,848:INFO:             uvicorn: 0.34.0
2025-02-28 12:43:18,848:INFO:              m2cgen: 0.10.0
2025-02-28 12:43:18,848:INFO:           evidently: 0.4.40
2025-02-28 12:43:18,848:INFO:               fugue: 0.8.7
2025-02-28 12:43:18,848:INFO:           streamlit: Not installed
2025-02-28 12:43:18,848:INFO:             prophet: Not installed
2025-02-28 12:43:18,848:INFO:None
2025-02-28 12:43:18,848:INFO:Set up data.
2025-02-28 12:43:18,852:INFO:Set up folding strategy.
2025-02-28 12:43:18,852:INFO:Set up train/test split.
2025-02-28 12:43:18,854:INFO:Set up index.
2025-02-28 12:43:18,854:INFO:Assigning column types.
2025-02-28 12:43:18,855:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 12:43:18,857:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,859:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,861:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,887:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,907:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,907:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:18,909:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:18,909:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,911:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,913:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,939:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,959:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,959:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:18,961:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:18,961:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 12:43:18,963:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,965:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:18,991:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,011:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,011:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,013:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,015:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,017:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,043:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,062:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,063:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,064:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,064:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 12:43:19,069:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,095:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,114:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,115:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,115:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,121:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,146:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,166:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,166:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,167:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,167:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 12:43:19,197:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,217:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,217:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,219:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,249:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,268:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,269:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,270:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,271:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 12:43:19,301:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,321:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,322:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,353:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 12:43:19,374:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,375:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,376:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 12:43:19,427:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,428:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,478:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,480:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,480:INFO:Preparing preprocessing pipeline...
2025-02-28 12:43:19,481:INFO:Set up simple imputation.
2025-02-28 12:43:19,482:INFO:Set up encoding of categorical features.
2025-02-28 12:43:19,511:INFO:Finished creating preprocessing pipeline.
2025-02-28 12:43:19,514:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 12:43:19,514:INFO:Creating final display dataframe.
2025-02-28 12:43:19,612:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              9222
2025-02-28 12:43:19,667:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,668:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,718:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 12:43:19,719:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 12:43:19,720:INFO:setup() successfully completed in 0.88s...............
2025-02-28 12:43:19,745:INFO:Initializing compare_models()
2025-02-28 12:43:19,745:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 12:43:19,745:INFO:Checking exceptions
2025-02-28 12:43:19,747:INFO:Preparing display monitor
2025-02-28 12:43:19,765:INFO:Initializing Linear Regression
2025-02-28 12:43:19,765:INFO:Total runtime is 0.0 minutes
2025-02-28 12:43:19,769:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:19,769:INFO:Initializing create_model()
2025-02-28 12:43:19,769:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:19,769:INFO:Checking exceptions
2025-02-28 12:43:19,769:INFO:Importing libraries
2025-02-28 12:43:19,769:INFO:Copying training dataset
2025-02-28 12:43:19,772:INFO:Defining folds
2025-02-28 12:43:19,772:INFO:Declaring metric variables
2025-02-28 12:43:19,775:INFO:Importing untrained model
2025-02-28 12:43:19,778:INFO:Linear Regression Imported successfully
2025-02-28 12:43:19,783:INFO:Starting cross validation
2025-02-28 12:43:19,784:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:19,875:INFO:Calculating mean and std
2025-02-28 12:43:19,875:INFO:Creating metrics dataframe
2025-02-28 12:43:19,875:INFO:Uploading results into container
2025-02-28 12:43:19,877:INFO:Uploading model into container now
2025-02-28 12:43:19,877:INFO:_master_model_container: 1
2025-02-28 12:43:19,877:INFO:_display_container: 2
2025-02-28 12:43:19,877:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 12:43:19,877:INFO:create_model() successfully completed......................................
2025-02-28 12:43:19,953:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:19,953:INFO:Creating metrics dataframe
2025-02-28 12:43:19,957:INFO:Initializing Lasso Regression
2025-02-28 12:43:19,957:INFO:Total runtime is 0.003201345602671305 minutes
2025-02-28 12:43:19,959:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:19,960:INFO:Initializing create_model()
2025-02-28 12:43:19,960:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:19,960:INFO:Checking exceptions
2025-02-28 12:43:19,960:INFO:Importing libraries
2025-02-28 12:43:19,960:INFO:Copying training dataset
2025-02-28 12:43:19,963:INFO:Defining folds
2025-02-28 12:43:19,963:INFO:Declaring metric variables
2025-02-28 12:43:19,965:INFO:Importing untrained model
2025-02-28 12:43:19,967:INFO:Lasso Regression Imported successfully
2025-02-28 12:43:19,970:INFO:Starting cross validation
2025-02-28 12:43:19,971:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,063:INFO:Calculating mean and std
2025-02-28 12:43:20,063:INFO:Creating metrics dataframe
2025-02-28 12:43:20,064:INFO:Uploading results into container
2025-02-28 12:43:20,064:INFO:Uploading model into container now
2025-02-28 12:43:20,065:INFO:_master_model_container: 2
2025-02-28 12:43:20,065:INFO:_display_container: 2
2025-02-28 12:43:20,065:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 12:43:20,065:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,144:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,144:INFO:Creating metrics dataframe
2025-02-28 12:43:20,148:INFO:Initializing Ridge Regression
2025-02-28 12:43:20,148:INFO:Total runtime is 0.006386613845825196 minutes
2025-02-28 12:43:20,150:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,150:INFO:Initializing create_model()
2025-02-28 12:43:20,150:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,150:INFO:Checking exceptions
2025-02-28 12:43:20,150:INFO:Importing libraries
2025-02-28 12:43:20,150:INFO:Copying training dataset
2025-02-28 12:43:20,152:INFO:Defining folds
2025-02-28 12:43:20,152:INFO:Declaring metric variables
2025-02-28 12:43:20,155:INFO:Importing untrained model
2025-02-28 12:43:20,157:INFO:Ridge Regression Imported successfully
2025-02-28 12:43:20,161:INFO:Starting cross validation
2025-02-28 12:43:20,162:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,249:INFO:Calculating mean and std
2025-02-28 12:43:20,249:INFO:Creating metrics dataframe
2025-02-28 12:43:20,250:INFO:Uploading results into container
2025-02-28 12:43:20,250:INFO:Uploading model into container now
2025-02-28 12:43:20,251:INFO:_master_model_container: 3
2025-02-28 12:43:20,251:INFO:_display_container: 2
2025-02-28 12:43:20,251:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 12:43:20,251:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,329:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,329:INFO:Creating metrics dataframe
2025-02-28 12:43:20,333:INFO:Initializing Elastic Net
2025-02-28 12:43:20,333:INFO:Total runtime is 0.009476137161254884 minutes
2025-02-28 12:43:20,335:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,335:INFO:Initializing create_model()
2025-02-28 12:43:20,335:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,335:INFO:Checking exceptions
2025-02-28 12:43:20,335:INFO:Importing libraries
2025-02-28 12:43:20,335:INFO:Copying training dataset
2025-02-28 12:43:20,338:INFO:Defining folds
2025-02-28 12:43:20,338:INFO:Declaring metric variables
2025-02-28 12:43:20,340:INFO:Importing untrained model
2025-02-28 12:43:20,343:INFO:Elastic Net Imported successfully
2025-02-28 12:43:20,347:INFO:Starting cross validation
2025-02-28 12:43:20,348:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,435:INFO:Calculating mean and std
2025-02-28 12:43:20,435:INFO:Creating metrics dataframe
2025-02-28 12:43:20,436:INFO:Uploading results into container
2025-02-28 12:43:20,436:INFO:Uploading model into container now
2025-02-28 12:43:20,437:INFO:_master_model_container: 4
2025-02-28 12:43:20,437:INFO:_display_container: 2
2025-02-28 12:43:20,437:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 12:43:20,437:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,514:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,514:INFO:Creating metrics dataframe
2025-02-28 12:43:20,518:INFO:Initializing Least Angle Regression
2025-02-28 12:43:20,518:INFO:Total runtime is 0.012556175390879314 minutes
2025-02-28 12:43:20,520:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,520:INFO:Initializing create_model()
2025-02-28 12:43:20,521:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,521:INFO:Checking exceptions
2025-02-28 12:43:20,521:INFO:Importing libraries
2025-02-28 12:43:20,521:INFO:Copying training dataset
2025-02-28 12:43:20,523:INFO:Defining folds
2025-02-28 12:43:20,523:INFO:Declaring metric variables
2025-02-28 12:43:20,525:INFO:Importing untrained model
2025-02-28 12:43:20,528:INFO:Least Angle Regression Imported successfully
2025-02-28 12:43:20,532:INFO:Starting cross validation
2025-02-28 12:43:20,533:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,620:INFO:Calculating mean and std
2025-02-28 12:43:20,620:INFO:Creating metrics dataframe
2025-02-28 12:43:20,621:INFO:Uploading results into container
2025-02-28 12:43:20,621:INFO:Uploading model into container now
2025-02-28 12:43:20,621:INFO:_master_model_container: 5
2025-02-28 12:43:20,622:INFO:_display_container: 2
2025-02-28 12:43:20,622:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 12:43:20,622:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,696:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,696:INFO:Creating metrics dataframe
2025-02-28 12:43:20,700:INFO:Initializing Lasso Least Angle Regression
2025-02-28 12:43:20,700:INFO:Total runtime is 0.015590274333953859 minutes
2025-02-28 12:43:20,702:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,702:INFO:Initializing create_model()
2025-02-28 12:43:20,703:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,703:INFO:Checking exceptions
2025-02-28 12:43:20,703:INFO:Importing libraries
2025-02-28 12:43:20,703:INFO:Copying training dataset
2025-02-28 12:43:20,705:INFO:Defining folds
2025-02-28 12:43:20,705:INFO:Declaring metric variables
2025-02-28 12:43:20,708:INFO:Importing untrained model
2025-02-28 12:43:20,710:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 12:43:20,714:INFO:Starting cross validation
2025-02-28 12:43:20,715:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,791:INFO:Calculating mean and std
2025-02-28 12:43:20,791:INFO:Creating metrics dataframe
2025-02-28 12:43:20,793:INFO:Uploading results into container
2025-02-28 12:43:20,793:INFO:Uploading model into container now
2025-02-28 12:43:20,793:INFO:_master_model_container: 6
2025-02-28 12:43:20,793:INFO:_display_container: 2
2025-02-28 12:43:20,794:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 12:43:20,794:INFO:create_model() successfully completed......................................
2025-02-28 12:43:20,884:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:20,884:INFO:Creating metrics dataframe
2025-02-28 12:43:20,888:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 12:43:20,888:INFO:Total runtime is 0.018726305166880293 minutes
2025-02-28 12:43:20,890:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:20,890:INFO:Initializing create_model()
2025-02-28 12:43:20,890:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:20,890:INFO:Checking exceptions
2025-02-28 12:43:20,890:INFO:Importing libraries
2025-02-28 12:43:20,890:INFO:Copying training dataset
2025-02-28 12:43:20,893:INFO:Defining folds
2025-02-28 12:43:20,893:INFO:Declaring metric variables
2025-02-28 12:43:20,895:INFO:Importing untrained model
2025-02-28 12:43:20,897:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 12:43:20,902:INFO:Starting cross validation
2025-02-28 12:43:20,903:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:20,992:INFO:Calculating mean and std
2025-02-28 12:43:20,992:INFO:Creating metrics dataframe
2025-02-28 12:43:20,993:INFO:Uploading results into container
2025-02-28 12:43:20,993:INFO:Uploading model into container now
2025-02-28 12:43:20,993:INFO:_master_model_container: 7
2025-02-28 12:43:20,993:INFO:_display_container: 2
2025-02-28 12:43:20,994:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 12:43:20,994:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,071:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,071:INFO:Creating metrics dataframe
2025-02-28 12:43:21,076:INFO:Initializing Bayesian Ridge
2025-02-28 12:43:21,077:INFO:Total runtime is 0.021868137518564864 minutes
2025-02-28 12:43:21,079:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,079:INFO:Initializing create_model()
2025-02-28 12:43:21,079:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,079:INFO:Checking exceptions
2025-02-28 12:43:21,079:INFO:Importing libraries
2025-02-28 12:43:21,079:INFO:Copying training dataset
2025-02-28 12:43:21,082:INFO:Defining folds
2025-02-28 12:43:21,082:INFO:Declaring metric variables
2025-02-28 12:43:21,084:INFO:Importing untrained model
2025-02-28 12:43:21,087:INFO:Bayesian Ridge Imported successfully
2025-02-28 12:43:21,091:INFO:Starting cross validation
2025-02-28 12:43:21,092:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,181:INFO:Calculating mean and std
2025-02-28 12:43:21,181:INFO:Creating metrics dataframe
2025-02-28 12:43:21,182:INFO:Uploading results into container
2025-02-28 12:43:21,183:INFO:Uploading model into container now
2025-02-28 12:43:21,183:INFO:_master_model_container: 8
2025-02-28 12:43:21,183:INFO:_display_container: 2
2025-02-28 12:43:21,183:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 12:43:21,184:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,263:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,263:INFO:Creating metrics dataframe
2025-02-28 12:43:21,267:INFO:Initializing Passive Aggressive Regressor
2025-02-28 12:43:21,267:INFO:Total runtime is 0.025045239925384526 minutes
2025-02-28 12:43:21,269:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,269:INFO:Initializing create_model()
2025-02-28 12:43:21,269:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,269:INFO:Checking exceptions
2025-02-28 12:43:21,269:INFO:Importing libraries
2025-02-28 12:43:21,269:INFO:Copying training dataset
2025-02-28 12:43:21,272:INFO:Defining folds
2025-02-28 12:43:21,272:INFO:Declaring metric variables
2025-02-28 12:43:21,274:INFO:Importing untrained model
2025-02-28 12:43:21,276:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 12:43:21,280:INFO:Starting cross validation
2025-02-28 12:43:21,281:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,367:INFO:Calculating mean and std
2025-02-28 12:43:21,368:INFO:Creating metrics dataframe
2025-02-28 12:43:21,369:INFO:Uploading results into container
2025-02-28 12:43:21,369:INFO:Uploading model into container now
2025-02-28 12:43:21,369:INFO:_master_model_container: 9
2025-02-28 12:43:21,369:INFO:_display_container: 2
2025-02-28 12:43:21,370:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 12:43:21,370:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,447:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,447:INFO:Creating metrics dataframe
2025-02-28 12:43:21,452:INFO:Initializing Huber Regressor
2025-02-28 12:43:21,452:INFO:Total runtime is 0.0281157930692037 minutes
2025-02-28 12:43:21,454:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,455:INFO:Initializing create_model()
2025-02-28 12:43:21,455:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,455:INFO:Checking exceptions
2025-02-28 12:43:21,455:INFO:Importing libraries
2025-02-28 12:43:21,455:INFO:Copying training dataset
2025-02-28 12:43:21,457:INFO:Defining folds
2025-02-28 12:43:21,457:INFO:Declaring metric variables
2025-02-28 12:43:21,459:INFO:Importing untrained model
2025-02-28 12:43:21,461:INFO:Huber Regressor Imported successfully
2025-02-28 12:43:21,465:INFO:Starting cross validation
2025-02-28 12:43:21,467:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,529:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,534:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,535:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,535:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,538:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,538:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,541:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,543:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,552:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,555:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 12:43:21,568:INFO:Calculating mean and std
2025-02-28 12:43:21,569:INFO:Creating metrics dataframe
2025-02-28 12:43:21,570:INFO:Uploading results into container
2025-02-28 12:43:21,570:INFO:Uploading model into container now
2025-02-28 12:43:21,570:INFO:_master_model_container: 10
2025-02-28 12:43:21,571:INFO:_display_container: 2
2025-02-28 12:43:21,571:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 12:43:21,571:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,652:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,652:INFO:Creating metrics dataframe
2025-02-28 12:43:21,657:INFO:Initializing K Neighbors Regressor
2025-02-28 12:43:21,657:INFO:Total runtime is 0.03153393665949504 minutes
2025-02-28 12:43:21,659:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,659:INFO:Initializing create_model()
2025-02-28 12:43:21,659:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,659:INFO:Checking exceptions
2025-02-28 12:43:21,659:INFO:Importing libraries
2025-02-28 12:43:21,659:INFO:Copying training dataset
2025-02-28 12:43:21,662:INFO:Defining folds
2025-02-28 12:43:21,662:INFO:Declaring metric variables
2025-02-28 12:43:21,664:INFO:Importing untrained model
2025-02-28 12:43:21,665:INFO:K Neighbors Regressor Imported successfully
2025-02-28 12:43:21,670:INFO:Starting cross validation
2025-02-28 12:43:21,671:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,758:INFO:Calculating mean and std
2025-02-28 12:43:21,759:INFO:Creating metrics dataframe
2025-02-28 12:43:21,761:INFO:Uploading results into container
2025-02-28 12:43:21,761:INFO:Uploading model into container now
2025-02-28 12:43:21,761:INFO:_master_model_container: 11
2025-02-28 12:43:21,761:INFO:_display_container: 2
2025-02-28 12:43:21,762:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 12:43:21,762:INFO:create_model() successfully completed......................................
2025-02-28 12:43:21,841:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:21,841:INFO:Creating metrics dataframe
2025-02-28 12:43:21,845:INFO:Initializing Decision Tree Regressor
2025-02-28 12:43:21,845:INFO:Total runtime is 0.03467075030008952 minutes
2025-02-28 12:43:21,847:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:21,847:INFO:Initializing create_model()
2025-02-28 12:43:21,848:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:21,848:INFO:Checking exceptions
2025-02-28 12:43:21,848:INFO:Importing libraries
2025-02-28 12:43:21,848:INFO:Copying training dataset
2025-02-28 12:43:21,850:INFO:Defining folds
2025-02-28 12:43:21,850:INFO:Declaring metric variables
2025-02-28 12:43:21,853:INFO:Importing untrained model
2025-02-28 12:43:21,855:INFO:Decision Tree Regressor Imported successfully
2025-02-28 12:43:21,858:INFO:Starting cross validation
2025-02-28 12:43:21,859:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:21,976:INFO:Calculating mean and std
2025-02-28 12:43:21,976:INFO:Creating metrics dataframe
2025-02-28 12:43:21,978:INFO:Uploading results into container
2025-02-28 12:43:21,978:INFO:Uploading model into container now
2025-02-28 12:43:21,978:INFO:_master_model_container: 12
2025-02-28 12:43:21,978:INFO:_display_container: 2
2025-02-28 12:43:21,978:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 12:43:21,978:INFO:create_model() successfully completed......................................
2025-02-28 12:43:22,057:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:22,057:INFO:Creating metrics dataframe
2025-02-28 12:43:22,062:INFO:Initializing Random Forest Regressor
2025-02-28 12:43:22,062:INFO:Total runtime is 0.03829707304636638 minutes
2025-02-28 12:43:22,064:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:22,064:INFO:Initializing create_model()
2025-02-28 12:43:22,065:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:22,065:INFO:Checking exceptions
2025-02-28 12:43:22,065:INFO:Importing libraries
2025-02-28 12:43:22,065:INFO:Copying training dataset
2025-02-28 12:43:22,067:INFO:Defining folds
2025-02-28 12:43:22,067:INFO:Declaring metric variables
2025-02-28 12:43:22,069:INFO:Importing untrained model
2025-02-28 12:43:22,072:INFO:Random Forest Regressor Imported successfully
2025-02-28 12:43:22,075:INFO:Starting cross validation
2025-02-28 12:43:22,076:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:22,927:INFO:Calculating mean and std
2025-02-28 12:43:22,928:INFO:Creating metrics dataframe
2025-02-28 12:43:22,929:INFO:Uploading results into container
2025-02-28 12:43:22,929:INFO:Uploading model into container now
2025-02-28 12:43:22,929:INFO:_master_model_container: 13
2025-02-28 12:43:22,930:INFO:_display_container: 2
2025-02-28 12:43:22,930:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 12:43:22,930:INFO:create_model() successfully completed......................................
2025-02-28 12:43:23,007:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:23,008:INFO:Creating metrics dataframe
2025-02-28 12:43:23,013:INFO:Initializing Extra Trees Regressor
2025-02-28 12:43:23,013:INFO:Total runtime is 0.05413374106089275 minutes
2025-02-28 12:43:23,015:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:23,015:INFO:Initializing create_model()
2025-02-28 12:43:23,015:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:23,015:INFO:Checking exceptions
2025-02-28 12:43:23,015:INFO:Importing libraries
2025-02-28 12:43:23,015:INFO:Copying training dataset
2025-02-28 12:43:23,017:INFO:Defining folds
2025-02-28 12:43:23,017:INFO:Declaring metric variables
2025-02-28 12:43:23,019:INFO:Importing untrained model
2025-02-28 12:43:23,021:INFO:Extra Trees Regressor Imported successfully
2025-02-28 12:43:23,025:INFO:Starting cross validation
2025-02-28 12:43:23,027:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:23,580:INFO:Calculating mean and std
2025-02-28 12:43:23,581:INFO:Creating metrics dataframe
2025-02-28 12:43:23,582:INFO:Uploading results into container
2025-02-28 12:43:23,582:INFO:Uploading model into container now
2025-02-28 12:43:23,582:INFO:_master_model_container: 14
2025-02-28 12:43:23,583:INFO:_display_container: 2
2025-02-28 12:43:23,583:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 12:43:23,583:INFO:create_model() successfully completed......................................
2025-02-28 12:43:23,667:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:23,667:INFO:Creating metrics dataframe
2025-02-28 12:43:23,673:INFO:Initializing AdaBoost Regressor
2025-02-28 12:43:23,673:INFO:Total runtime is 0.06514684359232585 minutes
2025-02-28 12:43:23,676:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:23,676:INFO:Initializing create_model()
2025-02-28 12:43:23,676:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:23,677:INFO:Checking exceptions
2025-02-28 12:43:23,677:INFO:Importing libraries
2025-02-28 12:43:23,677:INFO:Copying training dataset
2025-02-28 12:43:23,679:INFO:Defining folds
2025-02-28 12:43:23,679:INFO:Declaring metric variables
2025-02-28 12:43:23,681:INFO:Importing untrained model
2025-02-28 12:43:23,683:INFO:AdaBoost Regressor Imported successfully
2025-02-28 12:43:23,688:INFO:Starting cross validation
2025-02-28 12:43:23,688:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:23,861:INFO:Calculating mean and std
2025-02-28 12:43:23,861:INFO:Creating metrics dataframe
2025-02-28 12:43:23,862:INFO:Uploading results into container
2025-02-28 12:43:23,863:INFO:Uploading model into container now
2025-02-28 12:43:23,863:INFO:_master_model_container: 15
2025-02-28 12:43:23,863:INFO:_display_container: 2
2025-02-28 12:43:23,863:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 12:43:23,863:INFO:create_model() successfully completed......................................
2025-02-28 12:43:23,939:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:23,939:INFO:Creating metrics dataframe
2025-02-28 12:43:23,944:INFO:Initializing Gradient Boosting Regressor
2025-02-28 12:43:23,945:INFO:Total runtime is 0.06968005100886028 minutes
2025-02-28 12:43:23,947:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:23,947:INFO:Initializing create_model()
2025-02-28 12:43:23,948:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:23,948:INFO:Checking exceptions
2025-02-28 12:43:23,948:INFO:Importing libraries
2025-02-28 12:43:23,948:INFO:Copying training dataset
2025-02-28 12:43:23,950:INFO:Defining folds
2025-02-28 12:43:23,950:INFO:Declaring metric variables
2025-02-28 12:43:23,952:INFO:Importing untrained model
2025-02-28 12:43:23,954:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 12:43:23,958:INFO:Starting cross validation
2025-02-28 12:43:23,959:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:24,372:INFO:Calculating mean and std
2025-02-28 12:43:24,373:INFO:Creating metrics dataframe
2025-02-28 12:43:24,374:INFO:Uploading results into container
2025-02-28 12:43:24,374:INFO:Uploading model into container now
2025-02-28 12:43:24,374:INFO:_master_model_container: 16
2025-02-28 12:43:24,374:INFO:_display_container: 2
2025-02-28 12:43:24,375:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 12:43:24,375:INFO:create_model() successfully completed......................................
2025-02-28 12:43:24,453:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:24,453:INFO:Creating metrics dataframe
2025-02-28 12:43:24,458:INFO:Initializing Extreme Gradient Boosting
2025-02-28 12:43:24,458:INFO:Total runtime is 0.07822784185409547 minutes
2025-02-28 12:43:24,460:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:24,461:INFO:Initializing create_model()
2025-02-28 12:43:24,461:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:24,461:INFO:Checking exceptions
2025-02-28 12:43:24,461:INFO:Importing libraries
2025-02-28 12:43:24,461:INFO:Copying training dataset
2025-02-28 12:43:24,463:INFO:Defining folds
2025-02-28 12:43:24,463:INFO:Declaring metric variables
2025-02-28 12:43:24,465:INFO:Importing untrained model
2025-02-28 12:43:24,468:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:43:24,472:INFO:Starting cross validation
2025-02-28 12:43:24,473:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:24,963:INFO:Calculating mean and std
2025-02-28 12:43:24,964:INFO:Creating metrics dataframe
2025-02-28 12:43:24,965:INFO:Uploading results into container
2025-02-28 12:43:24,965:INFO:Uploading model into container now
2025-02-28 12:43:24,965:INFO:_master_model_container: 17
2025-02-28 12:43:24,965:INFO:_display_container: 2
2025-02-28 12:43:24,966:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:43:24,966:INFO:create_model() successfully completed......................................
2025-02-28 12:43:25,042:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:25,042:INFO:Creating metrics dataframe
2025-02-28 12:43:25,048:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 12:43:25,048:INFO:Total runtime is 0.08806429306666057 minutes
2025-02-28 12:43:25,050:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:25,050:INFO:Initializing create_model()
2025-02-28 12:43:25,050:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:25,050:INFO:Checking exceptions
2025-02-28 12:43:25,050:INFO:Importing libraries
2025-02-28 12:43:25,050:INFO:Copying training dataset
2025-02-28 12:43:25,053:INFO:Defining folds
2025-02-28 12:43:25,053:INFO:Declaring metric variables
2025-02-28 12:43:25,055:INFO:Importing untrained model
2025-02-28 12:43:25,058:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 12:43:25,062:INFO:Starting cross validation
2025-02-28 12:43:25,063:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:26,432:INFO:Calculating mean and std
2025-02-28 12:43:26,433:INFO:Creating metrics dataframe
2025-02-28 12:43:26,434:INFO:Uploading results into container
2025-02-28 12:43:26,435:INFO:Uploading model into container now
2025-02-28 12:43:26,435:INFO:_master_model_container: 18
2025-02-28 12:43:26,435:INFO:_display_container: 2
2025-02-28 12:43:26,435:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 12:43:26,435:INFO:create_model() successfully completed......................................
2025-02-28 12:43:26,531:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:26,531:INFO:Creating metrics dataframe
2025-02-28 12:43:26,537:INFO:Initializing CatBoost Regressor
2025-02-28 12:43:26,537:INFO:Total runtime is 0.11286828517913819 minutes
2025-02-28 12:43:26,539:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:26,540:INFO:Initializing create_model()
2025-02-28 12:43:26,540:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:26,540:INFO:Checking exceptions
2025-02-28 12:43:26,540:INFO:Importing libraries
2025-02-28 12:43:26,540:INFO:Copying training dataset
2025-02-28 12:43:26,543:INFO:Defining folds
2025-02-28 12:43:26,543:INFO:Declaring metric variables
2025-02-28 12:43:26,546:INFO:Importing untrained model
2025-02-28 12:43:26,548:INFO:CatBoost Regressor Imported successfully
2025-02-28 12:43:26,553:INFO:Starting cross validation
2025-02-28 12:43:26,554:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:29,176:INFO:Calculating mean and std
2025-02-28 12:43:29,177:INFO:Creating metrics dataframe
2025-02-28 12:43:29,178:INFO:Uploading results into container
2025-02-28 12:43:29,178:INFO:Uploading model into container now
2025-02-28 12:43:29,178:INFO:_master_model_container: 19
2025-02-28 12:43:29,178:INFO:_display_container: 2
2025-02-28 12:43:29,178:INFO:<catboost.core.CatBoostRegressor object at 0x000002188D17B1F0>
2025-02-28 12:43:29,179:INFO:create_model() successfully completed......................................
2025-02-28 12:43:29,258:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:29,258:INFO:Creating metrics dataframe
2025-02-28 12:43:29,264:INFO:Initializing Dummy Regressor
2025-02-28 12:43:29,264:INFO:Total runtime is 0.1583217223485311 minutes
2025-02-28 12:43:29,266:INFO:SubProcess create_model() called ==================================
2025-02-28 12:43:29,266:INFO:Initializing create_model()
2025-02-28 12:43:29,266:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188E5F07C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:29,266:INFO:Checking exceptions
2025-02-28 12:43:29,266:INFO:Importing libraries
2025-02-28 12:43:29,266:INFO:Copying training dataset
2025-02-28 12:43:29,269:INFO:Defining folds
2025-02-28 12:43:29,269:INFO:Declaring metric variables
2025-02-28 12:43:29,271:INFO:Importing untrained model
2025-02-28 12:43:29,274:INFO:Dummy Regressor Imported successfully
2025-02-28 12:43:29,279:INFO:Starting cross validation
2025-02-28 12:43:29,280:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:29,363:INFO:Calculating mean and std
2025-02-28 12:43:29,364:INFO:Creating metrics dataframe
2025-02-28 12:43:29,365:INFO:Uploading results into container
2025-02-28 12:43:29,365:INFO:Uploading model into container now
2025-02-28 12:43:29,365:INFO:_master_model_container: 20
2025-02-28 12:43:29,365:INFO:_display_container: 2
2025-02-28 12:43:29,366:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:43:29,366:INFO:create_model() successfully completed......................................
2025-02-28 12:43:29,451:INFO:SubProcess create_model() end ==================================
2025-02-28 12:43:29,451:INFO:Creating metrics dataframe
2025-02-28 12:43:29,458:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 12:43:29,462:INFO:Initializing create_model()
2025-02-28 12:43:29,462:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:29,462:INFO:Checking exceptions
2025-02-28 12:43:29,463:INFO:Importing libraries
2025-02-28 12:43:29,463:INFO:Copying training dataset
2025-02-28 12:43:29,465:INFO:Defining folds
2025-02-28 12:43:29,465:INFO:Declaring metric variables
2025-02-28 12:43:29,465:INFO:Importing untrained model
2025-02-28 12:43:29,465:INFO:Declaring custom model
2025-02-28 12:43:29,465:INFO:Dummy Regressor Imported successfully
2025-02-28 12:43:29,467:INFO:Cross validation set to False
2025-02-28 12:43:29,467:INFO:Fitting Model
2025-02-28 12:43:29,490:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:43:29,490:INFO:create_model() successfully completed......................................
2025-02-28 12:43:29,589:INFO:_master_model_container: 20
2025-02-28 12:43:29,589:INFO:_display_container: 2
2025-02-28 12:43:29,589:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:43:29,589:INFO:compare_models() successfully completed......................................
2025-02-28 12:43:29,629:INFO:Initializing create_model()
2025-02-28 12:43:29,629:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:29,629:INFO:Checking exceptions
2025-02-28 12:43:29,638:INFO:Importing libraries
2025-02-28 12:43:29,638:INFO:Copying training dataset
2025-02-28 12:43:29,641:INFO:Defining folds
2025-02-28 12:43:29,641:INFO:Declaring metric variables
2025-02-28 12:43:29,643:INFO:Importing untrained model
2025-02-28 12:43:29,645:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 12:43:29,650:INFO:Starting cross validation
2025-02-28 12:43:29,651:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:43:30,125:INFO:Calculating mean and std
2025-02-28 12:43:30,125:INFO:Creating metrics dataframe
2025-02-28 12:43:30,128:INFO:Finalizing model
2025-02-28 12:43:30,295:INFO:Uploading results into container
2025-02-28 12:43:30,296:INFO:Uploading model into container now
2025-02-28 12:43:30,303:INFO:_master_model_container: 21
2025-02-28 12:43:30,303:INFO:_display_container: 3
2025-02-28 12:43:30,303:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 12:43:30,304:INFO:create_model() successfully completed......................................
2025-02-28 12:43:30,414:INFO:Initializing plot_model()
2025-02-28 12:43:30,414:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:43:30,414:INFO:Checking exceptions
2025-02-28 12:43:30,418:INFO:Preloading libraries
2025-02-28 12:43:30,423:INFO:Copying training dataset
2025-02-28 12:43:30,423:INFO:Plot type: residuals
2025-02-28 12:43:30,581:INFO:Fitting Model
2025-02-28 12:43:30,627:INFO:Scoring test/hold-out set
2025-02-28 12:43:30,930:INFO:Visual Rendered Successfully
2025-02-28 12:43:31,014:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:31,039:INFO:Initializing plot_model()
2025-02-28 12:43:31,039:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:43:31,039:INFO:Checking exceptions
2025-02-28 12:43:31,042:INFO:Preloading libraries
2025-02-28 12:43:31,047:INFO:Copying training dataset
2025-02-28 12:43:31,047:INFO:Plot type: error
2025-02-28 12:43:31,181:INFO:Fitting Model
2025-02-28 12:43:31,182:INFO:Scoring test/hold-out set
2025-02-28 12:43:31,343:INFO:Visual Rendered Successfully
2025-02-28 12:43:31,430:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:31,447:INFO:Initializing plot_model()
2025-02-28 12:43:31,447:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:43:31,447:INFO:Checking exceptions
2025-02-28 12:43:31,450:INFO:Preloading libraries
2025-02-28 12:43:31,454:INFO:Copying training dataset
2025-02-28 12:43:31,454:INFO:Plot type: feature
2025-02-28 12:43:31,455:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 12:43:31,585:INFO:Visual Rendered Successfully
2025-02-28 12:43:31,674:INFO:plot_model() successfully completed......................................
2025-02-28 12:43:34,208:WARNING:C:\Users\dagir\AppData\Local\Temp\ipykernel_24096\1216279964.py:849: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  import pkg_resources

2025-02-28 12:43:54,140:INFO:Initializing create_model()
2025-02-28 12:43:54,140:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=Dummy Regressor, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:43:54,140:INFO:Checking exceptions
2025-02-28 12:44:15,140:INFO:Initializing create_model()
2025-02-28 12:44:15,141:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=dummy, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:44:15,141:INFO:Checking exceptions
2025-02-28 12:44:15,150:INFO:Importing libraries
2025-02-28 12:44:15,150:INFO:Copying training dataset
2025-02-28 12:44:15,153:INFO:Defining folds
2025-02-28 12:44:15,153:INFO:Declaring metric variables
2025-02-28 12:44:15,155:INFO:Importing untrained model
2025-02-28 12:44:15,157:INFO:Dummy Regressor Imported successfully
2025-02-28 12:44:15,160:INFO:Starting cross validation
2025-02-28 12:44:15,161:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:44:15,247:INFO:Calculating mean and std
2025-02-28 12:44:15,247:INFO:Creating metrics dataframe
2025-02-28 12:44:15,250:INFO:Finalizing model
2025-02-28 12:44:15,275:INFO:Uploading results into container
2025-02-28 12:44:15,275:INFO:Uploading model into container now
2025-02-28 12:44:15,281:INFO:_master_model_container: 22
2025-02-28 12:44:15,281:INFO:_display_container: 4
2025-02-28 12:44:15,281:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 12:44:15,281:INFO:create_model() successfully completed......................................
2025-02-28 12:44:17,652:INFO:Initializing plot_model()
2025-02-28 12:44:17,652:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:17,652:INFO:Checking exceptions
2025-02-28 12:44:17,655:INFO:Preloading libraries
2025-02-28 12:44:17,655:INFO:Copying training dataset
2025-02-28 12:44:17,655:INFO:Plot type: residuals
2025-02-28 12:44:17,766:INFO:Fitting Model
2025-02-28 12:44:17,782:INFO:Scoring test/hold-out set
2025-02-28 12:44:17,965:INFO:Visual Rendered Successfully
2025-02-28 12:44:18,074:INFO:plot_model() successfully completed......................................
2025-02-28 12:44:20,454:INFO:Initializing plot_model()
2025-02-28 12:44:20,454:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:20,454:INFO:Checking exceptions
2025-02-28 12:44:20,456:INFO:Preloading libraries
2025-02-28 12:44:20,456:INFO:Copying training dataset
2025-02-28 12:44:20,457:INFO:Plot type: error
2025-02-28 12:44:20,553:INFO:Fitting Model
2025-02-28 12:44:20,553:INFO:Scoring test/hold-out set
2025-02-28 12:44:20,661:INFO:Visual Rendered Successfully
2025-02-28 12:44:20,771:INFO:plot_model() successfully completed......................................
2025-02-28 12:44:22,542:INFO:Initializing plot_model()
2025-02-28 12:44:22,542:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:22,542:INFO:Checking exceptions
2025-02-28 12:44:41,286:INFO:Initializing create_model()
2025-02-28 12:44:41,287:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, estimator=omp, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 12:44:41,287:INFO:Checking exceptions
2025-02-28 12:44:41,294:INFO:Importing libraries
2025-02-28 12:44:41,295:INFO:Copying training dataset
2025-02-28 12:44:41,297:INFO:Defining folds
2025-02-28 12:44:41,297:INFO:Declaring metric variables
2025-02-28 12:44:41,299:INFO:Importing untrained model
2025-02-28 12:44:41,301:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 12:44:41,305:INFO:Starting cross validation
2025-02-28 12:44:41,307:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 12:44:41,405:INFO:Calculating mean and std
2025-02-28 12:44:41,405:INFO:Creating metrics dataframe
2025-02-28 12:44:41,408:INFO:Finalizing model
2025-02-28 12:44:41,433:INFO:Uploading results into container
2025-02-28 12:44:41,434:INFO:Uploading model into container now
2025-02-28 12:44:41,438:INFO:_master_model_container: 23
2025-02-28 12:44:41,438:INFO:_display_container: 5
2025-02-28 12:44:41,438:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 12:44:41,438:INFO:create_model() successfully completed......................................
2025-02-28 12:44:42,335:INFO:Initializing plot_model()
2025-02-28 12:44:42,335:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:42,335:INFO:Checking exceptions
2025-02-28 12:44:42,338:INFO:Preloading libraries
2025-02-28 12:44:42,338:INFO:Copying training dataset
2025-02-28 12:44:42,338:INFO:Plot type: residuals
2025-02-28 12:44:42,449:INFO:Fitting Model
2025-02-28 12:44:42,449:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 12:44:42,467:INFO:Scoring test/hold-out set
2025-02-28 12:44:42,716:INFO:Visual Rendered Successfully
2025-02-28 12:44:42,835:INFO:plot_model() successfully completed......................................
2025-02-28 12:44:44,334:INFO:Initializing plot_model()
2025-02-28 12:44:44,334:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:44,334:INFO:Checking exceptions
2025-02-28 12:44:44,337:INFO:Preloading libraries
2025-02-28 12:44:44,337:INFO:Copying training dataset
2025-02-28 12:44:44,337:INFO:Plot type: error
2025-02-28 12:44:44,433:INFO:Fitting Model
2025-02-28 12:44:44,433:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 12:44:44,433:INFO:Scoring test/hold-out set
2025-02-28 12:44:44,547:INFO:Visual Rendered Successfully
2025-02-28 12:44:44,654:INFO:plot_model() successfully completed......................................
2025-02-28 12:44:45,510:INFO:Initializing plot_model()
2025-02-28 12:44:45,510:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188D118910>, system=True)
2025-02-28 12:44:45,510:INFO:Checking exceptions
2025-02-28 12:44:45,512:INFO:Preloading libraries
2025-02-28 12:44:45,512:INFO:Copying training dataset
2025-02-28 12:44:45,512:INFO:Plot type: feature
2025-02-28 12:44:45,640:INFO:Visual Rendered Successfully
2025-02-28 12:44:45,755:INFO:plot_model() successfully completed......................................
2025-02-28 13:00:12,662:INFO:PyCaret ClassificationExperiment
2025-02-28 13:00:12,663:INFO:Logging name: clf-default-name
2025-02-28 13:00:12,663:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 13:00:12,663:INFO:version 3.3.2
2025-02-28 13:00:12,663:INFO:Initializing setup()
2025-02-28 13:00:12,663:INFO:self.USI: cc4d
2025-02-28 13:00:12,664:INFO:self._variable_keys: {'gpu_param', 'target_param', 'exp_id', 'data', 'USI', 'gpu_n_jobs_param', 'fix_imbalance', '_ml_usecase', 'y_train', 'pipeline', '_available_plots', 'seed', 'X_train', 'fold_generator', 'is_multiclass', 'logging_param', 'X', 'fold_shuffle_param', 'html_param', 'idx', 'exp_name_log', 'memory', 'y', 'fold_groups_param', 'log_plots_param', 'y_test', 'X_test', 'n_jobs_param'}
2025-02-28 13:00:12,664:INFO:Checking environment
2025-02-28 13:00:12,664:INFO:python_version: 3.10.16
2025-02-28 13:00:12,664:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 13:00:12,664:INFO:machine: AMD64
2025-02-28 13:00:12,664:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 13:00:12,664:INFO:Memory: svmem(total=34200334336, available=18904117248, percent=44.7, used=15296217088, free=18904117248)
2025-02-28 13:00:12,664:INFO:Physical Core: 24
2025-02-28 13:00:12,664:INFO:Logical Core: 32
2025-02-28 13:00:12,664:INFO:Checking libraries
2025-02-28 13:00:12,664:INFO:System:
2025-02-28 13:00:12,664:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 13:00:12,665:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 13:00:12,665:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 13:00:12,665:INFO:PyCaret required dependencies:
2025-02-28 13:00:12,665:INFO:                 pip: 25.0
2025-02-28 13:00:12,665:INFO:          setuptools: 75.8.0
2025-02-28 13:00:12,665:INFO:             pycaret: 3.3.2
2025-02-28 13:00:12,665:INFO:             IPython: 8.30.0
2025-02-28 13:00:12,665:INFO:          ipywidgets: 8.1.5
2025-02-28 13:00:12,665:INFO:                tqdm: 4.67.1
2025-02-28 13:00:12,665:INFO:               numpy: 1.26.4
2025-02-28 13:00:12,665:INFO:              pandas: 2.1.4
2025-02-28 13:00:12,665:INFO:              jinja2: 3.1.5
2025-02-28 13:00:12,665:INFO:               scipy: 1.11.4
2025-02-28 13:00:12,665:INFO:              joblib: 1.3.2
2025-02-28 13:00:12,665:INFO:             sklearn: 1.4.2
2025-02-28 13:00:12,665:INFO:                pyod: 2.0.3
2025-02-28 13:00:12,665:INFO:            imblearn: 0.13.0
2025-02-28 13:00:12,665:INFO:   category_encoders: 2.7.0
2025-02-28 13:00:12,665:INFO:            lightgbm: 4.5.0
2025-02-28 13:00:12,665:INFO:               numba: 0.61.0
2025-02-28 13:00:12,665:INFO:            requests: 2.32.3
2025-02-28 13:00:12,666:INFO:          matplotlib: 3.7.5
2025-02-28 13:00:12,666:INFO:          scikitplot: 0.3.7
2025-02-28 13:00:12,666:INFO:         yellowbrick: 1.5
2025-02-28 13:00:12,666:INFO:              plotly: 5.24.1
2025-02-28 13:00:12,666:INFO:    plotly-resampler: Not installed
2025-02-28 13:00:12,666:INFO:             kaleido: 0.2.1
2025-02-28 13:00:12,666:INFO:           schemdraw: 0.15
2025-02-28 13:00:12,666:INFO:         statsmodels: 0.14.4
2025-02-28 13:00:12,666:INFO:              sktime: 0.26.0
2025-02-28 13:00:12,666:INFO:               tbats: 1.1.3
2025-02-28 13:00:12,666:INFO:            pmdarima: 2.0.4
2025-02-28 13:00:12,666:INFO:              psutil: 5.9.0
2025-02-28 13:00:12,666:INFO:          markupsafe: 2.1.5
2025-02-28 13:00:12,666:INFO:             pickle5: Not installed
2025-02-28 13:00:12,666:INFO:         cloudpickle: 3.1.1
2025-02-28 13:00:12,666:INFO:         deprecation: 2.1.0
2025-02-28 13:00:12,666:INFO:              xxhash: 3.5.0
2025-02-28 13:00:12,666:INFO:           wurlitzer: Not installed
2025-02-28 13:00:12,666:INFO:PyCaret optional dependencies:
2025-02-28 13:00:12,666:INFO:                shap: 0.44.1
2025-02-28 13:00:12,666:INFO:           interpret: 0.6.9
2025-02-28 13:00:12,666:INFO:                umap: 0.5.7
2025-02-28 13:00:12,667:INFO:     ydata_profiling: 4.12.2
2025-02-28 13:00:12,667:INFO:  explainerdashboard: 0.4.8
2025-02-28 13:00:12,667:INFO:             autoviz: Not installed
2025-02-28 13:00:12,667:INFO:           fairlearn: 0.7.0
2025-02-28 13:00:12,667:INFO:          deepchecks: Not installed
2025-02-28 13:00:12,667:INFO:             xgboost: 2.1.4
2025-02-28 13:00:12,667:INFO:            catboost: 1.2.7
2025-02-28 13:00:12,667:INFO:              kmodes: 0.12.2
2025-02-28 13:00:12,667:INFO:             mlxtend: 0.23.4
2025-02-28 13:00:12,667:INFO:       statsforecast: 1.5.0
2025-02-28 13:00:12,667:INFO:        tune_sklearn: Not installed
2025-02-28 13:00:12,667:INFO:                 ray: Not installed
2025-02-28 13:00:12,667:INFO:            hyperopt: 0.2.7
2025-02-28 13:00:12,667:INFO:              optuna: 4.2.0
2025-02-28 13:00:12,667:INFO:               skopt: 0.10.2
2025-02-28 13:00:12,667:INFO:              mlflow: 2.20.1
2025-02-28 13:00:12,667:INFO:              gradio: 5.15.0
2025-02-28 13:00:12,667:INFO:             fastapi: 0.115.8
2025-02-28 13:00:12,667:INFO:             uvicorn: 0.34.0
2025-02-28 13:00:12,667:INFO:              m2cgen: 0.10.0
2025-02-28 13:00:12,667:INFO:           evidently: 0.4.40
2025-02-28 13:00:12,667:INFO:               fugue: 0.8.7
2025-02-28 13:00:12,668:INFO:           streamlit: Not installed
2025-02-28 13:00:12,668:INFO:             prophet: Not installed
2025-02-28 13:00:12,668:INFO:None
2025-02-28 13:00:12,668:INFO:Set up data.
2025-02-28 13:00:12,671:INFO:Set up folding strategy.
2025-02-28 13:00:12,671:INFO:Set up train/test split.
2025-02-28 13:00:12,675:INFO:Set up index.
2025-02-28 13:00:12,675:INFO:Assigning column types.
2025-02-28 13:00:12,677:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 13:00:12,697:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,697:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,709:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,711:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,730:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,731:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,744:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,745:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,746:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 13:00:12,765:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,778:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,779:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,800:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:00:12,812:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,813:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,813:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 13:00:12,845:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,847:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,879:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:12,881:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:12,881:INFO:Preparing preprocessing pipeline...
2025-02-28 13:00:12,882:INFO:Set up simple imputation.
2025-02-28 13:00:12,883:INFO:Set up encoding of categorical features.
2025-02-28 13:00:12,914:INFO:Finished creating preprocessing pipeline.
2025-02-28 13:00:12,917:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 13:00:12,917:INFO:Creating final display dataframe.
2025-02-28 13:00:13,015:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              cc4d
2025-02-28 13:00:13,051:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:13,053:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:13,086:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:13,087:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:13,088:INFO:setup() successfully completed in 0.43s...............
2025-02-28 13:00:13,100:INFO:Initializing compare_models()
2025-02-28 13:00:13,100:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 13:00:13,100:INFO:Checking exceptions
2025-02-28 13:00:13,103:INFO:Preparing display monitor
2025-02-28 13:00:13,118:INFO:Initializing Logistic Regression
2025-02-28 13:00:13,119:INFO:Total runtime is 9.51687494913737e-06 minutes
2025-02-28 13:00:13,122:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:13,122:INFO:Initializing create_model()
2025-02-28 13:00:13,122:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:13,122:INFO:Checking exceptions
2025-02-28 13:00:13,122:INFO:Importing libraries
2025-02-28 13:00:13,122:INFO:Copying training dataset
2025-02-28 13:00:13,126:INFO:Defining folds
2025-02-28 13:00:13,126:INFO:Declaring metric variables
2025-02-28 13:00:13,127:INFO:Importing untrained model
2025-02-28 13:00:13,130:INFO:Logistic Regression Imported successfully
2025-02-28 13:00:13,136:INFO:Starting cross validation
2025-02-28 13:00:13,137:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:16,695:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,709:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,741:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,751:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,754:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,755:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,769:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,771:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,772:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,778:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,781:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,786:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,792:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,793:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,796:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,797:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,803:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:00:16,808:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,816:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:16,827:INFO:Calculating mean and std
2025-02-28 13:00:16,828:INFO:Creating metrics dataframe
2025-02-28 13:00:16,830:INFO:Uploading results into container
2025-02-28 13:00:16,830:INFO:Uploading model into container now
2025-02-28 13:00:16,830:INFO:_master_model_container: 1
2025-02-28 13:00:16,830:INFO:_display_container: 2
2025-02-28 13:00:16,831:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 13:00:16,831:INFO:create_model() successfully completed......................................
2025-02-28 13:00:16,960:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:16,960:INFO:Creating metrics dataframe
2025-02-28 13:00:16,964:INFO:Initializing K Neighbors Classifier
2025-02-28 13:00:16,964:INFO:Total runtime is 0.06410011847813925 minutes
2025-02-28 13:00:16,966:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:16,967:INFO:Initializing create_model()
2025-02-28 13:00:16,967:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:16,967:INFO:Checking exceptions
2025-02-28 13:00:16,967:INFO:Importing libraries
2025-02-28 13:00:16,967:INFO:Copying training dataset
2025-02-28 13:00:16,970:INFO:Defining folds
2025-02-28 13:00:16,970:INFO:Declaring metric variables
2025-02-28 13:00:16,973:INFO:Importing untrained model
2025-02-28 13:00:16,975:INFO:K Neighbors Classifier Imported successfully
2025-02-28 13:00:16,979:INFO:Starting cross validation
2025-02-28 13:00:16,980:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:19,793:INFO:Calculating mean and std
2025-02-28 13:00:19,794:INFO:Creating metrics dataframe
2025-02-28 13:00:19,796:INFO:Uploading results into container
2025-02-28 13:00:19,797:INFO:Uploading model into container now
2025-02-28 13:00:19,797:INFO:_master_model_container: 2
2025-02-28 13:00:19,797:INFO:_display_container: 2
2025-02-28 13:00:19,798:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 13:00:19,798:INFO:create_model() successfully completed......................................
2025-02-28 13:00:19,918:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:19,918:INFO:Creating metrics dataframe
2025-02-28 13:00:19,922:INFO:Initializing Naive Bayes
2025-02-28 13:00:19,922:INFO:Total runtime is 0.11340227921803793 minutes
2025-02-28 13:00:19,924:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:19,925:INFO:Initializing create_model()
2025-02-28 13:00:19,925:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:19,925:INFO:Checking exceptions
2025-02-28 13:00:19,925:INFO:Importing libraries
2025-02-28 13:00:19,925:INFO:Copying training dataset
2025-02-28 13:00:19,927:INFO:Defining folds
2025-02-28 13:00:19,928:INFO:Declaring metric variables
2025-02-28 13:00:19,930:INFO:Importing untrained model
2025-02-28 13:00:19,932:INFO:Naive Bayes Imported successfully
2025-02-28 13:00:19,936:INFO:Starting cross validation
2025-02-28 13:00:19,937:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:23,069:INFO:Calculating mean and std
2025-02-28 13:00:23,070:INFO:Creating metrics dataframe
2025-02-28 13:00:23,071:INFO:Uploading results into container
2025-02-28 13:00:23,071:INFO:Uploading model into container now
2025-02-28 13:00:23,072:INFO:_master_model_container: 3
2025-02-28 13:00:23,072:INFO:_display_container: 2
2025-02-28 13:00:23,072:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 13:00:23,072:INFO:create_model() successfully completed......................................
2025-02-28 13:00:23,195:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:23,197:INFO:Creating metrics dataframe
2025-02-28 13:00:23,200:INFO:Initializing Decision Tree Classifier
2025-02-28 13:00:23,200:INFO:Total runtime is 0.16803147395451865 minutes
2025-02-28 13:00:23,202:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:23,203:INFO:Initializing create_model()
2025-02-28 13:00:23,203:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:23,203:INFO:Checking exceptions
2025-02-28 13:00:23,203:INFO:Importing libraries
2025-02-28 13:00:23,203:INFO:Copying training dataset
2025-02-28 13:00:23,207:INFO:Defining folds
2025-02-28 13:00:23,207:INFO:Declaring metric variables
2025-02-28 13:00:23,210:INFO:Importing untrained model
2025-02-28 13:00:23,213:INFO:Decision Tree Classifier Imported successfully
2025-02-28 13:00:23,220:INFO:Starting cross validation
2025-02-28 13:00:23,221:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:25,328:INFO:Calculating mean and std
2025-02-28 13:00:25,329:INFO:Creating metrics dataframe
2025-02-28 13:00:25,329:INFO:Uploading results into container
2025-02-28 13:00:25,330:INFO:Uploading model into container now
2025-02-28 13:00:25,330:INFO:_master_model_container: 4
2025-02-28 13:00:25,330:INFO:_display_container: 2
2025-02-28 13:00:25,330:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 13:00:25,330:INFO:create_model() successfully completed......................................
2025-02-28 13:00:25,444:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:25,445:INFO:Creating metrics dataframe
2025-02-28 13:00:25,449:INFO:Initializing SVM - Linear Kernel
2025-02-28 13:00:25,449:INFO:Total runtime is 0.20550336837768557 minutes
2025-02-28 13:00:25,451:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:25,451:INFO:Initializing create_model()
2025-02-28 13:00:25,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:25,451:INFO:Checking exceptions
2025-02-28 13:00:25,451:INFO:Importing libraries
2025-02-28 13:00:25,451:INFO:Copying training dataset
2025-02-28 13:00:25,455:INFO:Defining folds
2025-02-28 13:00:25,455:INFO:Declaring metric variables
2025-02-28 13:00:25,458:INFO:Importing untrained model
2025-02-28 13:00:25,461:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 13:00:25,465:INFO:Starting cross validation
2025-02-28 13:00:25,466:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:25,568:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,573:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,585:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,586:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,588:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,590:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,602:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,602:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,602:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,602:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,605:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,621:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,629:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,629:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,631:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,631:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:25,640:INFO:Calculating mean and std
2025-02-28 13:00:25,641:INFO:Creating metrics dataframe
2025-02-28 13:00:25,642:INFO:Uploading results into container
2025-02-28 13:00:25,642:INFO:Uploading model into container now
2025-02-28 13:00:25,642:INFO:_master_model_container: 5
2025-02-28 13:00:25,642:INFO:_display_container: 2
2025-02-28 13:00:25,643:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 13:00:25,643:INFO:create_model() successfully completed......................................
2025-02-28 13:00:25,754:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:25,754:INFO:Creating metrics dataframe
2025-02-28 13:00:25,759:INFO:Initializing Ridge Classifier
2025-02-28 13:00:25,759:INFO:Total runtime is 0.2106738090515137 minutes
2025-02-28 13:00:25,760:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:25,760:INFO:Initializing create_model()
2025-02-28 13:00:25,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:25,761:INFO:Checking exceptions
2025-02-28 13:00:25,761:INFO:Importing libraries
2025-02-28 13:00:25,761:INFO:Copying training dataset
2025-02-28 13:00:25,763:INFO:Defining folds
2025-02-28 13:00:25,763:INFO:Declaring metric variables
2025-02-28 13:00:25,766:INFO:Importing untrained model
2025-02-28 13:00:25,768:INFO:Ridge Classifier Imported successfully
2025-02-28 13:00:25,773:INFO:Starting cross validation
2025-02-28 13:00:25,774:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:25,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,829:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,830:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,830:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,838:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,838:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,838:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,840:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,852:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,857:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:25,875:INFO:Calculating mean and std
2025-02-28 13:00:25,877:INFO:Creating metrics dataframe
2025-02-28 13:00:25,880:INFO:Uploading results into container
2025-02-28 13:00:25,881:INFO:Uploading model into container now
2025-02-28 13:00:25,881:INFO:_master_model_container: 6
2025-02-28 13:00:25,881:INFO:_display_container: 2
2025-02-28 13:00:25,881:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 13:00:25,881:INFO:create_model() successfully completed......................................
2025-02-28 13:00:25,998:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:25,998:INFO:Creating metrics dataframe
2025-02-28 13:00:26,003:INFO:Initializing Random Forest Classifier
2025-02-28 13:00:26,003:INFO:Total runtime is 0.21474105517069503 minutes
2025-02-28 13:00:26,005:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:26,005:INFO:Initializing create_model()
2025-02-28 13:00:26,005:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:26,005:INFO:Checking exceptions
2025-02-28 13:00:26,005:INFO:Importing libraries
2025-02-28 13:00:26,005:INFO:Copying training dataset
2025-02-28 13:00:26,008:INFO:Defining folds
2025-02-28 13:00:26,008:INFO:Declaring metric variables
2025-02-28 13:00:26,011:INFO:Importing untrained model
2025-02-28 13:00:26,014:INFO:Random Forest Classifier Imported successfully
2025-02-28 13:00:26,018:INFO:Starting cross validation
2025-02-28 13:00:26,019:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:26,420:INFO:Calculating mean and std
2025-02-28 13:00:26,421:INFO:Creating metrics dataframe
2025-02-28 13:00:26,423:INFO:Uploading results into container
2025-02-28 13:00:26,423:INFO:Uploading model into container now
2025-02-28 13:00:26,424:INFO:_master_model_container: 7
2025-02-28 13:00:26,424:INFO:_display_container: 2
2025-02-28 13:00:26,424:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 13:00:26,424:INFO:create_model() successfully completed......................................
2025-02-28 13:00:26,534:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:26,534:INFO:Creating metrics dataframe
2025-02-28 13:00:26,539:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 13:00:26,539:INFO:Total runtime is 0.2236805359522502 minutes
2025-02-28 13:00:26,541:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:26,541:INFO:Initializing create_model()
2025-02-28 13:00:26,541:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:26,541:INFO:Checking exceptions
2025-02-28 13:00:26,541:INFO:Importing libraries
2025-02-28 13:00:26,541:INFO:Copying training dataset
2025-02-28 13:00:26,544:INFO:Defining folds
2025-02-28 13:00:26,544:INFO:Declaring metric variables
2025-02-28 13:00:26,546:INFO:Importing untrained model
2025-02-28 13:00:26,549:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 13:00:26,553:INFO:Starting cross validation
2025-02-28 13:00:26,554:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:26,591:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,592:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,597:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,598:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,599:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,601:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,601:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,604:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:00:26,606:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,610:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,610:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,612:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,614:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,615:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,616:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,616:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,616:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,617:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,620:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,627:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,631:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:26,639:INFO:Calculating mean and std
2025-02-28 13:00:26,639:INFO:Creating metrics dataframe
2025-02-28 13:00:26,641:INFO:Uploading results into container
2025-02-28 13:00:26,641:INFO:Uploading model into container now
2025-02-28 13:00:26,641:INFO:_master_model_container: 8
2025-02-28 13:00:26,641:INFO:_display_container: 2
2025-02-28 13:00:26,642:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 13:00:26,642:INFO:create_model() successfully completed......................................
2025-02-28 13:00:26,759:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:26,759:INFO:Creating metrics dataframe
2025-02-28 13:00:26,764:INFO:Initializing Ada Boost Classifier
2025-02-28 13:00:26,764:INFO:Total runtime is 0.22743436495463057 minutes
2025-02-28 13:00:26,766:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:26,767:INFO:Initializing create_model()
2025-02-28 13:00:26,767:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:26,767:INFO:Checking exceptions
2025-02-28 13:00:26,767:INFO:Importing libraries
2025-02-28 13:00:26,767:INFO:Copying training dataset
2025-02-28 13:00:26,769:INFO:Defining folds
2025-02-28 13:00:26,769:INFO:Declaring metric variables
2025-02-28 13:00:26,772:INFO:Importing untrained model
2025-02-28 13:00:26,774:INFO:Ada Boost Classifier Imported successfully
2025-02-28 13:00:26,778:INFO:Starting cross validation
2025-02-28 13:00:26,779:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:26,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,816:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,817:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,820:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,820:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,821:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,822:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,829:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:00:26,962:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,969:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,984:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,987:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,991:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,995:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,999:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:26,999:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:27,003:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:27,005:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:27,014:INFO:Calculating mean and std
2025-02-28 13:00:27,015:INFO:Creating metrics dataframe
2025-02-28 13:00:27,016:INFO:Uploading results into container
2025-02-28 13:00:27,016:INFO:Uploading model into container now
2025-02-28 13:00:27,016:INFO:_master_model_container: 9
2025-02-28 13:00:27,016:INFO:_display_container: 2
2025-02-28 13:00:27,017:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 13:00:27,017:INFO:create_model() successfully completed......................................
2025-02-28 13:00:27,125:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:27,125:INFO:Creating metrics dataframe
2025-02-28 13:00:27,132:INFO:Initializing Gradient Boosting Classifier
2025-02-28 13:00:27,132:INFO:Total runtime is 0.2335558374722799 minutes
2025-02-28 13:00:27,134:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:27,134:INFO:Initializing create_model()
2025-02-28 13:00:27,134:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:27,135:INFO:Checking exceptions
2025-02-28 13:00:27,135:INFO:Importing libraries
2025-02-28 13:00:27,135:INFO:Copying training dataset
2025-02-28 13:00:27,137:INFO:Defining folds
2025-02-28 13:00:27,137:INFO:Declaring metric variables
2025-02-28 13:00:27,140:INFO:Importing untrained model
2025-02-28 13:00:27,143:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 13:00:27,147:INFO:Starting cross validation
2025-02-28 13:00:27,148:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:29,490:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,495:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,506:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,512:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,540:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,549:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,571:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,574:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,585:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,600:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,620:INFO:Calculating mean and std
2025-02-28 13:00:29,620:INFO:Creating metrics dataframe
2025-02-28 13:00:29,622:INFO:Uploading results into container
2025-02-28 13:00:29,623:INFO:Uploading model into container now
2025-02-28 13:00:29,623:INFO:_master_model_container: 10
2025-02-28 13:00:29,623:INFO:_display_container: 2
2025-02-28 13:00:29,623:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 13:00:29,623:INFO:create_model() successfully completed......................................
2025-02-28 13:00:29,728:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:29,728:INFO:Creating metrics dataframe
2025-02-28 13:00:29,733:INFO:Initializing Linear Discriminant Analysis
2025-02-28 13:00:29,734:INFO:Total runtime is 0.276926875114441 minutes
2025-02-28 13:00:29,736:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:29,736:INFO:Initializing create_model()
2025-02-28 13:00:29,736:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:29,736:INFO:Checking exceptions
2025-02-28 13:00:29,737:INFO:Importing libraries
2025-02-28 13:00:29,737:INFO:Copying training dataset
2025-02-28 13:00:29,739:INFO:Defining folds
2025-02-28 13:00:29,739:INFO:Declaring metric variables
2025-02-28 13:00:29,743:INFO:Importing untrained model
2025-02-28 13:00:29,746:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 13:00:29,753:INFO:Starting cross validation
2025-02-28 13:00:29,755:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:29,804:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,805:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,805:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,807:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,809:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,811:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,816:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,823:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,825:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,829:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:00:29,839:INFO:Calculating mean and std
2025-02-28 13:00:29,840:INFO:Creating metrics dataframe
2025-02-28 13:00:29,841:INFO:Uploading results into container
2025-02-28 13:00:29,841:INFO:Uploading model into container now
2025-02-28 13:00:29,841:INFO:_master_model_container: 11
2025-02-28 13:00:29,841:INFO:_display_container: 2
2025-02-28 13:00:29,842:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 13:00:29,842:INFO:create_model() successfully completed......................................
2025-02-28 13:00:29,949:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:29,949:INFO:Creating metrics dataframe
2025-02-28 13:00:29,955:INFO:Initializing Extra Trees Classifier
2025-02-28 13:00:29,955:INFO:Total runtime is 0.2806074579556784 minutes
2025-02-28 13:00:29,957:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:29,957:INFO:Initializing create_model()
2025-02-28 13:00:29,957:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:29,957:INFO:Checking exceptions
2025-02-28 13:00:29,958:INFO:Importing libraries
2025-02-28 13:00:29,958:INFO:Copying training dataset
2025-02-28 13:00:29,960:INFO:Defining folds
2025-02-28 13:00:29,960:INFO:Declaring metric variables
2025-02-28 13:00:29,963:INFO:Importing untrained model
2025-02-28 13:00:29,965:INFO:Extra Trees Classifier Imported successfully
2025-02-28 13:00:29,971:INFO:Starting cross validation
2025-02-28 13:00:29,972:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:30,341:INFO:Calculating mean and std
2025-02-28 13:00:30,342:INFO:Creating metrics dataframe
2025-02-28 13:00:30,344:INFO:Uploading results into container
2025-02-28 13:00:30,345:INFO:Uploading model into container now
2025-02-28 13:00:30,345:INFO:_master_model_container: 12
2025-02-28 13:00:30,345:INFO:_display_container: 2
2025-02-28 13:00:30,345:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 13:00:30,346:INFO:create_model() successfully completed......................................
2025-02-28 13:00:30,456:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:30,456:INFO:Creating metrics dataframe
2025-02-28 13:00:30,460:INFO:Initializing Extreme Gradient Boosting
2025-02-28 13:00:30,460:INFO:Total runtime is 0.2890308777491252 minutes
2025-02-28 13:00:30,463:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:30,463:INFO:Initializing create_model()
2025-02-28 13:00:30,463:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:30,463:INFO:Checking exceptions
2025-02-28 13:00:30,463:INFO:Importing libraries
2025-02-28 13:00:30,463:INFO:Copying training dataset
2025-02-28 13:00:30,466:INFO:Defining folds
2025-02-28 13:00:30,466:INFO:Declaring metric variables
2025-02-28 13:00:30,469:INFO:Importing untrained model
2025-02-28 13:00:30,471:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 13:00:30,475:INFO:Starting cross validation
2025-02-28 13:00:30,476:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:31,387:INFO:Calculating mean and std
2025-02-28 13:00:31,388:INFO:Creating metrics dataframe
2025-02-28 13:00:31,390:INFO:Uploading results into container
2025-02-28 13:00:31,391:INFO:Uploading model into container now
2025-02-28 13:00:31,391:INFO:_master_model_container: 13
2025-02-28 13:00:31,391:INFO:_display_container: 2
2025-02-28 13:00:31,392:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 13:00:31,393:INFO:create_model() successfully completed......................................
2025-02-28 13:00:31,528:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:31,528:INFO:Creating metrics dataframe
2025-02-28 13:00:31,533:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 13:00:31,534:INFO:Total runtime is 0.306930700937907 minutes
2025-02-28 13:00:31,537:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:31,537:INFO:Initializing create_model()
2025-02-28 13:00:31,537:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:31,537:INFO:Checking exceptions
2025-02-28 13:00:31,537:INFO:Importing libraries
2025-02-28 13:00:31,537:INFO:Copying training dataset
2025-02-28 13:00:31,540:INFO:Defining folds
2025-02-28 13:00:31,540:INFO:Declaring metric variables
2025-02-28 13:00:31,544:INFO:Importing untrained model
2025-02-28 13:00:31,547:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:00:31,551:INFO:Starting cross validation
2025-02-28 13:00:31,552:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:39,491:INFO:Calculating mean and std
2025-02-28 13:00:39,492:INFO:Creating metrics dataframe
2025-02-28 13:00:39,494:INFO:Uploading results into container
2025-02-28 13:00:39,494:INFO:Uploading model into container now
2025-02-28 13:00:39,495:INFO:_master_model_container: 14
2025-02-28 13:00:39,495:INFO:_display_container: 2
2025-02-28 13:00:39,495:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:00:39,495:INFO:create_model() successfully completed......................................
2025-02-28 13:00:39,625:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:39,625:INFO:Creating metrics dataframe
2025-02-28 13:00:39,630:INFO:Initializing CatBoost Classifier
2025-02-28 13:00:39,630:INFO:Total runtime is 0.44186822970708217 minutes
2025-02-28 13:00:39,634:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:39,634:INFO:Initializing create_model()
2025-02-28 13:00:39,634:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:39,634:INFO:Checking exceptions
2025-02-28 13:00:39,634:INFO:Importing libraries
2025-02-28 13:00:39,634:INFO:Copying training dataset
2025-02-28 13:00:39,637:INFO:Defining folds
2025-02-28 13:00:39,637:INFO:Declaring metric variables
2025-02-28 13:00:39,639:INFO:Importing untrained model
2025-02-28 13:00:39,641:INFO:CatBoost Classifier Imported successfully
2025-02-28 13:00:39,645:INFO:Starting cross validation
2025-02-28 13:00:39,646:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:46,614:INFO:Calculating mean and std
2025-02-28 13:00:46,615:INFO:Creating metrics dataframe
2025-02-28 13:00:46,616:INFO:Uploading results into container
2025-02-28 13:00:46,616:INFO:Uploading model into container now
2025-02-28 13:00:46,617:INFO:_master_model_container: 15
2025-02-28 13:00:46,617:INFO:_display_container: 2
2025-02-28 13:00:46,617:INFO:<catboost.core.CatBoostClassifier object at 0x0000021913E86830>
2025-02-28 13:00:46,617:INFO:create_model() successfully completed......................................
2025-02-28 13:00:46,726:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:46,726:INFO:Creating metrics dataframe
2025-02-28 13:00:46,731:INFO:Initializing Dummy Classifier
2025-02-28 13:00:46,731:INFO:Total runtime is 0.5602180043856304 minutes
2025-02-28 13:00:46,733:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:46,734:INFO:Initializing create_model()
2025-02-28 13:00:46,734:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021913512830>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:46,734:INFO:Checking exceptions
2025-02-28 13:00:46,734:INFO:Importing libraries
2025-02-28 13:00:46,734:INFO:Copying training dataset
2025-02-28 13:00:46,736:INFO:Defining folds
2025-02-28 13:00:46,737:INFO:Declaring metric variables
2025-02-28 13:00:46,739:INFO:Importing untrained model
2025-02-28 13:00:46,741:INFO:Dummy Classifier Imported successfully
2025-02-28 13:00:46,745:INFO:Starting cross validation
2025-02-28 13:00:46,746:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:46,792:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,793:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,793:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,798:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,801:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,802:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,802:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,804:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,807:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,809:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:00:46,819:INFO:Calculating mean and std
2025-02-28 13:00:46,819:INFO:Creating metrics dataframe
2025-02-28 13:00:46,820:INFO:Uploading results into container
2025-02-28 13:00:46,820:INFO:Uploading model into container now
2025-02-28 13:00:46,820:INFO:_master_model_container: 16
2025-02-28 13:00:46,820:INFO:_display_container: 2
2025-02-28 13:00:46,820:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 13:00:46,820:INFO:create_model() successfully completed......................................
2025-02-28 13:00:46,929:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:46,929:INFO:Creating metrics dataframe
2025-02-28 13:00:46,936:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 13:00:46,942:INFO:Initializing create_model()
2025-02-28 13:00:46,942:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:46,942:INFO:Checking exceptions
2025-02-28 13:00:46,943:INFO:Importing libraries
2025-02-28 13:00:46,943:INFO:Copying training dataset
2025-02-28 13:00:46,945:INFO:Defining folds
2025-02-28 13:00:46,945:INFO:Declaring metric variables
2025-02-28 13:00:46,945:INFO:Importing untrained model
2025-02-28 13:00:46,945:INFO:Declaring custom model
2025-02-28 13:00:46,945:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:00:46,946:INFO:Cross validation set to False
2025-02-28 13:00:46,946:INFO:Fitting Model
2025-02-28 13:00:46,969:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.
2025-02-28 13:00:46,969:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 20
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.773639
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.804411
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.765257
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.800944
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.827239
2025-02-28 13:00:46,969:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:00:47,279:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:00:47,279:INFO:create_model() successfully completed......................................
2025-02-28 13:00:47,417:INFO:_master_model_container: 16
2025-02-28 13:00:47,418:INFO:_display_container: 2
2025-02-28 13:00:47,418:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:00:47,418:INFO:compare_models() successfully completed......................................
2025-02-28 13:00:47,432:INFO:Initializing plot_model()
2025-02-28 13:00:47,432:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, system=True)
2025-02-28 13:00:47,432:INFO:Checking exceptions
2025-02-28 13:00:47,434:INFO:Preloading libraries
2025-02-28 13:00:47,471:INFO:Copying training dataset
2025-02-28 13:00:47,471:INFO:Plot type: confusion_matrix
2025-02-28 13:00:47,594:INFO:Fitting Model
2025-02-28 13:00:47,594:INFO:Scoring test/hold-out set
2025-02-28 13:00:47,726:INFO:Visual Rendered Successfully
2025-02-28 13:00:47,843:INFO:plot_model() successfully completed......................................
2025-02-28 13:00:47,855:INFO:Initializing plot_model()
2025-02-28 13:00:47,855:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, system=True)
2025-02-28 13:00:47,855:INFO:Checking exceptions
2025-02-28 13:00:47,858:INFO:Preloading libraries
2025-02-28 13:00:47,900:INFO:Copying training dataset
2025-02-28 13:00:47,901:INFO:Plot type: auc
2025-02-28 13:00:48,023:INFO:Fitting Model
2025-02-28 13:00:48,023:INFO:Scoring test/hold-out set
2025-02-28 13:00:48,164:INFO:Visual Rendered Successfully
2025-02-28 13:00:48,277:INFO:plot_model() successfully completed......................................
2025-02-28 13:00:48,298:INFO:Initializing plot_model()
2025-02-28 13:00:48,298:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021913E5B5B0>, system=True)
2025-02-28 13:00:48,298:INFO:Checking exceptions
2025-02-28 13:00:48,300:INFO:Preloading libraries
2025-02-28 13:00:48,341:INFO:Copying training dataset
2025-02-28 13:00:48,341:INFO:Plot type: feature
2025-02-28 13:00:48,341:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 13:00:48,462:INFO:Visual Rendered Successfully
2025-02-28 13:00:48,576:INFO:plot_model() successfully completed......................................
2025-02-28 13:00:48,603:INFO:PyCaret RegressionExperiment
2025-02-28 13:00:48,603:INFO:Logging name: reg-default-name
2025-02-28 13:00:48,603:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 13:00:48,603:INFO:version 3.3.2
2025-02-28 13:00:48,603:INFO:Initializing setup()
2025-02-28 13:00:48,603:INFO:self.USI: cbca
2025-02-28 13:00:48,603:INFO:self._variable_keys: {'transform_target_param', 'gpu_param', 'target_param', 'exp_id', 'data', 'USI', 'gpu_n_jobs_param', '_ml_usecase', 'y_train', 'pipeline', '_available_plots', 'seed', 'X_train', 'fold_generator', 'logging_param', 'X', 'fold_shuffle_param', 'html_param', 'idx', 'exp_name_log', 'memory', 'y', 'fold_groups_param', 'log_plots_param', 'y_test', 'X_test', 'n_jobs_param'}
2025-02-28 13:00:48,603:INFO:Checking environment
2025-02-28 13:00:48,603:INFO:python_version: 3.10.16
2025-02-28 13:00:48,603:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 13:00:48,603:INFO:machine: AMD64
2025-02-28 13:00:48,603:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 13:00:48,603:INFO:Memory: svmem(total=34200334336, available=11507613696, percent=66.4, used=22692720640, free=11507613696)
2025-02-28 13:00:48,604:INFO:Physical Core: 24
2025-02-28 13:00:48,604:INFO:Logical Core: 32
2025-02-28 13:00:48,604:INFO:Checking libraries
2025-02-28 13:00:48,604:INFO:System:
2025-02-28 13:00:48,604:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 13:00:48,604:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 13:00:48,604:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 13:00:48,604:INFO:PyCaret required dependencies:
2025-02-28 13:00:48,604:INFO:                 pip: 25.0
2025-02-28 13:00:48,604:INFO:          setuptools: 75.8.0
2025-02-28 13:00:48,604:INFO:             pycaret: 3.3.2
2025-02-28 13:00:48,604:INFO:             IPython: 8.30.0
2025-02-28 13:00:48,604:INFO:          ipywidgets: 8.1.5
2025-02-28 13:00:48,605:INFO:                tqdm: 4.67.1
2025-02-28 13:00:48,605:INFO:               numpy: 1.26.4
2025-02-28 13:00:48,605:INFO:              pandas: 2.1.4
2025-02-28 13:00:48,605:INFO:              jinja2: 3.1.5
2025-02-28 13:00:48,605:INFO:               scipy: 1.11.4
2025-02-28 13:00:48,605:INFO:              joblib: 1.3.2
2025-02-28 13:00:48,605:INFO:             sklearn: 1.4.2
2025-02-28 13:00:48,605:INFO:                pyod: 2.0.3
2025-02-28 13:00:48,605:INFO:            imblearn: 0.13.0
2025-02-28 13:00:48,605:INFO:   category_encoders: 2.7.0
2025-02-28 13:00:48,605:INFO:            lightgbm: 4.5.0
2025-02-28 13:00:48,605:INFO:               numba: 0.61.0
2025-02-28 13:00:48,605:INFO:            requests: 2.32.3
2025-02-28 13:00:48,605:INFO:          matplotlib: 3.7.5
2025-02-28 13:00:48,605:INFO:          scikitplot: 0.3.7
2025-02-28 13:00:48,605:INFO:         yellowbrick: 1.5
2025-02-28 13:00:48,606:INFO:              plotly: 5.24.1
2025-02-28 13:00:48,606:INFO:    plotly-resampler: Not installed
2025-02-28 13:00:48,606:INFO:             kaleido: 0.2.1
2025-02-28 13:00:48,606:INFO:           schemdraw: 0.15
2025-02-28 13:00:48,606:INFO:         statsmodels: 0.14.4
2025-02-28 13:00:48,606:INFO:              sktime: 0.26.0
2025-02-28 13:00:48,606:INFO:               tbats: 1.1.3
2025-02-28 13:00:48,606:INFO:            pmdarima: 2.0.4
2025-02-28 13:00:48,606:INFO:              psutil: 5.9.0
2025-02-28 13:00:48,606:INFO:          markupsafe: 2.1.5
2025-02-28 13:00:48,606:INFO:             pickle5: Not installed
2025-02-28 13:00:48,606:INFO:         cloudpickle: 3.1.1
2025-02-28 13:00:48,606:INFO:         deprecation: 2.1.0
2025-02-28 13:00:48,606:INFO:              xxhash: 3.5.0
2025-02-28 13:00:48,606:INFO:           wurlitzer: Not installed
2025-02-28 13:00:48,606:INFO:PyCaret optional dependencies:
2025-02-28 13:00:48,606:INFO:                shap: 0.44.1
2025-02-28 13:00:48,606:INFO:           interpret: 0.6.9
2025-02-28 13:00:48,606:INFO:                umap: 0.5.7
2025-02-28 13:00:48,606:INFO:     ydata_profiling: 4.12.2
2025-02-28 13:00:48,607:INFO:  explainerdashboard: 0.4.8
2025-02-28 13:00:48,607:INFO:             autoviz: Not installed
2025-02-28 13:00:48,607:INFO:           fairlearn: 0.7.0
2025-02-28 13:00:48,607:INFO:          deepchecks: Not installed
2025-02-28 13:00:48,607:INFO:             xgboost: 2.1.4
2025-02-28 13:00:48,607:INFO:            catboost: 1.2.7
2025-02-28 13:00:48,607:INFO:              kmodes: 0.12.2
2025-02-28 13:00:48,607:INFO:             mlxtend: 0.23.4
2025-02-28 13:00:48,607:INFO:       statsforecast: 1.5.0
2025-02-28 13:00:48,607:INFO:        tune_sklearn: Not installed
2025-02-28 13:00:48,607:INFO:                 ray: Not installed
2025-02-28 13:00:48,607:INFO:            hyperopt: 0.2.7
2025-02-28 13:00:48,607:INFO:              optuna: 4.2.0
2025-02-28 13:00:48,607:INFO:               skopt: 0.10.2
2025-02-28 13:00:48,607:INFO:              mlflow: 2.20.1
2025-02-28 13:00:48,607:INFO:              gradio: 5.15.0
2025-02-28 13:00:48,607:INFO:             fastapi: 0.115.8
2025-02-28 13:00:48,608:INFO:             uvicorn: 0.34.0
2025-02-28 13:00:48,608:INFO:              m2cgen: 0.10.0
2025-02-28 13:00:48,608:INFO:           evidently: 0.4.40
2025-02-28 13:00:48,608:INFO:               fugue: 0.8.7
2025-02-28 13:00:48,608:INFO:           streamlit: Not installed
2025-02-28 13:00:48,608:INFO:             prophet: Not installed
2025-02-28 13:00:48,608:INFO:None
2025-02-28 13:00:48,608:INFO:Set up data.
2025-02-28 13:00:48,611:INFO:Set up folding strategy.
2025-02-28 13:00:48,611:INFO:Set up train/test split.
2025-02-28 13:00:48,614:INFO:Set up index.
2025-02-28 13:00:48,614:INFO:Assigning column types.
2025-02-28 13:00:48,616:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 13:00:48,616:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,618:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,620:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,646:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,664:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,665:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,666:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,666:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,668:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,670:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,697:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,716:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,716:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,718:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,718:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 13:00:48,720:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,722:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,748:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,767:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,768:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,769:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,771:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,773:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,799:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,819:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,820:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,821:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,821:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 13:00:48,825:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,852:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,871:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,871:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,872:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,876:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,903:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,922:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,922:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,923:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:48,923:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 13:00:48,954:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,972:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:48,973:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:48,974:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,002:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:49,023:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:00:49,024:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,025:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,025:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 13:00:49,056:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:49,075:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,077:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,106:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:00:49,125:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,126:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,126:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 13:00:49,175:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,176:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,228:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,229:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,229:INFO:Preparing preprocessing pipeline...
2025-02-28 13:00:49,229:INFO:Set up simple imputation.
2025-02-28 13:00:49,231:INFO:Set up encoding of categorical features.
2025-02-28 13:00:49,259:INFO:Finished creating preprocessing pipeline.
2025-02-28 13:00:49,262:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 13:00:49,262:INFO:Creating final display dataframe.
2025-02-28 13:00:49,375:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              cbca
2025-02-28 13:00:49,430:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,431:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,482:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:00:49,484:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:00:49,484:INFO:setup() successfully completed in 0.88s...............
2025-02-28 13:00:49,503:INFO:Initializing compare_models()
2025-02-28 13:00:49,503:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 13:00:49,503:INFO:Checking exceptions
2025-02-28 13:00:49,505:INFO:Preparing display monitor
2025-02-28 13:00:49,521:INFO:Initializing Linear Regression
2025-02-28 13:00:49,521:INFO:Total runtime is 0.0 minutes
2025-02-28 13:00:49,523:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:49,523:INFO:Initializing create_model()
2025-02-28 13:00:49,523:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:49,524:INFO:Checking exceptions
2025-02-28 13:00:49,524:INFO:Importing libraries
2025-02-28 13:00:49,524:INFO:Copying training dataset
2025-02-28 13:00:49,528:INFO:Defining folds
2025-02-28 13:00:49,529:INFO:Declaring metric variables
2025-02-28 13:00:49,530:INFO:Importing untrained model
2025-02-28 13:00:49,533:INFO:Linear Regression Imported successfully
2025-02-28 13:00:49,539:INFO:Starting cross validation
2025-02-28 13:00:49,540:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:49,650:INFO:Calculating mean and std
2025-02-28 13:00:49,651:INFO:Creating metrics dataframe
2025-02-28 13:00:49,652:INFO:Uploading results into container
2025-02-28 13:00:49,653:INFO:Uploading model into container now
2025-02-28 13:00:49,653:INFO:_master_model_container: 1
2025-02-28 13:00:49,653:INFO:_display_container: 2
2025-02-28 13:00:49,653:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 13:00:49,653:INFO:create_model() successfully completed......................................
2025-02-28 13:00:49,763:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:49,763:INFO:Creating metrics dataframe
2025-02-28 13:00:49,766:INFO:Initializing Lasso Regression
2025-02-28 13:00:49,766:INFO:Total runtime is 0.004083216190338135 minutes
2025-02-28 13:00:49,768:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:49,768:INFO:Initializing create_model()
2025-02-28 13:00:49,768:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:49,769:INFO:Checking exceptions
2025-02-28 13:00:49,769:INFO:Importing libraries
2025-02-28 13:00:49,769:INFO:Copying training dataset
2025-02-28 13:00:49,771:INFO:Defining folds
2025-02-28 13:00:49,771:INFO:Declaring metric variables
2025-02-28 13:00:49,772:INFO:Importing untrained model
2025-02-28 13:00:49,775:INFO:Lasso Regression Imported successfully
2025-02-28 13:00:49,778:INFO:Starting cross validation
2025-02-28 13:00:49,779:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:49,853:INFO:Calculating mean and std
2025-02-28 13:00:49,853:INFO:Creating metrics dataframe
2025-02-28 13:00:49,854:INFO:Uploading results into container
2025-02-28 13:00:49,854:INFO:Uploading model into container now
2025-02-28 13:00:49,855:INFO:_master_model_container: 2
2025-02-28 13:00:49,855:INFO:_display_container: 2
2025-02-28 13:00:49,855:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 13:00:49,855:INFO:create_model() successfully completed......................................
2025-02-28 13:00:49,968:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:49,968:INFO:Creating metrics dataframe
2025-02-28 13:00:49,972:INFO:Initializing Ridge Regression
2025-02-28 13:00:49,972:INFO:Total runtime is 0.0075266361236572266 minutes
2025-02-28 13:00:49,974:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:49,974:INFO:Initializing create_model()
2025-02-28 13:00:49,974:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:49,974:INFO:Checking exceptions
2025-02-28 13:00:49,974:INFO:Importing libraries
2025-02-28 13:00:49,974:INFO:Copying training dataset
2025-02-28 13:00:49,977:INFO:Defining folds
2025-02-28 13:00:49,977:INFO:Declaring metric variables
2025-02-28 13:00:49,979:INFO:Importing untrained model
2025-02-28 13:00:49,981:INFO:Ridge Regression Imported successfully
2025-02-28 13:00:49,985:INFO:Starting cross validation
2025-02-28 13:00:49,986:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,056:INFO:Calculating mean and std
2025-02-28 13:00:50,056:INFO:Creating metrics dataframe
2025-02-28 13:00:50,057:INFO:Uploading results into container
2025-02-28 13:00:50,057:INFO:Uploading model into container now
2025-02-28 13:00:50,058:INFO:_master_model_container: 3
2025-02-28 13:00:50,058:INFO:_display_container: 2
2025-02-28 13:00:50,058:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 13:00:50,058:INFO:create_model() successfully completed......................................
2025-02-28 13:00:50,174:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:50,174:INFO:Creating metrics dataframe
2025-02-28 13:00:50,178:INFO:Initializing Elastic Net
2025-02-28 13:00:50,178:INFO:Total runtime is 0.010950211683909097 minutes
2025-02-28 13:00:50,180:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:50,180:INFO:Initializing create_model()
2025-02-28 13:00:50,180:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:50,181:INFO:Checking exceptions
2025-02-28 13:00:50,181:INFO:Importing libraries
2025-02-28 13:00:50,181:INFO:Copying training dataset
2025-02-28 13:00:50,183:INFO:Defining folds
2025-02-28 13:00:50,183:INFO:Declaring metric variables
2025-02-28 13:00:50,185:INFO:Importing untrained model
2025-02-28 13:00:50,187:INFO:Elastic Net Imported successfully
2025-02-28 13:00:50,191:INFO:Starting cross validation
2025-02-28 13:00:50,192:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,273:INFO:Calculating mean and std
2025-02-28 13:00:50,274:INFO:Creating metrics dataframe
2025-02-28 13:00:50,275:INFO:Uploading results into container
2025-02-28 13:00:50,276:INFO:Uploading model into container now
2025-02-28 13:00:50,276:INFO:_master_model_container: 4
2025-02-28 13:00:50,276:INFO:_display_container: 2
2025-02-28 13:00:50,276:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 13:00:50,276:INFO:create_model() successfully completed......................................
2025-02-28 13:00:50,392:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:50,393:INFO:Creating metrics dataframe
2025-02-28 13:00:50,397:INFO:Initializing Least Angle Regression
2025-02-28 13:00:50,397:INFO:Total runtime is 0.014599299430847167 minutes
2025-02-28 13:00:50,399:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:50,399:INFO:Initializing create_model()
2025-02-28 13:00:50,399:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:50,399:INFO:Checking exceptions
2025-02-28 13:00:50,399:INFO:Importing libraries
2025-02-28 13:00:50,399:INFO:Copying training dataset
2025-02-28 13:00:50,402:INFO:Defining folds
2025-02-28 13:00:50,402:INFO:Declaring metric variables
2025-02-28 13:00:50,404:INFO:Importing untrained model
2025-02-28 13:00:50,407:INFO:Least Angle Regression Imported successfully
2025-02-28 13:00:50,411:INFO:Starting cross validation
2025-02-28 13:00:50,412:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,489:INFO:Calculating mean and std
2025-02-28 13:00:50,489:INFO:Creating metrics dataframe
2025-02-28 13:00:50,490:INFO:Uploading results into container
2025-02-28 13:00:50,490:INFO:Uploading model into container now
2025-02-28 13:00:50,490:INFO:_master_model_container: 5
2025-02-28 13:00:50,490:INFO:_display_container: 2
2025-02-28 13:00:50,490:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 13:00:50,490:INFO:create_model() successfully completed......................................
2025-02-28 13:00:50,599:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:50,599:INFO:Creating metrics dataframe
2025-02-28 13:00:50,604:INFO:Initializing Lasso Least Angle Regression
2025-02-28 13:00:50,604:INFO:Total runtime is 0.018060855070749917 minutes
2025-02-28 13:00:50,606:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:50,606:INFO:Initializing create_model()
2025-02-28 13:00:50,606:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:50,606:INFO:Checking exceptions
2025-02-28 13:00:50,606:INFO:Importing libraries
2025-02-28 13:00:50,606:INFO:Copying training dataset
2025-02-28 13:00:50,609:INFO:Defining folds
2025-02-28 13:00:50,609:INFO:Declaring metric variables
2025-02-28 13:00:50,612:INFO:Importing untrained model
2025-02-28 13:00:50,613:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 13:00:50,618:INFO:Starting cross validation
2025-02-28 13:00:50,619:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,691:INFO:Calculating mean and std
2025-02-28 13:00:50,691:INFO:Creating metrics dataframe
2025-02-28 13:00:50,692:INFO:Uploading results into container
2025-02-28 13:00:50,692:INFO:Uploading model into container now
2025-02-28 13:00:50,693:INFO:_master_model_container: 6
2025-02-28 13:00:50,693:INFO:_display_container: 2
2025-02-28 13:00:50,693:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 13:00:50,693:INFO:create_model() successfully completed......................................
2025-02-28 13:00:50,806:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:50,806:INFO:Creating metrics dataframe
2025-02-28 13:00:50,811:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 13:00:50,811:INFO:Total runtime is 0.0215025266011556 minutes
2025-02-28 13:00:50,813:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:50,813:INFO:Initializing create_model()
2025-02-28 13:00:50,813:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:50,813:INFO:Checking exceptions
2025-02-28 13:00:50,813:INFO:Importing libraries
2025-02-28 13:00:50,813:INFO:Copying training dataset
2025-02-28 13:00:50,816:INFO:Defining folds
2025-02-28 13:00:50,817:INFO:Declaring metric variables
2025-02-28 13:00:50,819:INFO:Importing untrained model
2025-02-28 13:00:50,822:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 13:00:50,826:INFO:Starting cross validation
2025-02-28 13:00:50,827:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:50,907:INFO:Calculating mean and std
2025-02-28 13:00:50,908:INFO:Creating metrics dataframe
2025-02-28 13:00:50,908:INFO:Uploading results into container
2025-02-28 13:00:50,908:INFO:Uploading model into container now
2025-02-28 13:00:50,908:INFO:_master_model_container: 7
2025-02-28 13:00:50,908:INFO:_display_container: 2
2025-02-28 13:00:50,909:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 13:00:50,909:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,021:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,021:INFO:Creating metrics dataframe
2025-02-28 13:00:51,025:INFO:Initializing Bayesian Ridge
2025-02-28 13:00:51,025:INFO:Total runtime is 0.02508093516031901 minutes
2025-02-28 13:00:51,027:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,027:INFO:Initializing create_model()
2025-02-28 13:00:51,027:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,027:INFO:Checking exceptions
2025-02-28 13:00:51,027:INFO:Importing libraries
2025-02-28 13:00:51,027:INFO:Copying training dataset
2025-02-28 13:00:51,031:INFO:Defining folds
2025-02-28 13:00:51,031:INFO:Declaring metric variables
2025-02-28 13:00:51,033:INFO:Importing untrained model
2025-02-28 13:00:51,036:INFO:Bayesian Ridge Imported successfully
2025-02-28 13:00:51,039:INFO:Starting cross validation
2025-02-28 13:00:51,040:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:51,124:INFO:Calculating mean and std
2025-02-28 13:00:51,124:INFO:Creating metrics dataframe
2025-02-28 13:00:51,125:INFO:Uploading results into container
2025-02-28 13:00:51,125:INFO:Uploading model into container now
2025-02-28 13:00:51,126:INFO:_master_model_container: 8
2025-02-28 13:00:51,126:INFO:_display_container: 2
2025-02-28 13:00:51,126:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 13:00:51,126:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,233:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,234:INFO:Creating metrics dataframe
2025-02-28 13:00:51,238:INFO:Initializing Passive Aggressive Regressor
2025-02-28 13:00:51,238:INFO:Total runtime is 0.02862518628438314 minutes
2025-02-28 13:00:51,241:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,241:INFO:Initializing create_model()
2025-02-28 13:00:51,241:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,241:INFO:Checking exceptions
2025-02-28 13:00:51,241:INFO:Importing libraries
2025-02-28 13:00:51,241:INFO:Copying training dataset
2025-02-28 13:00:51,244:INFO:Defining folds
2025-02-28 13:00:51,244:INFO:Declaring metric variables
2025-02-28 13:00:51,246:INFO:Importing untrained model
2025-02-28 13:00:51,248:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 13:00:51,252:INFO:Starting cross validation
2025-02-28 13:00:51,253:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:51,327:INFO:Calculating mean and std
2025-02-28 13:00:51,327:INFO:Creating metrics dataframe
2025-02-28 13:00:51,328:INFO:Uploading results into container
2025-02-28 13:00:51,328:INFO:Uploading model into container now
2025-02-28 13:00:51,329:INFO:_master_model_container: 9
2025-02-28 13:00:51,329:INFO:_display_container: 2
2025-02-28 13:00:51,329:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 13:00:51,329:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,438:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,438:INFO:Creating metrics dataframe
2025-02-28 13:00:51,443:INFO:Initializing Huber Regressor
2025-02-28 13:00:51,443:INFO:Total runtime is 0.032040309906005864 minutes
2025-02-28 13:00:51,445:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,445:INFO:Initializing create_model()
2025-02-28 13:00:51,445:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,445:INFO:Checking exceptions
2025-02-28 13:00:51,445:INFO:Importing libraries
2025-02-28 13:00:51,445:INFO:Copying training dataset
2025-02-28 13:00:51,448:INFO:Defining folds
2025-02-28 13:00:51,448:INFO:Declaring metric variables
2025-02-28 13:00:51,450:INFO:Importing untrained model
2025-02-28 13:00:51,453:INFO:Huber Regressor Imported successfully
2025-02-28 13:00:51,457:INFO:Starting cross validation
2025-02-28 13:00:51,458:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:51,516:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,520:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,521:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,522:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,527:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,529:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,532:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,533:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,534:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,542:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:00:51,560:INFO:Calculating mean and std
2025-02-28 13:00:51,560:INFO:Creating metrics dataframe
2025-02-28 13:00:51,562:INFO:Uploading results into container
2025-02-28 13:00:51,562:INFO:Uploading model into container now
2025-02-28 13:00:51,562:INFO:_master_model_container: 10
2025-02-28 13:00:51,562:INFO:_display_container: 2
2025-02-28 13:00:51,563:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 13:00:51,563:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,677:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,677:INFO:Creating metrics dataframe
2025-02-28 13:00:51,681:INFO:Initializing K Neighbors Regressor
2025-02-28 13:00:51,681:INFO:Total runtime is 0.03601033290227255 minutes
2025-02-28 13:00:51,684:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,684:INFO:Initializing create_model()
2025-02-28 13:00:51,684:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,684:INFO:Checking exceptions
2025-02-28 13:00:51,684:INFO:Importing libraries
2025-02-28 13:00:51,684:INFO:Copying training dataset
2025-02-28 13:00:51,688:INFO:Defining folds
2025-02-28 13:00:51,688:INFO:Declaring metric variables
2025-02-28 13:00:51,690:INFO:Importing untrained model
2025-02-28 13:00:51,692:INFO:K Neighbors Regressor Imported successfully
2025-02-28 13:00:51,696:INFO:Starting cross validation
2025-02-28 13:00:51,697:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:51,779:INFO:Calculating mean and std
2025-02-28 13:00:51,779:INFO:Creating metrics dataframe
2025-02-28 13:00:51,779:INFO:Uploading results into container
2025-02-28 13:00:51,781:INFO:Uploading model into container now
2025-02-28 13:00:51,781:INFO:_master_model_container: 11
2025-02-28 13:00:51,781:INFO:_display_container: 2
2025-02-28 13:00:51,781:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 13:00:51,781:INFO:create_model() successfully completed......................................
2025-02-28 13:00:51,897:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:51,897:INFO:Creating metrics dataframe
2025-02-28 13:00:51,901:INFO:Initializing Decision Tree Regressor
2025-02-28 13:00:51,901:INFO:Total runtime is 0.03967777490615845 minutes
2025-02-28 13:00:51,904:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:51,904:INFO:Initializing create_model()
2025-02-28 13:00:51,904:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:51,904:INFO:Checking exceptions
2025-02-28 13:00:51,904:INFO:Importing libraries
2025-02-28 13:00:51,904:INFO:Copying training dataset
2025-02-28 13:00:51,907:INFO:Defining folds
2025-02-28 13:00:51,907:INFO:Declaring metric variables
2025-02-28 13:00:51,909:INFO:Importing untrained model
2025-02-28 13:00:51,911:INFO:Decision Tree Regressor Imported successfully
2025-02-28 13:00:51,916:INFO:Starting cross validation
2025-02-28 13:00:51,916:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:52,010:INFO:Calculating mean and std
2025-02-28 13:00:52,010:INFO:Creating metrics dataframe
2025-02-28 13:00:52,011:INFO:Uploading results into container
2025-02-28 13:00:52,012:INFO:Uploading model into container now
2025-02-28 13:00:52,012:INFO:_master_model_container: 12
2025-02-28 13:00:52,012:INFO:_display_container: 2
2025-02-28 13:00:52,012:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 13:00:52,012:INFO:create_model() successfully completed......................................
2025-02-28 13:00:52,130:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:52,130:INFO:Creating metrics dataframe
2025-02-28 13:00:52,135:INFO:Initializing Random Forest Regressor
2025-02-28 13:00:52,136:INFO:Total runtime is 0.043579455216725675 minutes
2025-02-28 13:00:52,138:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:52,138:INFO:Initializing create_model()
2025-02-28 13:00:52,138:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:52,138:INFO:Checking exceptions
2025-02-28 13:00:52,138:INFO:Importing libraries
2025-02-28 13:00:52,138:INFO:Copying training dataset
2025-02-28 13:00:52,141:INFO:Defining folds
2025-02-28 13:00:52,141:INFO:Declaring metric variables
2025-02-28 13:00:52,144:INFO:Importing untrained model
2025-02-28 13:00:52,146:INFO:Random Forest Regressor Imported successfully
2025-02-28 13:00:52,151:INFO:Starting cross validation
2025-02-28 13:00:52,153:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:53,024:INFO:Calculating mean and std
2025-02-28 13:00:53,025:INFO:Creating metrics dataframe
2025-02-28 13:00:53,026:INFO:Uploading results into container
2025-02-28 13:00:53,027:INFO:Uploading model into container now
2025-02-28 13:00:53,027:INFO:_master_model_container: 13
2025-02-28 13:00:53,027:INFO:_display_container: 2
2025-02-28 13:00:53,027:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 13:00:53,027:INFO:create_model() successfully completed......................................
2025-02-28 13:00:53,138:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:53,138:INFO:Creating metrics dataframe
2025-02-28 13:00:53,144:INFO:Initializing Extra Trees Regressor
2025-02-28 13:00:53,144:INFO:Total runtime is 0.06039078235626222 minutes
2025-02-28 13:00:53,146:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:53,146:INFO:Initializing create_model()
2025-02-28 13:00:53,146:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:53,146:INFO:Checking exceptions
2025-02-28 13:00:53,146:INFO:Importing libraries
2025-02-28 13:00:53,146:INFO:Copying training dataset
2025-02-28 13:00:53,149:INFO:Defining folds
2025-02-28 13:00:53,149:INFO:Declaring metric variables
2025-02-28 13:00:53,153:INFO:Importing untrained model
2025-02-28 13:00:53,155:INFO:Extra Trees Regressor Imported successfully
2025-02-28 13:00:53,159:INFO:Starting cross validation
2025-02-28 13:00:53,160:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:53,697:INFO:Calculating mean and std
2025-02-28 13:00:53,698:INFO:Creating metrics dataframe
2025-02-28 13:00:53,700:INFO:Uploading results into container
2025-02-28 13:00:53,700:INFO:Uploading model into container now
2025-02-28 13:00:53,700:INFO:_master_model_container: 14
2025-02-28 13:00:53,700:INFO:_display_container: 2
2025-02-28 13:00:53,702:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 13:00:53,702:INFO:create_model() successfully completed......................................
2025-02-28 13:00:53,825:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:53,825:INFO:Creating metrics dataframe
2025-02-28 13:00:53,830:INFO:Initializing AdaBoost Regressor
2025-02-28 13:00:53,830:INFO:Total runtime is 0.07182797590891521 minutes
2025-02-28 13:00:53,834:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:53,834:INFO:Initializing create_model()
2025-02-28 13:00:53,834:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:53,834:INFO:Checking exceptions
2025-02-28 13:00:53,834:INFO:Importing libraries
2025-02-28 13:00:53,834:INFO:Copying training dataset
2025-02-28 13:00:53,837:INFO:Defining folds
2025-02-28 13:00:53,837:INFO:Declaring metric variables
2025-02-28 13:00:53,839:INFO:Importing untrained model
2025-02-28 13:00:53,842:INFO:AdaBoost Regressor Imported successfully
2025-02-28 13:00:53,846:INFO:Starting cross validation
2025-02-28 13:00:53,847:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:54,042:INFO:Calculating mean and std
2025-02-28 13:00:54,042:INFO:Creating metrics dataframe
2025-02-28 13:00:54,044:INFO:Uploading results into container
2025-02-28 13:00:54,044:INFO:Uploading model into container now
2025-02-28 13:00:54,044:INFO:_master_model_container: 15
2025-02-28 13:00:54,044:INFO:_display_container: 2
2025-02-28 13:00:54,044:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 13:00:54,044:INFO:create_model() successfully completed......................................
2025-02-28 13:00:54,154:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:54,154:INFO:Creating metrics dataframe
2025-02-28 13:00:54,159:INFO:Initializing Gradient Boosting Regressor
2025-02-28 13:00:54,159:INFO:Total runtime is 0.07730949719746909 minutes
2025-02-28 13:00:54,161:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:54,161:INFO:Initializing create_model()
2025-02-28 13:00:54,161:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:54,161:INFO:Checking exceptions
2025-02-28 13:00:54,161:INFO:Importing libraries
2025-02-28 13:00:54,161:INFO:Copying training dataset
2025-02-28 13:00:54,164:INFO:Defining folds
2025-02-28 13:00:54,164:INFO:Declaring metric variables
2025-02-28 13:00:54,166:INFO:Importing untrained model
2025-02-28 13:00:54,169:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 13:00:54,173:INFO:Starting cross validation
2025-02-28 13:00:54,174:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:54,589:INFO:Calculating mean and std
2025-02-28 13:00:54,589:INFO:Creating metrics dataframe
2025-02-28 13:00:54,590:INFO:Uploading results into container
2025-02-28 13:00:54,591:INFO:Uploading model into container now
2025-02-28 13:00:54,591:INFO:_master_model_container: 16
2025-02-28 13:00:54,591:INFO:_display_container: 2
2025-02-28 13:00:54,591:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 13:00:54,591:INFO:create_model() successfully completed......................................
2025-02-28 13:00:54,702:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:54,702:INFO:Creating metrics dataframe
2025-02-28 13:00:54,708:INFO:Initializing Extreme Gradient Boosting
2025-02-28 13:00:54,708:INFO:Total runtime is 0.08645181258519492 minutes
2025-02-28 13:00:54,709:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:54,710:INFO:Initializing create_model()
2025-02-28 13:00:54,710:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:54,710:INFO:Checking exceptions
2025-02-28 13:00:54,710:INFO:Importing libraries
2025-02-28 13:00:54,710:INFO:Copying training dataset
2025-02-28 13:00:54,712:INFO:Defining folds
2025-02-28 13:00:54,712:INFO:Declaring metric variables
2025-02-28 13:00:54,715:INFO:Importing untrained model
2025-02-28 13:00:54,718:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 13:00:54,722:INFO:Starting cross validation
2025-02-28 13:00:54,723:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:55,196:INFO:Calculating mean and std
2025-02-28 13:00:55,197:INFO:Creating metrics dataframe
2025-02-28 13:00:55,198:INFO:Uploading results into container
2025-02-28 13:00:55,198:INFO:Uploading model into container now
2025-02-28 13:00:55,198:INFO:_master_model_container: 17
2025-02-28 13:00:55,198:INFO:_display_container: 2
2025-02-28 13:00:55,199:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 13:00:55,199:INFO:create_model() successfully completed......................................
2025-02-28 13:00:55,310:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:55,311:INFO:Creating metrics dataframe
2025-02-28 13:00:55,315:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 13:00:55,316:INFO:Total runtime is 0.09658335844675701 minutes
2025-02-28 13:00:55,318:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:55,318:INFO:Initializing create_model()
2025-02-28 13:00:55,318:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:55,318:INFO:Checking exceptions
2025-02-28 13:00:55,318:INFO:Importing libraries
2025-02-28 13:00:55,318:INFO:Copying training dataset
2025-02-28 13:00:55,320:INFO:Defining folds
2025-02-28 13:00:55,321:INFO:Declaring metric variables
2025-02-28 13:00:55,323:INFO:Importing untrained model
2025-02-28 13:00:55,325:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:00:55,329:INFO:Starting cross validation
2025-02-28 13:00:55,330:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:56,716:INFO:Calculating mean and std
2025-02-28 13:00:56,717:INFO:Creating metrics dataframe
2025-02-28 13:00:56,718:INFO:Uploading results into container
2025-02-28 13:00:56,719:INFO:Uploading model into container now
2025-02-28 13:00:56,719:INFO:_master_model_container: 18
2025-02-28 13:00:56,719:INFO:_display_container: 2
2025-02-28 13:00:56,720:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:00:56,720:INFO:create_model() successfully completed......................................
2025-02-28 13:00:56,855:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:56,855:INFO:Creating metrics dataframe
2025-02-28 13:00:56,863:INFO:Initializing CatBoost Regressor
2025-02-28 13:00:56,863:INFO:Total runtime is 0.12236943244934084 minutes
2025-02-28 13:00:56,865:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:56,865:INFO:Initializing create_model()
2025-02-28 13:00:56,865:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:56,865:INFO:Checking exceptions
2025-02-28 13:00:56,865:INFO:Importing libraries
2025-02-28 13:00:56,865:INFO:Copying training dataset
2025-02-28 13:00:56,868:INFO:Defining folds
2025-02-28 13:00:56,868:INFO:Declaring metric variables
2025-02-28 13:00:56,870:INFO:Importing untrained model
2025-02-28 13:00:56,873:INFO:CatBoost Regressor Imported successfully
2025-02-28 13:00:56,877:INFO:Starting cross validation
2025-02-28 13:00:56,878:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:59,657:INFO:Calculating mean and std
2025-02-28 13:00:59,658:INFO:Creating metrics dataframe
2025-02-28 13:00:59,660:INFO:Uploading results into container
2025-02-28 13:00:59,661:INFO:Uploading model into container now
2025-02-28 13:00:59,661:INFO:_master_model_container: 19
2025-02-28 13:00:59,661:INFO:_display_container: 2
2025-02-28 13:00:59,661:INFO:<catboost.core.CatBoostRegressor object at 0x0000021913F031C0>
2025-02-28 13:00:59,662:INFO:create_model() successfully completed......................................
2025-02-28 13:00:59,775:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:59,775:INFO:Creating metrics dataframe
2025-02-28 13:00:59,780:INFO:Initializing Dummy Regressor
2025-02-28 13:00:59,780:INFO:Total runtime is 0.17099305788675945 minutes
2025-02-28 13:00:59,782:INFO:SubProcess create_model() called ==================================
2025-02-28 13:00:59,782:INFO:Initializing create_model()
2025-02-28 13:00:59,782:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002188D362980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:59,782:INFO:Checking exceptions
2025-02-28 13:00:59,783:INFO:Importing libraries
2025-02-28 13:00:59,783:INFO:Copying training dataset
2025-02-28 13:00:59,786:INFO:Defining folds
2025-02-28 13:00:59,786:INFO:Declaring metric variables
2025-02-28 13:00:59,789:INFO:Importing untrained model
2025-02-28 13:00:59,791:INFO:Dummy Regressor Imported successfully
2025-02-28 13:00:59,795:INFO:Starting cross validation
2025-02-28 13:00:59,796:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:00:59,874:INFO:Calculating mean and std
2025-02-28 13:00:59,874:INFO:Creating metrics dataframe
2025-02-28 13:00:59,875:INFO:Uploading results into container
2025-02-28 13:00:59,876:INFO:Uploading model into container now
2025-02-28 13:00:59,876:INFO:_master_model_container: 20
2025-02-28 13:00:59,876:INFO:_display_container: 2
2025-02-28 13:00:59,876:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 13:00:59,876:INFO:create_model() successfully completed......................................
2025-02-28 13:00:59,985:INFO:SubProcess create_model() end ==================================
2025-02-28 13:00:59,986:INFO:Creating metrics dataframe
2025-02-28 13:00:59,992:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 13:00:59,997:INFO:Initializing create_model()
2025-02-28 13:00:59,997:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:00:59,997:INFO:Checking exceptions
2025-02-28 13:00:59,999:INFO:Importing libraries
2025-02-28 13:00:59,999:INFO:Copying training dataset
2025-02-28 13:01:00,002:INFO:Defining folds
2025-02-28 13:01:00,002:INFO:Declaring metric variables
2025-02-28 13:01:00,002:INFO:Importing untrained model
2025-02-28 13:01:00,002:INFO:Declaring custom model
2025-02-28 13:01:00,003:INFO:Dummy Regressor Imported successfully
2025-02-28 13:01:00,003:INFO:Cross validation set to False
2025-02-28 13:01:00,003:INFO:Fitting Model
2025-02-28 13:01:00,025:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 13:01:00,025:INFO:create_model() successfully completed......................................
2025-02-28 13:01:00,153:INFO:_master_model_container: 20
2025-02-28 13:01:00,153:INFO:_display_container: 2
2025-02-28 13:01:00,154:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 13:01:00,154:INFO:compare_models() successfully completed......................................
2025-02-28 13:01:00,191:INFO:Initializing create_model()
2025-02-28 13:01:00,191:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, estimator=omp, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:01:00,191:INFO:Checking exceptions
2025-02-28 13:01:00,199:INFO:Importing libraries
2025-02-28 13:01:00,199:INFO:Copying training dataset
2025-02-28 13:01:00,202:INFO:Defining folds
2025-02-28 13:01:00,202:INFO:Declaring metric variables
2025-02-28 13:01:00,204:INFO:Importing untrained model
2025-02-28 13:01:00,207:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 13:01:00,213:INFO:Starting cross validation
2025-02-28 13:01:00,214:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:01:00,294:INFO:Calculating mean and std
2025-02-28 13:01:00,294:INFO:Creating metrics dataframe
2025-02-28 13:01:00,297:INFO:Finalizing model
2025-02-28 13:01:00,322:INFO:Uploading results into container
2025-02-28 13:01:00,322:INFO:Uploading model into container now
2025-02-28 13:01:00,327:INFO:_master_model_container: 21
2025-02-28 13:01:00,327:INFO:_display_container: 3
2025-02-28 13:01:00,327:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 13:01:00,327:INFO:create_model() successfully completed......................................
2025-02-28 13:01:00,458:INFO:Initializing plot_model()
2025-02-28 13:01:00,458:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, system=True)
2025-02-28 13:01:00,458:INFO:Checking exceptions
2025-02-28 13:01:00,461:INFO:Preloading libraries
2025-02-28 13:01:00,461:INFO:Copying training dataset
2025-02-28 13:01:00,461:INFO:Plot type: residuals
2025-02-28 13:01:00,568:INFO:Fitting Model
2025-02-28 13:01:00,568:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 13:01:00,585:INFO:Scoring test/hold-out set
2025-02-28 13:01:00,825:INFO:Visual Rendered Successfully
2025-02-28 13:01:00,941:INFO:plot_model() successfully completed......................................
2025-02-28 13:01:00,957:INFO:Initializing plot_model()
2025-02-28 13:01:00,957:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, system=True)
2025-02-28 13:01:00,957:INFO:Checking exceptions
2025-02-28 13:01:00,959:INFO:Preloading libraries
2025-02-28 13:01:00,959:INFO:Copying training dataset
2025-02-28 13:01:00,960:INFO:Plot type: error
2025-02-28 13:01:01,056:INFO:Fitting Model
2025-02-28 13:01:01,056:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 13:01:01,056:INFO:Scoring test/hold-out set
2025-02-28 13:01:01,163:INFO:Visual Rendered Successfully
2025-02-28 13:01:01,277:INFO:plot_model() successfully completed......................................
2025-02-28 13:01:01,300:INFO:Initializing plot_model()
2025-02-28 13:01:01,300:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000002188CE55030>, system=True)
2025-02-28 13:01:01,300:INFO:Checking exceptions
2025-02-28 13:01:01,303:INFO:Preloading libraries
2025-02-28 13:01:01,303:INFO:Copying training dataset
2025-02-28 13:01:01,303:INFO:Plot type: feature
2025-02-28 13:01:01,421:INFO:Visual Rendered Successfully
2025-02-28 13:01:01,532:INFO:plot_model() successfully completed......................................
2025-02-28 13:45:59,782:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:45:59,783:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:45:59,783:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:45:59,783:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:01,205:INFO:PyCaret ClassificationExperiment
2025-02-28 13:46:01,205:INFO:Logging name: clf-default-name
2025-02-28 13:46:01,205:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 13:46:01,205:INFO:version 3.3.2
2025-02-28 13:46:01,205:INFO:Initializing setup()
2025-02-28 13:46:01,205:INFO:self.USI: c6d5
2025-02-28 13:46:01,205:INFO:self._variable_keys: {'X_test', 'logging_param', 'log_plots_param', 'y', 'y_test', '_ml_usecase', 'n_jobs_param', 'USI', 'gpu_param', 'memory', 'gpu_n_jobs_param', 'y_train', 'exp_name_log', 'fold_shuffle_param', 'fold_generator', 'seed', 'target_param', 'X', '_available_plots', 'html_param', 'exp_id', 'X_train', 'pipeline', 'fix_imbalance', 'data', 'is_multiclass', 'fold_groups_param', 'idx'}
2025-02-28 13:46:01,205:INFO:Checking environment
2025-02-28 13:46:01,205:INFO:python_version: 3.10.16
2025-02-28 13:46:01,206:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 13:46:01,206:INFO:machine: AMD64
2025-02-28 13:46:01,206:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 13:46:01,206:INFO:Memory: svmem(total=34200334336, available=18157150208, percent=46.9, used=16043184128, free=18157150208)
2025-02-28 13:46:01,206:INFO:Physical Core: 24
2025-02-28 13:46:01,206:INFO:Logical Core: 32
2025-02-28 13:46:01,206:INFO:Checking libraries
2025-02-28 13:46:01,206:INFO:System:
2025-02-28 13:46:01,206:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 13:46:01,206:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 13:46:01,206:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 13:46:01,206:INFO:PyCaret required dependencies:
2025-02-28 13:46:01,777:INFO:                 pip: 25.0
2025-02-28 13:46:01,777:INFO:          setuptools: 75.8.0
2025-02-28 13:46:01,777:INFO:             pycaret: 3.3.2
2025-02-28 13:46:01,777:INFO:             IPython: 8.30.0
2025-02-28 13:46:01,778:INFO:          ipywidgets: 8.1.5
2025-02-28 13:46:01,778:INFO:                tqdm: 4.67.1
2025-02-28 13:46:01,778:INFO:               numpy: 1.26.4
2025-02-28 13:46:01,778:INFO:              pandas: 2.1.4
2025-02-28 13:46:01,778:INFO:              jinja2: 3.1.5
2025-02-28 13:46:01,778:INFO:               scipy: 1.11.4
2025-02-28 13:46:01,778:INFO:              joblib: 1.3.2
2025-02-28 13:46:01,778:INFO:             sklearn: 1.4.2
2025-02-28 13:46:01,778:INFO:                pyod: 2.0.3
2025-02-28 13:46:01,778:INFO:            imblearn: 0.13.0
2025-02-28 13:46:01,778:INFO:   category_encoders: 2.7.0
2025-02-28 13:46:01,778:INFO:            lightgbm: 4.5.0
2025-02-28 13:46:01,778:INFO:               numba: 0.61.0
2025-02-28 13:46:01,778:INFO:            requests: 2.32.3
2025-02-28 13:46:01,778:INFO:          matplotlib: 3.7.5
2025-02-28 13:46:01,778:INFO:          scikitplot: 0.3.7
2025-02-28 13:46:01,778:INFO:         yellowbrick: 1.5
2025-02-28 13:46:01,778:INFO:              plotly: 5.24.1
2025-02-28 13:46:01,778:INFO:    plotly-resampler: Not installed
2025-02-28 13:46:01,778:INFO:             kaleido: 0.2.1
2025-02-28 13:46:01,778:INFO:           schemdraw: 0.15
2025-02-28 13:46:01,778:INFO:         statsmodels: 0.14.4
2025-02-28 13:46:01,778:INFO:              sktime: 0.26.0
2025-02-28 13:46:01,778:INFO:               tbats: 1.1.3
2025-02-28 13:46:01,778:INFO:            pmdarima: 2.0.4
2025-02-28 13:46:01,778:INFO:              psutil: 5.9.0
2025-02-28 13:46:01,778:INFO:          markupsafe: 2.1.5
2025-02-28 13:46:01,778:INFO:             pickle5: Not installed
2025-02-28 13:46:01,778:INFO:         cloudpickle: 3.1.1
2025-02-28 13:46:01,778:INFO:         deprecation: 2.1.0
2025-02-28 13:46:01,778:INFO:              xxhash: 3.5.0
2025-02-28 13:46:01,778:INFO:           wurlitzer: Not installed
2025-02-28 13:46:01,779:INFO:PyCaret optional dependencies:
2025-02-28 13:46:03,524:INFO:                shap: 0.44.1
2025-02-28 13:46:03,524:INFO:           interpret: 0.6.9
2025-02-28 13:46:03,524:INFO:                umap: 0.5.7
2025-02-28 13:46:03,524:INFO:     ydata_profiling: 4.12.2
2025-02-28 13:46:03,524:INFO:  explainerdashboard: 0.4.8
2025-02-28 13:46:03,524:INFO:             autoviz: Not installed
2025-02-28 13:46:03,524:INFO:           fairlearn: 0.7.0
2025-02-28 13:46:03,524:INFO:          deepchecks: Not installed
2025-02-28 13:46:03,524:INFO:             xgboost: 2.1.4
2025-02-28 13:46:03,524:INFO:            catboost: 1.2.7
2025-02-28 13:46:03,524:INFO:              kmodes: 0.12.2
2025-02-28 13:46:03,524:INFO:             mlxtend: 0.23.4
2025-02-28 13:46:03,524:INFO:       statsforecast: 1.5.0
2025-02-28 13:46:03,524:INFO:        tune_sklearn: Not installed
2025-02-28 13:46:03,524:INFO:                 ray: Not installed
2025-02-28 13:46:03,524:INFO:            hyperopt: 0.2.7
2025-02-28 13:46:03,524:INFO:              optuna: 4.2.0
2025-02-28 13:46:03,524:INFO:               skopt: 0.10.2
2025-02-28 13:46:03,524:INFO:              mlflow: 2.20.1
2025-02-28 13:46:03,524:INFO:              gradio: 5.15.0
2025-02-28 13:46:03,524:INFO:             fastapi: 0.115.8
2025-02-28 13:46:03,524:INFO:             uvicorn: 0.34.0
2025-02-28 13:46:03,524:INFO:              m2cgen: 0.10.0
2025-02-28 13:46:03,524:INFO:           evidently: 0.4.40
2025-02-28 13:46:03,524:INFO:               fugue: 0.8.7
2025-02-28 13:46:03,524:INFO:           streamlit: Not installed
2025-02-28 13:46:03,524:INFO:             prophet: Not installed
2025-02-28 13:46:03,524:INFO:None
2025-02-28 13:46:03,524:INFO:Set up GPU usage.
2025-02-28 13:46:03,524:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:03,524:WARNING:cuML is outdated or not found. Required version is >=23.08.
                Please visit https://rapids.ai/install for installation instructions.
2025-02-28 13:46:03,524:INFO:Set up data.
2025-02-28 13:46:03,527:INFO:Set up folding strategy.
2025-02-28 13:46:03,527:INFO:Set up train/test split.
2025-02-28 13:46:03,531:INFO:Set up index.
2025-02-28 13:46:03,531:INFO:Assigning column types.
2025-02-28 13:46:03,533:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 13:46:03,533:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:03,554:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:46:03,554:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:03,556:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:03,556:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:46:03,556:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:03,569:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:03,571:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:03,572:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:46:03,576:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\joblib\externals\loky\backend\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:
[WinError 2] The system cannot find the file specified
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(

2025-02-28 13:46:05,621:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:46:05,639:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,672:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:46:05,672:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,672:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,672:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:46:05,672:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,689:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,692:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,693:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:46:05,876:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:46:05,877:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 13:46:05,877:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,911:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,911:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,911:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:46:05,912:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,928:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,932:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:05,932:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:46:06,032:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:46:06,032:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,065:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,066:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,066:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:46:06,066:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,083:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,087:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,088:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:46:06,190:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:46:06,190:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 13:46:06,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,223:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,223:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,224:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,239:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,243:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,244:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:46:06,335:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:46:06,335:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,368:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,369:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,369:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,385:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,389:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,390:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:46:06,478:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:46:06,479:INFO:Preparing preprocessing pipeline...
2025-02-28 13:46:06,480:INFO:Set up simple imputation.
2025-02-28 13:46:06,482:INFO:Set up encoding of categorical features.
2025-02-28 13:46:06,523:INFO:Finished creating preprocessing pipeline.
2025-02-28 13:46:06,527:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 13:46:06,528:INFO:Creating final display dataframe.
2025-02-28 13:46:06,641:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU              True
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              c6d5
2025-02-28 13:46:06,645:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,665:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,665:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,666:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,675:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,677:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,677:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:46:06,775:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:46:06,775:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,808:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,809:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,810:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,826:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,829:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:46:06,830:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:46:06,915:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:46:06,916:INFO:setup() successfully completed in 5.71s...............
2025-02-28 13:46:06,926:INFO:Initializing compare_models()
2025-02-28 13:46:06,926:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 13:46:06,926:INFO:Checking exceptions
2025-02-28 13:46:06,930:INFO:Preparing display monitor
2025-02-28 13:46:06,945:INFO:Initializing Logistic Regression
2025-02-28 13:46:06,945:INFO:Total runtime is 0.0 minutes
2025-02-28 13:46:06,947:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:06,947:INFO:Initializing create_model()
2025-02-28 13:46:06,947:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:06,947:INFO:Checking exceptions
2025-02-28 13:46:06,947:INFO:Importing libraries
2025-02-28 13:46:06,947:INFO:Copying training dataset
2025-02-28 13:46:06,953:INFO:Defining folds
2025-02-28 13:46:06,953:INFO:Declaring metric variables
2025-02-28 13:46:06,956:INFO:Importing untrained model
2025-02-28 13:46:06,958:INFO:Logistic Regression Imported successfully
2025-02-28 13:46:06,962:INFO:Starting cross validation
2025-02-28 13:46:06,964:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:07,515:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:07,528:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:08,064:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:08,075:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:08,607:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:08,618:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:09,155:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:09,166:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:09,692:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:09,703:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:10,254:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:10,265:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:10,825:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:10,836:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:11,377:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:11,387:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:11,913:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:11,924:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:12,468:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:46:12,479:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:12,483:INFO:Calculating mean and std
2025-02-28 13:46:12,484:INFO:Creating metrics dataframe
2025-02-28 13:46:12,485:INFO:Uploading results into container
2025-02-28 13:46:12,485:INFO:Uploading model into container now
2025-02-28 13:46:12,485:INFO:_master_model_container: 1
2025-02-28 13:46:12,485:INFO:_display_container: 2
2025-02-28 13:46:12,486:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 13:46:12,486:INFO:create_model() successfully completed......................................
2025-02-28 13:46:12,562:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:12,563:INFO:Creating metrics dataframe
2025-02-28 13:46:12,566:INFO:Initializing K Neighbors Classifier
2025-02-28 13:46:12,566:INFO:Total runtime is 0.09368921120961507 minutes
2025-02-28 13:46:12,568:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:12,568:INFO:Initializing create_model()
2025-02-28 13:46:12,568:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:12,568:INFO:Checking exceptions
2025-02-28 13:46:12,568:INFO:Importing libraries
2025-02-28 13:46:12,568:INFO:Copying training dataset
2025-02-28 13:46:12,572:INFO:Defining folds
2025-02-28 13:46:12,572:INFO:Declaring metric variables
2025-02-28 13:46:12,573:INFO:Importing untrained model
2025-02-28 13:46:12,575:INFO:K Neighbors Classifier Imported successfully
2025-02-28 13:46:12,578:INFO:Starting cross validation
2025-02-28 13:46:12,579:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:13,938:INFO:Calculating mean and std
2025-02-28 13:46:13,939:INFO:Creating metrics dataframe
2025-02-28 13:46:13,940:INFO:Uploading results into container
2025-02-28 13:46:13,940:INFO:Uploading model into container now
2025-02-28 13:46:13,942:INFO:_master_model_container: 2
2025-02-28 13:46:13,942:INFO:_display_container: 2
2025-02-28 13:46:13,942:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 13:46:13,942:INFO:create_model() successfully completed......................................
2025-02-28 13:46:14,032:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:14,032:INFO:Creating metrics dataframe
2025-02-28 13:46:14,035:INFO:Initializing Naive Bayes
2025-02-28 13:46:14,035:INFO:Total runtime is 0.11817036072413126 minutes
2025-02-28 13:46:14,037:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:14,037:INFO:Initializing create_model()
2025-02-28 13:46:14,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:14,037:INFO:Checking exceptions
2025-02-28 13:46:14,037:INFO:Importing libraries
2025-02-28 13:46:14,037:INFO:Copying training dataset
2025-02-28 13:46:14,040:INFO:Defining folds
2025-02-28 13:46:14,041:INFO:Declaring metric variables
2025-02-28 13:46:14,043:INFO:Importing untrained model
2025-02-28 13:46:14,045:INFO:Naive Bayes Imported successfully
2025-02-28 13:46:14,049:INFO:Starting cross validation
2025-02-28 13:46:14,050:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:14,446:INFO:Calculating mean and std
2025-02-28 13:46:14,447:INFO:Creating metrics dataframe
2025-02-28 13:46:14,448:INFO:Uploading results into container
2025-02-28 13:46:14,448:INFO:Uploading model into container now
2025-02-28 13:46:14,449:INFO:_master_model_container: 3
2025-02-28 13:46:14,449:INFO:_display_container: 2
2025-02-28 13:46:14,449:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 13:46:14,449:INFO:create_model() successfully completed......................................
2025-02-28 13:46:14,527:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:14,527:INFO:Creating metrics dataframe
2025-02-28 13:46:14,531:INFO:Initializing Decision Tree Classifier
2025-02-28 13:46:14,531:INFO:Total runtime is 0.12643423080444335 minutes
2025-02-28 13:46:14,533:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:14,533:INFO:Initializing create_model()
2025-02-28 13:46:14,533:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:14,533:INFO:Checking exceptions
2025-02-28 13:46:14,534:INFO:Importing libraries
2025-02-28 13:46:14,534:INFO:Copying training dataset
2025-02-28 13:46:14,536:INFO:Defining folds
2025-02-28 13:46:14,536:INFO:Declaring metric variables
2025-02-28 13:46:14,538:INFO:Importing untrained model
2025-02-28 13:46:14,539:INFO:Decision Tree Classifier Imported successfully
2025-02-28 13:46:14,543:INFO:Starting cross validation
2025-02-28 13:46:14,544:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:15,137:INFO:Calculating mean and std
2025-02-28 13:46:15,137:INFO:Creating metrics dataframe
2025-02-28 13:46:15,139:INFO:Uploading results into container
2025-02-28 13:46:15,139:INFO:Uploading model into container now
2025-02-28 13:46:15,139:INFO:_master_model_container: 4
2025-02-28 13:46:15,139:INFO:_display_container: 2
2025-02-28 13:46:15,139:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 13:46:15,140:INFO:create_model() successfully completed......................................
2025-02-28 13:46:15,227:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:15,227:INFO:Creating metrics dataframe
2025-02-28 13:46:15,230:INFO:Initializing SVM - Linear Kernel
2025-02-28 13:46:15,230:INFO:Total runtime is 0.13808467785517375 minutes
2025-02-28 13:46:15,233:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:15,233:INFO:Initializing create_model()
2025-02-28 13:46:15,234:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:15,234:INFO:Checking exceptions
2025-02-28 13:46:15,234:INFO:Importing libraries
2025-02-28 13:46:15,234:INFO:Copying training dataset
2025-02-28 13:46:15,236:INFO:Defining folds
2025-02-28 13:46:15,236:INFO:Declaring metric variables
2025-02-28 13:46:15,238:INFO:Importing untrained model
2025-02-28 13:46:15,239:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 13:46:15,243:INFO:Starting cross validation
2025-02-28 13:46:15,244:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:15,324:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:15,327:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:15,401:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:15,403:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:15,479:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:15,482:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:15,558:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:15,560:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:15,634:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:15,636:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:15,712:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:15,714:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:15,790:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:15,793:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:15,886:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:15,888:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:15,965:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:15,966:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:16,043:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,045:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:16,047:INFO:Calculating mean and std
2025-02-28 13:46:16,047:INFO:Creating metrics dataframe
2025-02-28 13:46:16,048:INFO:Uploading results into container
2025-02-28 13:46:16,049:INFO:Uploading model into container now
2025-02-28 13:46:16,050:INFO:_master_model_container: 5
2025-02-28 13:46:16,050:INFO:_display_container: 2
2025-02-28 13:46:16,050:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 13:46:16,050:INFO:create_model() successfully completed......................................
2025-02-28 13:46:16,125:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:16,125:INFO:Creating metrics dataframe
2025-02-28 13:46:16,130:INFO:Initializing Ridge Classifier
2025-02-28 13:46:16,130:INFO:Total runtime is 0.15308056275049847 minutes
2025-02-28 13:46:16,132:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:16,132:INFO:Initializing create_model()
2025-02-28 13:46:16,132:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:16,132:INFO:Checking exceptions
2025-02-28 13:46:16,132:INFO:Importing libraries
2025-02-28 13:46:16,132:INFO:Copying training dataset
2025-02-28 13:46:16,135:INFO:Defining folds
2025-02-28 13:46:16,135:INFO:Declaring metric variables
2025-02-28 13:46:16,136:INFO:Importing untrained model
2025-02-28 13:46:16,138:INFO:Ridge Classifier Imported successfully
2025-02-28 13:46:16,142:INFO:Starting cross validation
2025-02-28 13:46:16,142:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:16,177:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,214:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,250:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,288:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,326:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,364:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,400:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,438:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,475:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,512:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:16,516:INFO:Calculating mean and std
2025-02-28 13:46:16,517:INFO:Creating metrics dataframe
2025-02-28 13:46:16,518:INFO:Uploading results into container
2025-02-28 13:46:16,519:INFO:Uploading model into container now
2025-02-28 13:46:16,519:INFO:_master_model_container: 6
2025-02-28 13:46:16,519:INFO:_display_container: 2
2025-02-28 13:46:16,519:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 13:46:16,519:INFO:create_model() successfully completed......................................
2025-02-28 13:46:16,595:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:16,595:INFO:Creating metrics dataframe
2025-02-28 13:46:16,599:INFO:Initializing Random Forest Classifier
2025-02-28 13:46:16,599:INFO:Total runtime is 0.16089936097462973 minutes
2025-02-28 13:46:16,601:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:16,601:INFO:Initializing create_model()
2025-02-28 13:46:16,601:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:16,601:INFO:Checking exceptions
2025-02-28 13:46:16,602:INFO:Importing libraries
2025-02-28 13:46:16,602:INFO:Copying training dataset
2025-02-28 13:46:16,604:INFO:Defining folds
2025-02-28 13:46:16,604:INFO:Declaring metric variables
2025-02-28 13:46:16,606:INFO:Importing untrained model
2025-02-28 13:46:16,607:INFO:Random Forest Classifier Imported successfully
2025-02-28 13:46:16,611:INFO:Starting cross validation
2025-02-28 13:46:16,612:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:18,630:INFO:Calculating mean and std
2025-02-28 13:46:18,630:INFO:Creating metrics dataframe
2025-02-28 13:46:18,631:INFO:Uploading results into container
2025-02-28 13:46:18,632:INFO:Uploading model into container now
2025-02-28 13:46:18,632:INFO:_master_model_container: 7
2025-02-28 13:46:18,632:INFO:_display_container: 2
2025-02-28 13:46:18,632:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 13:46:18,632:INFO:create_model() successfully completed......................................
2025-02-28 13:46:18,705:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:18,705:INFO:Creating metrics dataframe
2025-02-28 13:46:18,710:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 13:46:18,710:INFO:Total runtime is 0.1960770805676778 minutes
2025-02-28 13:46:18,711:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:18,712:INFO:Initializing create_model()
2025-02-28 13:46:18,712:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:18,712:INFO:Checking exceptions
2025-02-28 13:46:18,712:INFO:Importing libraries
2025-02-28 13:46:18,712:INFO:Copying training dataset
2025-02-28 13:46:18,714:INFO:Defining folds
2025-02-28 13:46:18,714:INFO:Declaring metric variables
2025-02-28 13:46:18,715:INFO:Importing untrained model
2025-02-28 13:46:18,717:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 13:46:18,720:INFO:Starting cross validation
2025-02-28 13:46:18,722:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:18,748:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:18,762:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:18,764:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:18,788:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:18,801:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:18,828:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:18,842:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:18,844:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:18,869:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:18,882:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:18,909:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:18,922:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:18,924:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:18,948:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:18,960:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:18,962:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:18,986:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:18,998:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:19,001:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:19,024:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:19,038:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:19,064:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:19,077:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:19,103:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:46:19,115:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:19,118:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:46:19,120:INFO:Calculating mean and std
2025-02-28 13:46:19,120:INFO:Creating metrics dataframe
2025-02-28 13:46:19,122:INFO:Uploading results into container
2025-02-28 13:46:19,122:INFO:Uploading model into container now
2025-02-28 13:46:19,122:INFO:_master_model_container: 8
2025-02-28 13:46:19,122:INFO:_display_container: 2
2025-02-28 13:46:19,123:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 13:46:19,123:INFO:create_model() successfully completed......................................
2025-02-28 13:46:19,199:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:19,200:INFO:Creating metrics dataframe
2025-02-28 13:46:19,204:INFO:Initializing Ada Boost Classifier
2025-02-28 13:46:19,204:INFO:Total runtime is 0.20431647300720215 minutes
2025-02-28 13:46:19,205:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:19,206:INFO:Initializing create_model()
2025-02-28 13:46:19,206:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:19,206:INFO:Checking exceptions
2025-02-28 13:46:19,206:INFO:Importing libraries
2025-02-28 13:46:19,206:INFO:Copying training dataset
2025-02-28 13:46:19,208:INFO:Defining folds
2025-02-28 13:46:19,208:INFO:Declaring metric variables
2025-02-28 13:46:19,209:INFO:Importing untrained model
2025-02-28 13:46:19,210:INFO:Ada Boost Classifier Imported successfully
2025-02-28 13:46:19,215:INFO:Starting cross validation
2025-02-28 13:46:19,216:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:19,238:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:19,364:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:19,390:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:19,516:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:19,542:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:19,666:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:19,692:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:19,818:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:19,843:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:19,966:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:19,992:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:20,117:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:20,142:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:20,271:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:20,298:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:20,426:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:20,453:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:20,586:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:20,612:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:46:20,739:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:20,743:INFO:Calculating mean and std
2025-02-28 13:46:20,744:INFO:Creating metrics dataframe
2025-02-28 13:46:20,745:INFO:Uploading results into container
2025-02-28 13:46:20,745:INFO:Uploading model into container now
2025-02-28 13:46:20,745:INFO:_master_model_container: 9
2025-02-28 13:46:20,745:INFO:_display_container: 2
2025-02-28 13:46:20,745:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 13:46:20,745:INFO:create_model() successfully completed......................................
2025-02-28 13:46:20,828:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:20,829:INFO:Creating metrics dataframe
2025-02-28 13:46:20,834:INFO:Initializing Gradient Boosting Classifier
2025-02-28 13:46:20,834:INFO:Total runtime is 0.23147547642389935 minutes
2025-02-28 13:46:20,836:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:20,836:INFO:Initializing create_model()
2025-02-28 13:46:20,836:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:20,837:INFO:Checking exceptions
2025-02-28 13:46:20,837:INFO:Importing libraries
2025-02-28 13:46:20,837:INFO:Copying training dataset
2025-02-28 13:46:20,839:INFO:Defining folds
2025-02-28 13:46:20,839:INFO:Declaring metric variables
2025-02-28 13:46:20,841:INFO:Importing untrained model
2025-02-28 13:46:20,842:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 13:46:20,846:INFO:Starting cross validation
2025-02-28 13:46:20,847:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:22,706:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:24,546:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:26,382:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:28,222:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:30,049:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:31,908:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:33,743:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:35,581:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:37,418:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,260:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,264:INFO:Calculating mean and std
2025-02-28 13:46:39,265:INFO:Creating metrics dataframe
2025-02-28 13:46:39,266:INFO:Uploading results into container
2025-02-28 13:46:39,266:INFO:Uploading model into container now
2025-02-28 13:46:39,266:INFO:_master_model_container: 10
2025-02-28 13:46:39,266:INFO:_display_container: 2
2025-02-28 13:46:39,267:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 13:46:39,267:INFO:create_model() successfully completed......................................
2025-02-28 13:46:39,345:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:39,345:INFO:Creating metrics dataframe
2025-02-28 13:46:39,350:INFO:Initializing Linear Discriminant Analysis
2025-02-28 13:46:39,350:INFO:Total runtime is 0.5400756001472473 minutes
2025-02-28 13:46:39,352:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:39,352:INFO:Initializing create_model()
2025-02-28 13:46:39,352:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:39,352:INFO:Checking exceptions
2025-02-28 13:46:39,352:INFO:Importing libraries
2025-02-28 13:46:39,352:INFO:Copying training dataset
2025-02-28 13:46:39,354:INFO:Defining folds
2025-02-28 13:46:39,354:INFO:Declaring metric variables
2025-02-28 13:46:39,356:INFO:Importing untrained model
2025-02-28 13:46:39,358:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 13:46:39,360:INFO:Starting cross validation
2025-02-28 13:46:39,362:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:39,401:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,439:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,479:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,517:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,556:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,594:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,633:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,671:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,711:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,749:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:46:39,753:INFO:Calculating mean and std
2025-02-28 13:46:39,754:INFO:Creating metrics dataframe
2025-02-28 13:46:39,755:INFO:Uploading results into container
2025-02-28 13:46:39,755:INFO:Uploading model into container now
2025-02-28 13:46:39,755:INFO:_master_model_container: 11
2025-02-28 13:46:39,755:INFO:_display_container: 2
2025-02-28 13:46:39,756:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 13:46:39,756:INFO:create_model() successfully completed......................................
2025-02-28 13:46:39,828:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:39,828:INFO:Creating metrics dataframe
2025-02-28 13:46:39,834:INFO:Initializing Extra Trees Classifier
2025-02-28 13:46:39,834:INFO:Total runtime is 0.5481411973635356 minutes
2025-02-28 13:46:39,836:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:39,836:INFO:Initializing create_model()
2025-02-28 13:46:39,836:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:39,836:INFO:Checking exceptions
2025-02-28 13:46:39,836:INFO:Importing libraries
2025-02-28 13:46:39,836:INFO:Copying training dataset
2025-02-28 13:46:39,838:INFO:Defining folds
2025-02-28 13:46:39,838:INFO:Declaring metric variables
2025-02-28 13:46:39,840:INFO:Importing untrained model
2025-02-28 13:46:39,841:INFO:Extra Trees Classifier Imported successfully
2025-02-28 13:46:39,844:INFO:Starting cross validation
2025-02-28 13:46:39,845:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:41,570:INFO:Calculating mean and std
2025-02-28 13:46:41,570:INFO:Creating metrics dataframe
2025-02-28 13:46:41,571:INFO:Uploading results into container
2025-02-28 13:46:41,572:INFO:Uploading model into container now
2025-02-28 13:46:41,572:INFO:_master_model_container: 12
2025-02-28 13:46:41,572:INFO:_display_container: 2
2025-02-28 13:46:41,572:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 13:46:41,572:INFO:create_model() successfully completed......................................
2025-02-28 13:46:41,650:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:41,650:INFO:Creating metrics dataframe
2025-02-28 13:46:41,656:INFO:Initializing Extreme Gradient Boosting
2025-02-28 13:46:41,656:INFO:Total runtime is 0.5785143733024598 minutes
2025-02-28 13:46:41,658:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:41,658:INFO:Initializing create_model()
2025-02-28 13:46:41,658:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:41,658:INFO:Checking exceptions
2025-02-28 13:46:41,658:INFO:Importing libraries
2025-02-28 13:46:41,658:INFO:Copying training dataset
2025-02-28 13:46:41,660:INFO:Defining folds
2025-02-28 13:46:41,660:INFO:Declaring metric variables
2025-02-28 13:46:41,662:INFO:Importing untrained model
2025-02-28 13:46:41,664:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 13:46:41,667:INFO:Starting cross validation
2025-02-28 13:46:41,668:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:53,713:INFO:Calculating mean and std
2025-02-28 13:46:53,714:INFO:Creating metrics dataframe
2025-02-28 13:46:53,715:INFO:Uploading results into container
2025-02-28 13:46:53,716:INFO:Uploading model into container now
2025-02-28 13:46:53,716:INFO:_master_model_container: 13
2025-02-28 13:46:53,716:INFO:_display_container: 2
2025-02-28 13:46:53,717:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='gpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 13:46:53,717:INFO:create_model() successfully completed......................................
2025-02-28 13:46:53,801:INFO:SubProcess create_model() end ==================================
2025-02-28 13:46:53,801:INFO:Creating metrics dataframe
2025-02-28 13:46:53,806:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 13:46:53,806:INFO:Total runtime is 0.7810183922449748 minutes
2025-02-28 13:46:53,808:INFO:SubProcess create_model() called ==================================
2025-02-28 13:46:53,808:INFO:Initializing create_model()
2025-02-28 13:46:53,808:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:46:53,808:INFO:Checking exceptions
2025-02-28 13:46:53,808:INFO:Importing libraries
2025-02-28 13:46:53,808:INFO:Copying training dataset
2025-02-28 13:46:53,810:INFO:Defining folds
2025-02-28 13:46:53,810:INFO:Declaring metric variables
2025-02-28 13:46:53,812:INFO:Importing untrained model
2025-02-28 13:46:53,814:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:46:53,818:INFO:Starting cross validation
2025-02-28 13:46:53,819:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:46:53,844:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:46:53,844:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:46:53,844:INFO:[LightGBM] [Info] Total Bins 984
2025-02-28 13:46:53,844:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:46:53,893:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:46:53,893:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:46:55,890:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:46:55,892:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:46:55,893:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000943 secs. 0 sparse feature groups
2025-02-28 13:46:55,893:INFO:[LightGBM] [Info] Start training from score -1.772891
2025-02-28 13:46:55,893:INFO:[LightGBM] [Info] Start training from score -1.805182
2025-02-28 13:46:55,893:INFO:[LightGBM] [Info] Start training from score -1.765442
2025-02-28 13:46:55,893:INFO:[LightGBM] [Info] Start training from score -1.799408
2025-02-28 13:46:55,893:INFO:[LightGBM] [Info] Start training from score -1.828621
2025-02-28 13:46:55,893:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:46:58,883:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:46:58,883:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:46:58,883:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 13:46:58,883:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:46:58,937:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:46:58,937:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:46:59,020:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:46:59,022:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:46:59,023:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000914 secs. 0 sparse feature groups
2025-02-28 13:46:59,023:INFO:[LightGBM] [Info] Start training from score -1.772891
2025-02-28 13:46:59,023:INFO:[LightGBM] [Info] Start training from score -1.805182
2025-02-28 13:46:59,023:INFO:[LightGBM] [Info] Start training from score -1.765442
2025-02-28 13:46:59,023:INFO:[LightGBM] [Info] Start training from score -1.799408
2025-02-28 13:46:59,023:INFO:[LightGBM] [Info] Start training from score -1.828621
2025-02-28 13:46:59,023:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:47:02,083:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:47:02,083:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:47:02,083:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 13:47:02,083:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:47:02,133:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:47:02,133:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:47:02,144:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:47:02,146:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:47:02,147:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000923 secs. 0 sparse feature groups
2025-02-28 13:47:02,147:INFO:[LightGBM] [Info] Start training from score -1.772891
2025-02-28 13:47:02,147:INFO:[LightGBM] [Info] Start training from score -1.805182
2025-02-28 13:47:02,147:INFO:[LightGBM] [Info] Start training from score -1.763589
2025-02-28 13:47:02,147:INFO:[LightGBM] [Info] Start training from score -1.801329
2025-02-28 13:47:02,147:INFO:[LightGBM] [Info] Start training from score -1.828621
2025-02-28 13:47:02,147:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:47:05,138:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:47:05,138:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:47:05,138:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 13:47:05,138:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:47:05,186:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:47:05,186:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:47:05,196:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:47:05,198:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:47:05,199:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000887 secs. 0 sparse feature groups
2025-02-28 13:47:05,199:INFO:[LightGBM] [Info] Start training from score -1.774762
2025-02-28 13:47:05,199:INFO:[LightGBM] [Info] Start training from score -1.803254
2025-02-28 13:47:05,199:INFO:[LightGBM] [Info] Start training from score -1.765442
2025-02-28 13:47:05,199:INFO:[LightGBM] [Info] Start training from score -1.801329
2025-02-28 13:47:05,199:INFO:[LightGBM] [Info] Start training from score -1.826647
2025-02-28 13:47:05,199:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:47:08,204:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:47:08,204:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:47:08,204:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 13:47:08,204:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:47:08,251:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:47:08,251:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:47:08,260:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:47:08,262:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:47:08,262:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000836 secs. 0 sparse feature groups
2025-02-28 13:47:08,263:INFO:[LightGBM] [Info] Start training from score -1.774762
2025-02-28 13:47:08,263:INFO:[LightGBM] [Info] Start training from score -1.803254
2025-02-28 13:47:08,263:INFO:[LightGBM] [Info] Start training from score -1.765442
2025-02-28 13:47:08,263:INFO:[LightGBM] [Info] Start training from score -1.801329
2025-02-28 13:47:08,263:INFO:[LightGBM] [Info] Start training from score -1.826647
2025-02-28 13:47:08,263:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:47:11,222:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:47:11,222:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:47:11,222:INFO:[LightGBM] [Info] Total Bins 984
2025-02-28 13:47:11,222:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:47:11,269:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:47:11,269:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:47:11,279:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:47:11,281:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:47:11,282:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000844 secs. 0 sparse feature groups
2025-02-28 13:47:11,283:INFO:[LightGBM] [Info] Start training from score -1.774762
2025-02-28 13:47:11,283:INFO:[LightGBM] [Info] Start training from score -1.803254
2025-02-28 13:47:11,283:INFO:[LightGBM] [Info] Start training from score -1.765442
2025-02-28 13:47:11,283:INFO:[LightGBM] [Info] Start training from score -1.801329
2025-02-28 13:47:11,283:INFO:[LightGBM] [Info] Start training from score -1.826647
2025-02-28 13:47:11,283:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:47:14,280:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:47:14,280:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:47:14,282:INFO:[LightGBM] [Info] Total Bins 984
2025-02-28 13:47:14,282:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:47:14,328:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:47:14,328:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:47:14,339:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:47:14,340:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:47:14,341:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000841 secs. 0 sparse feature groups
2025-02-28 13:47:14,342:INFO:[LightGBM] [Info] Start training from score -1.774762
2025-02-28 13:47:14,342:INFO:[LightGBM] [Info] Start training from score -1.803254
2025-02-28 13:47:14,342:INFO:[LightGBM] [Info] Start training from score -1.765442
2025-02-28 13:47:14,342:INFO:[LightGBM] [Info] Start training from score -1.801329
2025-02-28 13:47:14,342:INFO:[LightGBM] [Info] Start training from score -1.826647
2025-02-28 13:47:14,342:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:47:17,413:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:47:17,413:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:47:17,413:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 13:47:17,413:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:47:17,459:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:47:17,459:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:47:17,470:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:47:17,472:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:47:17,472:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000967 secs. 0 sparse feature groups
2025-02-28 13:47:17,472:INFO:[LightGBM] [Info] Start training from score -1.772891
2025-02-28 13:47:17,472:INFO:[LightGBM] [Info] Start training from score -1.805182
2025-02-28 13:47:17,473:INFO:[LightGBM] [Info] Start training from score -1.765442
2025-02-28 13:47:17,473:INFO:[LightGBM] [Info] Start training from score -1.801329
2025-02-28 13:47:17,473:INFO:[LightGBM] [Info] Start training from score -1.826647
2025-02-28 13:47:17,473:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:47:20,480:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:47:20,480:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:47:20,480:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 13:47:20,480:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:47:20,529:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:47:20,529:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:47:20,539:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:47:20,541:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:47:20,542:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000835 secs. 0 sparse feature groups
2025-02-28 13:47:20,542:INFO:[LightGBM] [Info] Start training from score -1.772891
2025-02-28 13:47:20,542:INFO:[LightGBM] [Info] Start training from score -1.805182
2025-02-28 13:47:20,542:INFO:[LightGBM] [Info] Start training from score -1.765442
2025-02-28 13:47:20,542:INFO:[LightGBM] [Info] Start training from score -1.801329
2025-02-28 13:47:20,542:INFO:[LightGBM] [Info] Start training from score -1.826647
2025-02-28 13:47:20,542:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:47:23,596:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:47:23,596:INFO:[LightGBM] [Info] This is the GPU trainer!!
2025-02-28 13:47:23,596:INFO:[LightGBM] [Info] Total Bins 984
2025-02-28 13:47:23,596:INFO:[LightGBM] [Info] Number of data points in the train set: 3150, number of used features: 20
2025-02-28 13:47:23,643:INFO:[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 Ti SUPER, Vendor: NVIDIA Corporation
2025-02-28 13:47:23,644:INFO:[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...
2025-02-28 13:47:23,653:INFO:[LightGBM] [Info] GPU programs have been built
2025-02-28 13:47:23,655:INFO:[LightGBM] [Info] Size of histogram bin entry: 8
2025-02-28 13:47:23,656:INFO:[LightGBM] [Info] 12 dense feature groups (0.04 MB) transferred to GPU in 0.000837 secs. 0 sparse feature groups
2025-02-28 13:47:23,656:INFO:[LightGBM] [Info] Start training from score -1.772891
2025-02-28 13:47:23,656:INFO:[LightGBM] [Info] Start training from score -1.805182
2025-02-28 13:47:23,656:INFO:[LightGBM] [Info] Start training from score -1.765442
2025-02-28 13:47:23,656:INFO:[LightGBM] [Info] Start training from score -1.801329
2025-02-28 13:47:23,656:INFO:[LightGBM] [Info] Start training from score -1.826647
2025-02-28 13:47:23,656:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:47:26,624:INFO:Calculating mean and std
2025-02-28 13:47:26,625:INFO:Creating metrics dataframe
2025-02-28 13:47:26,626:INFO:Uploading results into container
2025-02-28 13:47:26,626:INFO:Uploading model into container now
2025-02-28 13:47:26,626:INFO:_master_model_container: 14
2025-02-28 13:47:26,626:INFO:_display_container: 2
2025-02-28 13:47:26,627:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               device='gpu', importance_type='split', learning_rate=0.1,
               max_depth=-1, min_child_samples=20, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,
               objective=None, random_state=123, reg_alpha=0.0, reg_lambda=0.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:47:26,627:INFO:create_model() successfully completed......................................
2025-02-28 13:47:26,714:INFO:SubProcess create_model() end ==================================
2025-02-28 13:47:26,714:INFO:Creating metrics dataframe
2025-02-28 13:47:26,722:INFO:Initializing CatBoost Classifier
2025-02-28 13:47:26,722:INFO:Total runtime is 1.3296103040377298 minutes
2025-02-28 13:47:26,724:INFO:SubProcess create_model() called ==================================
2025-02-28 13:47:26,724:INFO:Initializing create_model()
2025-02-28 13:47:26,724:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:47:26,724:INFO:Checking exceptions
2025-02-28 13:47:26,724:INFO:Importing libraries
2025-02-28 13:47:26,725:INFO:Copying training dataset
2025-02-28 13:47:26,727:INFO:Defining folds
2025-02-28 13:47:26,727:INFO:Declaring metric variables
2025-02-28 13:47:26,729:INFO:Importing untrained model
2025-02-28 13:47:26,731:INFO:CatBoost Classifier Imported successfully
2025-02-28 13:47:26,734:INFO:Starting cross validation
2025-02-28 13:47:26,735:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:48:12,974:INFO:Calculating mean and std
2025-02-28 13:48:12,975:INFO:Creating metrics dataframe
2025-02-28 13:48:12,976:INFO:Uploading results into container
2025-02-28 13:48:12,976:INFO:Uploading model into container now
2025-02-28 13:48:12,976:INFO:_master_model_container: 15
2025-02-28 13:48:12,976:INFO:_display_container: 2
2025-02-28 13:48:12,976:INFO:<catboost.core.CatBoostClassifier object at 0x000001EF2733FEB0>
2025-02-28 13:48:12,977:INFO:create_model() successfully completed......................................
2025-02-28 13:48:13,048:INFO:SubProcess create_model() end ==================================
2025-02-28 13:48:13,049:INFO:Creating metrics dataframe
2025-02-28 13:48:13,054:INFO:Initializing Dummy Classifier
2025-02-28 13:48:13,054:INFO:Total runtime is 2.1018205960591634 minutes
2025-02-28 13:48:13,056:INFO:SubProcess create_model() called ==================================
2025-02-28 13:48:13,056:INFO:Initializing create_model()
2025-02-28 13:48:13,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EF1E0DA200>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:48:13,056:INFO:Checking exceptions
2025-02-28 13:48:13,057:INFO:Importing libraries
2025-02-28 13:48:13,057:INFO:Copying training dataset
2025-02-28 13:48:13,059:INFO:Defining folds
2025-02-28 13:48:13,059:INFO:Declaring metric variables
2025-02-28 13:48:13,060:INFO:Importing untrained model
2025-02-28 13:48:13,063:INFO:Dummy Classifier Imported successfully
2025-02-28 13:48:13,066:INFO:Starting cross validation
2025-02-28 13:48:13,067:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=1
2025-02-28 13:48:13,102:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,138:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,173:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,209:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,244:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,279:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,314:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,350:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,385:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,421:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:48:13,423:INFO:Calculating mean and std
2025-02-28 13:48:13,423:INFO:Creating metrics dataframe
2025-02-28 13:48:13,425:INFO:Uploading results into container
2025-02-28 13:48:13,425:INFO:Uploading model into container now
2025-02-28 13:48:13,425:INFO:_master_model_container: 16
2025-02-28 13:48:13,425:INFO:_display_container: 2
2025-02-28 13:48:13,425:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 13:48:13,425:INFO:create_model() successfully completed......................................
2025-02-28 13:48:13,501:INFO:SubProcess create_model() end ==================================
2025-02-28 13:48:13,501:INFO:Creating metrics dataframe
2025-02-28 13:48:13,507:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 13:48:13,511:INFO:Initializing create_model()
2025-02-28 13:48:13,511:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, estimator=DummyClassifier(constant=None, random_state=123, strategy='prior'), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:48:13,511:INFO:Checking exceptions
2025-02-28 13:48:13,512:INFO:Importing libraries
2025-02-28 13:48:13,512:INFO:Copying training dataset
2025-02-28 13:48:13,514:INFO:Defining folds
2025-02-28 13:48:13,514:INFO:Declaring metric variables
2025-02-28 13:48:13,514:INFO:Importing untrained model
2025-02-28 13:48:13,515:INFO:Declaring custom model
2025-02-28 13:48:13,515:INFO:Dummy Classifier Imported successfully
2025-02-28 13:48:13,515:INFO:Cross validation set to False
2025-02-28 13:48:13,515:INFO:Fitting Model
2025-02-28 13:48:13,536:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 13:48:13,536:INFO:create_model() successfully completed......................................
2025-02-28 13:48:13,626:INFO:_master_model_container: 16
2025-02-28 13:48:13,626:INFO:_display_container: 2
2025-02-28 13:48:13,626:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 13:48:13,627:INFO:compare_models() successfully completed......................................
2025-02-28 13:48:13,653:INFO:Initializing plot_model()
2025-02-28 13:48:13,653:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=DummyClassifier(constant=None, random_state=123, strategy='prior'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, system=True)
2025-02-28 13:48:13,653:INFO:Checking exceptions
2025-02-28 13:48:13,655:INFO:Preloading libraries
2025-02-28 13:48:13,656:INFO:Copying training dataset
2025-02-28 13:48:13,656:INFO:Plot type: confusion_matrix
2025-02-28 13:48:13,751:INFO:Fitting Model
2025-02-28 13:48:13,751:INFO:Scoring test/hold-out set
2025-02-28 13:48:13,846:INFO:Visual Rendered Successfully
2025-02-28 13:48:13,932:INFO:plot_model() successfully completed......................................
2025-02-28 13:48:13,948:INFO:Initializing plot_model()
2025-02-28 13:48:13,948:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=DummyClassifier(constant=None, random_state=123, strategy='prior'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, system=True)
2025-02-28 13:48:13,948:INFO:Checking exceptions
2025-02-28 13:48:13,950:INFO:Preloading libraries
2025-02-28 13:48:13,951:INFO:Copying training dataset
2025-02-28 13:48:13,951:INFO:Plot type: auc
2025-02-28 13:48:14,047:INFO:Fitting Model
2025-02-28 13:48:14,047:INFO:Scoring test/hold-out set
2025-02-28 13:48:14,156:INFO:Visual Rendered Successfully
2025-02-28 13:48:14,239:INFO:plot_model() successfully completed......................................
2025-02-28 13:48:14,256:INFO:Initializing plot_model()
2025-02-28 13:48:14,256:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=DummyClassifier(constant=None, random_state=123, strategy='prior'), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EF7EC680D0>, system=True)
2025-02-28 13:48:14,257:INFO:Checking exceptions
2025-02-28 13:49:02,386:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:49:02,386:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:49:02,386:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:49:02,386:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-02-28 13:49:03,755:INFO:PyCaret ClassificationExperiment
2025-02-28 13:49:03,755:INFO:Logging name: clf-default-name
2025-02-28 13:49:03,755:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-02-28 13:49:03,755:INFO:version 3.3.2
2025-02-28 13:49:03,755:INFO:Initializing setup()
2025-02-28 13:49:03,755:INFO:self.USI: 594f
2025-02-28 13:49:03,756:INFO:self._variable_keys: {'X', 'gpu_n_jobs_param', 'seed', 'USI', 'y_train', 'X_test', '_available_plots', 'X_train', 'logging_param', 'exp_name_log', 'memory', 'target_param', 'pipeline', 'n_jobs_param', 'log_plots_param', 'y_test', 'fix_imbalance', 'fold_shuffle_param', 'is_multiclass', 'exp_id', '_ml_usecase', 'gpu_param', 'idx', 'fold_groups_param', 'fold_generator', 'y', 'html_param', 'data'}
2025-02-28 13:49:03,756:INFO:Checking environment
2025-02-28 13:49:03,756:INFO:python_version: 3.10.16
2025-02-28 13:49:03,756:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 13:49:03,756:INFO:machine: AMD64
2025-02-28 13:49:03,756:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 13:49:03,756:INFO:Memory: svmem(total=34200334336, available=17376006144, percent=49.2, used=16824328192, free=17376006144)
2025-02-28 13:49:03,756:INFO:Physical Core: 24
2025-02-28 13:49:03,756:INFO:Logical Core: 32
2025-02-28 13:49:03,756:INFO:Checking libraries
2025-02-28 13:49:03,756:INFO:System:
2025-02-28 13:49:03,756:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 13:49:03,756:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 13:49:03,756:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 13:49:03,756:INFO:PyCaret required dependencies:
2025-02-28 13:49:04,297:INFO:                 pip: 25.0
2025-02-28 13:49:04,297:INFO:          setuptools: 75.8.0
2025-02-28 13:49:04,297:INFO:             pycaret: 3.3.2
2025-02-28 13:49:04,297:INFO:             IPython: 8.30.0
2025-02-28 13:49:04,297:INFO:          ipywidgets: 8.1.5
2025-02-28 13:49:04,297:INFO:                tqdm: 4.67.1
2025-02-28 13:49:04,297:INFO:               numpy: 1.26.4
2025-02-28 13:49:04,297:INFO:              pandas: 2.1.4
2025-02-28 13:49:04,297:INFO:              jinja2: 3.1.5
2025-02-28 13:49:04,298:INFO:               scipy: 1.11.4
2025-02-28 13:49:04,298:INFO:              joblib: 1.3.2
2025-02-28 13:49:04,298:INFO:             sklearn: 1.4.2
2025-02-28 13:49:04,298:INFO:                pyod: 2.0.3
2025-02-28 13:49:04,298:INFO:            imblearn: 0.13.0
2025-02-28 13:49:04,298:INFO:   category_encoders: 2.7.0
2025-02-28 13:49:04,298:INFO:            lightgbm: 4.5.0
2025-02-28 13:49:04,298:INFO:               numba: 0.61.0
2025-02-28 13:49:04,298:INFO:            requests: 2.32.3
2025-02-28 13:49:04,298:INFO:          matplotlib: 3.7.5
2025-02-28 13:49:04,298:INFO:          scikitplot: 0.3.7
2025-02-28 13:49:04,298:INFO:         yellowbrick: 1.5
2025-02-28 13:49:04,298:INFO:              plotly: 5.24.1
2025-02-28 13:49:04,298:INFO:    plotly-resampler: Not installed
2025-02-28 13:49:04,298:INFO:             kaleido: 0.2.1
2025-02-28 13:49:04,298:INFO:           schemdraw: 0.15
2025-02-28 13:49:04,298:INFO:         statsmodels: 0.14.4
2025-02-28 13:49:04,298:INFO:              sktime: 0.26.0
2025-02-28 13:49:04,299:INFO:               tbats: 1.1.3
2025-02-28 13:49:04,299:INFO:            pmdarima: 2.0.4
2025-02-28 13:49:04,299:INFO:              psutil: 5.9.0
2025-02-28 13:49:04,299:INFO:          markupsafe: 2.1.5
2025-02-28 13:49:04,299:INFO:             pickle5: Not installed
2025-02-28 13:49:04,299:INFO:         cloudpickle: 3.1.1
2025-02-28 13:49:04,299:INFO:         deprecation: 2.1.0
2025-02-28 13:49:04,299:INFO:              xxhash: 3.5.0
2025-02-28 13:49:04,299:INFO:           wurlitzer: Not installed
2025-02-28 13:49:04,299:INFO:PyCaret optional dependencies:
2025-02-28 13:49:06,057:INFO:                shap: 0.44.1
2025-02-28 13:49:06,057:INFO:           interpret: 0.6.9
2025-02-28 13:49:06,057:INFO:                umap: 0.5.7
2025-02-28 13:49:06,057:INFO:     ydata_profiling: 4.12.2
2025-02-28 13:49:06,057:INFO:  explainerdashboard: 0.4.8
2025-02-28 13:49:06,057:INFO:             autoviz: Not installed
2025-02-28 13:49:06,057:INFO:           fairlearn: 0.7.0
2025-02-28 13:49:06,057:INFO:          deepchecks: Not installed
2025-02-28 13:49:06,057:INFO:             xgboost: 2.1.4
2025-02-28 13:49:06,057:INFO:            catboost: 1.2.7
2025-02-28 13:49:06,057:INFO:              kmodes: 0.12.2
2025-02-28 13:49:06,057:INFO:             mlxtend: 0.23.4
2025-02-28 13:49:06,057:INFO:       statsforecast: 1.5.0
2025-02-28 13:49:06,057:INFO:        tune_sklearn: Not installed
2025-02-28 13:49:06,057:INFO:                 ray: Not installed
2025-02-28 13:49:06,057:INFO:            hyperopt: 0.2.7
2025-02-28 13:49:06,057:INFO:              optuna: 4.2.0
2025-02-28 13:49:06,057:INFO:               skopt: 0.10.2
2025-02-28 13:49:06,057:INFO:              mlflow: 2.20.1
2025-02-28 13:49:06,057:INFO:              gradio: 5.15.0
2025-02-28 13:49:06,057:INFO:             fastapi: 0.115.8
2025-02-28 13:49:06,057:INFO:             uvicorn: 0.34.0
2025-02-28 13:49:06,057:INFO:              m2cgen: 0.10.0
2025-02-28 13:49:06,057:INFO:           evidently: 0.4.40
2025-02-28 13:49:06,057:INFO:               fugue: 0.8.7
2025-02-28 13:49:06,057:INFO:           streamlit: Not installed
2025-02-28 13:49:06,057:INFO:             prophet: Not installed
2025-02-28 13:49:06,057:INFO:None
2025-02-28 13:49:06,057:INFO:Set up data.
2025-02-28 13:49:06,062:INFO:Set up folding strategy.
2025-02-28 13:49:06,062:INFO:Set up train/test split.
2025-02-28 13:49:06,065:INFO:Set up index.
2025-02-28 13:49:06,065:INFO:Assigning column types.
2025-02-28 13:49:06,068:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 13:49:06,087:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:06,089:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:49:06,105:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:06,108:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:06,141:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:06,142:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:49:06,154:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:06,156:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:06,156:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 13:49:06,176:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:49:06,188:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:06,190:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:06,211:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-02-28 13:49:06,223:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:06,224:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:06,225:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-02-28 13:49:06,257:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:06,259:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:06,292:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:06,293:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:06,295:INFO:Preparing preprocessing pipeline...
2025-02-28 13:49:06,295:INFO:Set up simple imputation.
2025-02-28 13:49:06,297:INFO:Set up encoding of categorical features.
2025-02-28 13:49:06,326:INFO:Finished creating preprocessing pipeline.
2025-02-28 13:49:06,330:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 13:49:06,330:INFO:Creating final display dataframe.
2025-02-28 13:49:06,427:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        Job_Offers
2                   Target type        Multiclass
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              594f
2025-02-28 13:49:06,463:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:06,465:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:06,497:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:06,498:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:06,499:INFO:setup() successfully completed in 2.74s...............
2025-02-28 13:49:06,517:INFO:Initializing compare_models()
2025-02-28 13:49:06,517:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-02-28 13:49:06,517:INFO:Checking exceptions
2025-02-28 13:49:06,520:INFO:Preparing display monitor
2025-02-28 13:49:06,538:INFO:Initializing Logistic Regression
2025-02-28 13:49:06,538:INFO:Total runtime is 0.0 minutes
2025-02-28 13:49:06,542:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:06,542:INFO:Initializing create_model()
2025-02-28 13:49:06,542:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:06,542:INFO:Checking exceptions
2025-02-28 13:49:06,542:INFO:Importing libraries
2025-02-28 13:49:06,542:INFO:Copying training dataset
2025-02-28 13:49:06,545:INFO:Defining folds
2025-02-28 13:49:06,545:INFO:Declaring metric variables
2025-02-28 13:49:06,548:INFO:Importing untrained model
2025-02-28 13:49:06,550:INFO:Logistic Regression Imported successfully
2025-02-28 13:49:06,555:INFO:Starting cross validation
2025-02-28 13:49:06,556:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:09,955:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:09,968:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,025:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:10,026:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:10,030:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:10,039:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,039:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:10,043:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,043:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,053:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,059:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:10,074:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,075:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:10,078:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:10,079:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:10,088:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,088:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-02-28 13:49:10,091:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,095:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,102:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:10,112:INFO:Calculating mean and std
2025-02-28 13:49:10,113:INFO:Creating metrics dataframe
2025-02-28 13:49:10,115:INFO:Uploading results into container
2025-02-28 13:49:10,116:INFO:Uploading model into container now
2025-02-28 13:49:10,116:INFO:_master_model_container: 1
2025-02-28 13:49:10,116:INFO:_display_container: 2
2025-02-28 13:49:10,117:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-02-28 13:49:10,117:INFO:create_model() successfully completed......................................
2025-02-28 13:49:10,199:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:10,199:INFO:Creating metrics dataframe
2025-02-28 13:49:10,202:INFO:Initializing K Neighbors Classifier
2025-02-28 13:49:10,202:INFO:Total runtime is 0.06106484731038411 minutes
2025-02-28 13:49:10,204:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:10,204:INFO:Initializing create_model()
2025-02-28 13:49:10,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:10,204:INFO:Checking exceptions
2025-02-28 13:49:10,204:INFO:Importing libraries
2025-02-28 13:49:10,204:INFO:Copying training dataset
2025-02-28 13:49:10,207:INFO:Defining folds
2025-02-28 13:49:10,207:INFO:Declaring metric variables
2025-02-28 13:49:10,208:INFO:Importing untrained model
2025-02-28 13:49:10,211:INFO:K Neighbors Classifier Imported successfully
2025-02-28 13:49:10,215:INFO:Starting cross validation
2025-02-28 13:49:10,215:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:13,003:INFO:Calculating mean and std
2025-02-28 13:49:13,004:INFO:Creating metrics dataframe
2025-02-28 13:49:13,005:INFO:Uploading results into container
2025-02-28 13:49:13,005:INFO:Uploading model into container now
2025-02-28 13:49:13,006:INFO:_master_model_container: 2
2025-02-28 13:49:13,006:INFO:_display_container: 2
2025-02-28 13:49:13,006:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-02-28 13:49:13,006:INFO:create_model() successfully completed......................................
2025-02-28 13:49:13,087:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:13,087:INFO:Creating metrics dataframe
2025-02-28 13:49:13,090:INFO:Initializing Naive Bayes
2025-02-28 13:49:13,090:INFO:Total runtime is 0.10919458468755086 minutes
2025-02-28 13:49:13,092:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:13,092:INFO:Initializing create_model()
2025-02-28 13:49:13,092:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:13,092:INFO:Checking exceptions
2025-02-28 13:49:13,092:INFO:Importing libraries
2025-02-28 13:49:13,092:INFO:Copying training dataset
2025-02-28 13:49:13,095:INFO:Defining folds
2025-02-28 13:49:13,095:INFO:Declaring metric variables
2025-02-28 13:49:13,097:INFO:Importing untrained model
2025-02-28 13:49:13,098:INFO:Naive Bayes Imported successfully
2025-02-28 13:49:13,103:INFO:Starting cross validation
2025-02-28 13:49:13,104:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:15,869:INFO:Calculating mean and std
2025-02-28 13:49:15,870:INFO:Creating metrics dataframe
2025-02-28 13:49:15,871:INFO:Uploading results into container
2025-02-28 13:49:15,872:INFO:Uploading model into container now
2025-02-28 13:49:15,872:INFO:_master_model_container: 3
2025-02-28 13:49:15,872:INFO:_display_container: 2
2025-02-28 13:49:15,872:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-02-28 13:49:15,872:INFO:create_model() successfully completed......................................
2025-02-28 13:49:15,953:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:15,953:INFO:Creating metrics dataframe
2025-02-28 13:49:15,957:INFO:Initializing Decision Tree Classifier
2025-02-28 13:49:15,957:INFO:Total runtime is 0.156984015305837 minutes
2025-02-28 13:49:15,959:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:15,960:INFO:Initializing create_model()
2025-02-28 13:49:15,960:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:15,960:INFO:Checking exceptions
2025-02-28 13:49:15,960:INFO:Importing libraries
2025-02-28 13:49:15,960:INFO:Copying training dataset
2025-02-28 13:49:15,963:INFO:Defining folds
2025-02-28 13:49:15,963:INFO:Declaring metric variables
2025-02-28 13:49:15,964:INFO:Importing untrained model
2025-02-28 13:49:15,967:INFO:Decision Tree Classifier Imported successfully
2025-02-28 13:49:15,971:INFO:Starting cross validation
2025-02-28 13:49:15,972:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:17,994:INFO:Calculating mean and std
2025-02-28 13:49:17,995:INFO:Creating metrics dataframe
2025-02-28 13:49:17,996:INFO:Uploading results into container
2025-02-28 13:49:17,996:INFO:Uploading model into container now
2025-02-28 13:49:17,997:INFO:_master_model_container: 4
2025-02-28 13:49:17,997:INFO:_display_container: 2
2025-02-28 13:49:17,997:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 13:49:17,997:INFO:create_model() successfully completed......................................
2025-02-28 13:49:18,075:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:18,075:INFO:Creating metrics dataframe
2025-02-28 13:49:18,079:INFO:Initializing SVM - Linear Kernel
2025-02-28 13:49:18,079:INFO:Total runtime is 0.19234358469645182 minutes
2025-02-28 13:49:18,081:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:18,081:INFO:Initializing create_model()
2025-02-28 13:49:18,081:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:18,081:INFO:Checking exceptions
2025-02-28 13:49:18,081:INFO:Importing libraries
2025-02-28 13:49:18,081:INFO:Copying training dataset
2025-02-28 13:49:18,084:INFO:Defining folds
2025-02-28 13:49:18,084:INFO:Declaring metric variables
2025-02-28 13:49:18,086:INFO:Importing untrained model
2025-02-28 13:49:18,088:INFO:SVM - Linear Kernel Imported successfully
2025-02-28 13:49:18,092:INFO:Starting cross validation
2025-02-28 13:49:18,093:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:18,205:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,207:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,210:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,211:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,216:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,219:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,220:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,223:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,232:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,232:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,235:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,235:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,246:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,246:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,246:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,246:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,249:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,249:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,249:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,249:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:18,260:INFO:Calculating mean and std
2025-02-28 13:49:18,260:INFO:Creating metrics dataframe
2025-02-28 13:49:18,262:INFO:Uploading results into container
2025-02-28 13:49:18,262:INFO:Uploading model into container now
2025-02-28 13:49:18,263:INFO:_master_model_container: 5
2025-02-28 13:49:18,263:INFO:_display_container: 2
2025-02-28 13:49:18,263:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 13:49:18,263:INFO:create_model() successfully completed......................................
2025-02-28 13:49:18,342:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:18,342:INFO:Creating metrics dataframe
2025-02-28 13:49:18,346:INFO:Initializing Ridge Classifier
2025-02-28 13:49:18,346:INFO:Total runtime is 0.1967901865641276 minutes
2025-02-28 13:49:18,348:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:18,348:INFO:Initializing create_model()
2025-02-28 13:49:18,348:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:18,349:INFO:Checking exceptions
2025-02-28 13:49:18,349:INFO:Importing libraries
2025-02-28 13:49:18,349:INFO:Copying training dataset
2025-02-28 13:49:18,351:INFO:Defining folds
2025-02-28 13:49:18,351:INFO:Declaring metric variables
2025-02-28 13:49:18,353:INFO:Importing untrained model
2025-02-28 13:49:18,355:INFO:Ridge Classifier Imported successfully
2025-02-28 13:49:18,359:INFO:Starting cross validation
2025-02-28 13:49:18,360:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:18,409:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,409:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,410:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,413:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,414:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,414:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,433:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,438:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,439:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,439:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:18,449:INFO:Calculating mean and std
2025-02-28 13:49:18,450:INFO:Creating metrics dataframe
2025-02-28 13:49:18,451:INFO:Uploading results into container
2025-02-28 13:49:18,451:INFO:Uploading model into container now
2025-02-28 13:49:18,451:INFO:_master_model_container: 6
2025-02-28 13:49:18,451:INFO:_display_container: 2
2025-02-28 13:49:18,452:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-02-28 13:49:18,452:INFO:create_model() successfully completed......................................
2025-02-28 13:49:18,526:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:18,526:INFO:Creating metrics dataframe
2025-02-28 13:49:18,531:INFO:Initializing Random Forest Classifier
2025-02-28 13:49:18,531:INFO:Total runtime is 0.19987731377283732 minutes
2025-02-28 13:49:18,533:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:18,533:INFO:Initializing create_model()
2025-02-28 13:49:18,533:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:18,533:INFO:Checking exceptions
2025-02-28 13:49:18,533:INFO:Importing libraries
2025-02-28 13:49:18,534:INFO:Copying training dataset
2025-02-28 13:49:18,536:INFO:Defining folds
2025-02-28 13:49:18,536:INFO:Declaring metric variables
2025-02-28 13:49:18,538:INFO:Importing untrained model
2025-02-28 13:49:18,540:INFO:Random Forest Classifier Imported successfully
2025-02-28 13:49:18,545:INFO:Starting cross validation
2025-02-28 13:49:18,546:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:18,964:INFO:Calculating mean and std
2025-02-28 13:49:18,965:INFO:Creating metrics dataframe
2025-02-28 13:49:18,966:INFO:Uploading results into container
2025-02-28 13:49:18,966:INFO:Uploading model into container now
2025-02-28 13:49:18,967:INFO:_master_model_container: 7
2025-02-28 13:49:18,967:INFO:_display_container: 2
2025-02-28 13:49:18,967:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-02-28 13:49:18,967:INFO:create_model() successfully completed......................................
2025-02-28 13:49:19,046:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:19,046:INFO:Creating metrics dataframe
2025-02-28 13:49:19,050:INFO:Initializing Quadratic Discriminant Analysis
2025-02-28 13:49:19,051:INFO:Total runtime is 0.20854291121164958 minutes
2025-02-28 13:49:19,053:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:19,053:INFO:Initializing create_model()
2025-02-28 13:49:19,053:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:19,053:INFO:Checking exceptions
2025-02-28 13:49:19,053:INFO:Importing libraries
2025-02-28 13:49:19,053:INFO:Copying training dataset
2025-02-28 13:49:19,055:INFO:Defining folds
2025-02-28 13:49:19,055:INFO:Declaring metric variables
2025-02-28 13:49:19,057:INFO:Importing untrained model
2025-02-28 13:49:19,060:INFO:Quadratic Discriminant Analysis Imported successfully
2025-02-28 13:49:19,064:INFO:Starting cross validation
2025-02-28 13:49:19,065:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:19,098:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,099:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,101:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,102:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,102:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,108:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,109:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,114:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,114:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,115:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,116:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,116:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:19,118:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:19,118:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,119:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,120:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:19,122:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,122:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-02-28 13:49:19,122:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:19,126:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,128:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:19,129:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,132:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,138:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,140:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:19,150:INFO:Calculating mean and std
2025-02-28 13:49:19,151:INFO:Creating metrics dataframe
2025-02-28 13:49:19,152:INFO:Uploading results into container
2025-02-28 13:49:19,152:INFO:Uploading model into container now
2025-02-28 13:49:19,152:INFO:_master_model_container: 8
2025-02-28 13:49:19,152:INFO:_display_container: 2
2025-02-28 13:49:19,152:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-02-28 13:49:19,152:INFO:create_model() successfully completed......................................
2025-02-28 13:49:19,229:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:19,230:INFO:Creating metrics dataframe
2025-02-28 13:49:19,235:INFO:Initializing Ada Boost Classifier
2025-02-28 13:49:19,235:INFO:Total runtime is 0.21161180337270102 minutes
2025-02-28 13:49:19,237:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:19,238:INFO:Initializing create_model()
2025-02-28 13:49:19,238:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:19,238:INFO:Checking exceptions
2025-02-28 13:49:19,238:INFO:Importing libraries
2025-02-28 13:49:19,238:INFO:Copying training dataset
2025-02-28 13:49:19,240:INFO:Defining folds
2025-02-28 13:49:19,240:INFO:Declaring metric variables
2025-02-28 13:49:19,242:INFO:Importing untrained model
2025-02-28 13:49:19,244:INFO:Ada Boost Classifier Imported successfully
2025-02-28 13:49:19,248:INFO:Starting cross validation
2025-02-28 13:49:19,249:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:19,281:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,282:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,285:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,288:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,290:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,290:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,294:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,297:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,300:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,305:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-02-28 13:49:19,435:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,436:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,438:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,444:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,445:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,448:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,453:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,453:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,457:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,458:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:19,474:INFO:Calculating mean and std
2025-02-28 13:49:19,475:INFO:Creating metrics dataframe
2025-02-28 13:49:19,476:INFO:Uploading results into container
2025-02-28 13:49:19,476:INFO:Uploading model into container now
2025-02-28 13:49:19,477:INFO:_master_model_container: 9
2025-02-28 13:49:19,477:INFO:_display_container: 2
2025-02-28 13:49:19,477:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-02-28 13:49:19,477:INFO:create_model() successfully completed......................................
2025-02-28 13:49:19,556:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:19,556:INFO:Creating metrics dataframe
2025-02-28 13:49:19,560:INFO:Initializing Gradient Boosting Classifier
2025-02-28 13:49:19,560:INFO:Total runtime is 0.21703295310338339 minutes
2025-02-28 13:49:19,564:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:19,564:INFO:Initializing create_model()
2025-02-28 13:49:19,564:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:19,564:INFO:Checking exceptions
2025-02-28 13:49:19,564:INFO:Importing libraries
2025-02-28 13:49:19,564:INFO:Copying training dataset
2025-02-28 13:49:19,566:INFO:Defining folds
2025-02-28 13:49:19,566:INFO:Declaring metric variables
2025-02-28 13:49:19,568:INFO:Importing untrained model
2025-02-28 13:49:19,571:INFO:Gradient Boosting Classifier Imported successfully
2025-02-28 13:49:19,574:INFO:Starting cross validation
2025-02-28 13:49:19,575:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:21,642:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,672:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,685:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,711:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,716:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,744:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,755:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,765:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,782:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,805:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,824:INFO:Calculating mean and std
2025-02-28 13:49:21,824:INFO:Creating metrics dataframe
2025-02-28 13:49:21,826:INFO:Uploading results into container
2025-02-28 13:49:21,826:INFO:Uploading model into container now
2025-02-28 13:49:21,826:INFO:_master_model_container: 10
2025-02-28 13:49:21,826:INFO:_display_container: 2
2025-02-28 13:49:21,827:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 13:49:21,827:INFO:create_model() successfully completed......................................
2025-02-28 13:49:21,906:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:21,906:INFO:Creating metrics dataframe
2025-02-28 13:49:21,912:INFO:Initializing Linear Discriminant Analysis
2025-02-28 13:49:21,912:INFO:Total runtime is 0.2562282164891561 minutes
2025-02-28 13:49:21,914:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:21,914:INFO:Initializing create_model()
2025-02-28 13:49:21,914:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:21,914:INFO:Checking exceptions
2025-02-28 13:49:21,914:INFO:Importing libraries
2025-02-28 13:49:21,914:INFO:Copying training dataset
2025-02-28 13:49:21,917:INFO:Defining folds
2025-02-28 13:49:21,917:INFO:Declaring metric variables
2025-02-28 13:49:21,919:INFO:Importing untrained model
2025-02-28 13:49:21,920:INFO:Linear Discriminant Analysis Imported successfully
2025-02-28 13:49:21,925:INFO:Starting cross validation
2025-02-28 13:49:21,926:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:21,972:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,978:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,982:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,983:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,985:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,987:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,989:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,991:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,991:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:21,991:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-02-28 13:49:22,013:INFO:Calculating mean and std
2025-02-28 13:49:22,014:INFO:Creating metrics dataframe
2025-02-28 13:49:22,015:INFO:Uploading results into container
2025-02-28 13:49:22,015:INFO:Uploading model into container now
2025-02-28 13:49:22,015:INFO:_master_model_container: 11
2025-02-28 13:49:22,016:INFO:_display_container: 2
2025-02-28 13:49:22,016:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-02-28 13:49:22,016:INFO:create_model() successfully completed......................................
2025-02-28 13:49:22,089:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:22,089:INFO:Creating metrics dataframe
2025-02-28 13:49:22,095:INFO:Initializing Extra Trees Classifier
2025-02-28 13:49:22,095:INFO:Total runtime is 0.2592737118403117 minutes
2025-02-28 13:49:22,097:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:22,098:INFO:Initializing create_model()
2025-02-28 13:49:22,098:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:22,098:INFO:Checking exceptions
2025-02-28 13:49:22,098:INFO:Importing libraries
2025-02-28 13:49:22,098:INFO:Copying training dataset
2025-02-28 13:49:22,100:INFO:Defining folds
2025-02-28 13:49:22,100:INFO:Declaring metric variables
2025-02-28 13:49:22,102:INFO:Importing untrained model
2025-02-28 13:49:22,104:INFO:Extra Trees Classifier Imported successfully
2025-02-28 13:49:22,108:INFO:Starting cross validation
2025-02-28 13:49:22,109:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:22,495:INFO:Calculating mean and std
2025-02-28 13:49:22,496:INFO:Creating metrics dataframe
2025-02-28 13:49:22,497:INFO:Uploading results into container
2025-02-28 13:49:22,497:INFO:Uploading model into container now
2025-02-28 13:49:22,497:INFO:_master_model_container: 12
2025-02-28 13:49:22,498:INFO:_display_container: 2
2025-02-28 13:49:22,498:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-02-28 13:49:22,498:INFO:create_model() successfully completed......................................
2025-02-28 13:49:22,576:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:22,576:INFO:Creating metrics dataframe
2025-02-28 13:49:22,581:INFO:Initializing Extreme Gradient Boosting
2025-02-28 13:49:22,581:INFO:Total runtime is 0.26737431287765506 minutes
2025-02-28 13:49:22,583:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:22,583:INFO:Initializing create_model()
2025-02-28 13:49:22,583:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:22,583:INFO:Checking exceptions
2025-02-28 13:49:22,583:INFO:Importing libraries
2025-02-28 13:49:22,583:INFO:Copying training dataset
2025-02-28 13:49:22,586:INFO:Defining folds
2025-02-28 13:49:22,586:INFO:Declaring metric variables
2025-02-28 13:49:22,587:INFO:Importing untrained model
2025-02-28 13:49:22,590:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 13:49:22,594:INFO:Starting cross validation
2025-02-28 13:49:22,595:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:23,479:INFO:Calculating mean and std
2025-02-28 13:49:23,480:INFO:Creating metrics dataframe
2025-02-28 13:49:23,480:INFO:Uploading results into container
2025-02-28 13:49:23,482:INFO:Uploading model into container now
2025-02-28 13:49:23,482:INFO:_master_model_container: 13
2025-02-28 13:49:23,482:INFO:_display_container: 2
2025-02-28 13:49:23,483:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-02-28 13:49:23,483:INFO:create_model() successfully completed......................................
2025-02-28 13:49:23,559:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:23,559:INFO:Creating metrics dataframe
2025-02-28 13:49:23,564:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 13:49:23,564:INFO:Total runtime is 0.28375877936681115 minutes
2025-02-28 13:49:23,567:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:23,567:INFO:Initializing create_model()
2025-02-28 13:49:23,567:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:23,567:INFO:Checking exceptions
2025-02-28 13:49:23,568:INFO:Importing libraries
2025-02-28 13:49:23,568:INFO:Copying training dataset
2025-02-28 13:49:23,570:INFO:Defining folds
2025-02-28 13:49:23,570:INFO:Declaring metric variables
2025-02-28 13:49:23,572:INFO:Importing untrained model
2025-02-28 13:49:23,574:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:49:23,578:INFO:Starting cross validation
2025-02-28 13:49:23,579:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:31,610:INFO:Calculating mean and std
2025-02-28 13:49:31,610:INFO:Creating metrics dataframe
2025-02-28 13:49:31,613:INFO:Uploading results into container
2025-02-28 13:49:31,613:INFO:Uploading model into container now
2025-02-28 13:49:31,613:INFO:_master_model_container: 14
2025-02-28 13:49:31,613:INFO:_display_container: 2
2025-02-28 13:49:31,614:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:49:31,614:INFO:create_model() successfully completed......................................
2025-02-28 13:49:31,698:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:31,698:INFO:Creating metrics dataframe
2025-02-28 13:49:31,705:INFO:Initializing CatBoost Classifier
2025-02-28 13:49:31,705:INFO:Total runtime is 0.41944794654846196 minutes
2025-02-28 13:49:31,708:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:31,708:INFO:Initializing create_model()
2025-02-28 13:49:31,708:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:31,708:INFO:Checking exceptions
2025-02-28 13:49:31,708:INFO:Importing libraries
2025-02-28 13:49:31,708:INFO:Copying training dataset
2025-02-28 13:49:31,710:INFO:Defining folds
2025-02-28 13:49:31,710:INFO:Declaring metric variables
2025-02-28 13:49:31,713:INFO:Importing untrained model
2025-02-28 13:49:31,715:INFO:CatBoost Classifier Imported successfully
2025-02-28 13:49:31,720:INFO:Starting cross validation
2025-02-28 13:49:31,721:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:38,493:INFO:Calculating mean and std
2025-02-28 13:49:38,493:INFO:Creating metrics dataframe
2025-02-28 13:49:38,495:INFO:Uploading results into container
2025-02-28 13:49:38,495:INFO:Uploading model into container now
2025-02-28 13:49:38,495:INFO:_master_model_container: 15
2025-02-28 13:49:38,496:INFO:_display_container: 2
2025-02-28 13:49:38,496:INFO:<catboost.core.CatBoostClassifier object at 0x0000028C12D42380>
2025-02-28 13:49:38,496:INFO:create_model() successfully completed......................................
2025-02-28 13:49:38,574:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:38,574:INFO:Creating metrics dataframe
2025-02-28 13:49:38,579:INFO:Initializing Dummy Classifier
2025-02-28 13:49:38,579:INFO:Total runtime is 0.534009075164795 minutes
2025-02-28 13:49:38,581:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:38,582:INFO:Initializing create_model()
2025-02-28 13:49:38,582:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7F6D5F00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:38,582:INFO:Checking exceptions
2025-02-28 13:49:38,582:INFO:Importing libraries
2025-02-28 13:49:38,582:INFO:Copying training dataset
2025-02-28 13:49:38,584:INFO:Defining folds
2025-02-28 13:49:38,584:INFO:Declaring metric variables
2025-02-28 13:49:38,586:INFO:Importing untrained model
2025-02-28 13:49:38,588:INFO:Dummy Classifier Imported successfully
2025-02-28 13:49:38,592:INFO:Starting cross validation
2025-02-28 13:49:38,593:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:38,644:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,645:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,647:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,648:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,652:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,652:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,655:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,659:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,659:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,665:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-02-28 13:49:38,682:INFO:Calculating mean and std
2025-02-28 13:49:38,683:INFO:Creating metrics dataframe
2025-02-28 13:49:38,684:INFO:Uploading results into container
2025-02-28 13:49:38,684:INFO:Uploading model into container now
2025-02-28 13:49:38,685:INFO:_master_model_container: 16
2025-02-28 13:49:38,685:INFO:_display_container: 2
2025-02-28 13:49:38,685:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-02-28 13:49:38,685:INFO:create_model() successfully completed......................................
2025-02-28 13:49:38,762:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:38,762:INFO:Creating metrics dataframe
2025-02-28 13:49:38,768:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 13:49:38,773:INFO:Initializing create_model()
2025-02-28 13:49:38,773:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:38,773:INFO:Checking exceptions
2025-02-28 13:49:38,774:INFO:Importing libraries
2025-02-28 13:49:38,774:INFO:Copying training dataset
2025-02-28 13:49:38,777:INFO:Defining folds
2025-02-28 13:49:38,777:INFO:Declaring metric variables
2025-02-28 13:49:38,777:INFO:Importing untrained model
2025-02-28 13:49:38,777:INFO:Declaring custom model
2025-02-28 13:49:38,777:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:49:38,778:INFO:Cross validation set to False
2025-02-28 13:49:38,778:INFO:Fitting Model
2025-02-28 13:49:38,808:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2025-02-28 13:49:38,808:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000195 seconds.
2025-02-28 13:49:38,808:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-02-28 13:49:38,809:INFO:[LightGBM] [Info] Total Bins 983
2025-02-28 13:49:38,809:INFO:[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 20
2025-02-28 13:49:38,809:INFO:[LightGBM] [Info] Start training from score -1.773639
2025-02-28 13:49:38,809:INFO:[LightGBM] [Info] Start training from score -1.804411
2025-02-28 13:49:38,809:INFO:[LightGBM] [Info] Start training from score -1.765257
2025-02-28 13:49:38,809:INFO:[LightGBM] [Info] Start training from score -1.800944
2025-02-28 13:49:38,809:INFO:[LightGBM] [Info] Start training from score -1.827239
2025-02-28 13:49:38,809:INFO:[LightGBM] [Info] Start training from score -1.780396
2025-02-28 13:49:39,334:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:49:39,334:INFO:create_model() successfully completed......................................
2025-02-28 13:49:39,444:INFO:_master_model_container: 16
2025-02-28 13:49:39,444:INFO:_display_container: 2
2025-02-28 13:49:39,444:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:49:39,444:INFO:compare_models() successfully completed......................................
2025-02-28 13:49:39,474:INFO:Initializing plot_model()
2025-02-28 13:49:39,474:INFO:plot_model(plot=confusion_matrix, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, system=True)
2025-02-28 13:49:39,474:INFO:Checking exceptions
2025-02-28 13:49:39,476:INFO:Preloading libraries
2025-02-28 13:49:39,518:INFO:Copying training dataset
2025-02-28 13:49:39,518:INFO:Plot type: confusion_matrix
2025-02-28 13:49:39,643:INFO:Fitting Model
2025-02-28 13:49:39,644:INFO:Scoring test/hold-out set
2025-02-28 13:49:39,775:INFO:Visual Rendered Successfully
2025-02-28 13:49:39,855:INFO:plot_model() successfully completed......................................
2025-02-28 13:49:39,873:INFO:Initializing plot_model()
2025-02-28 13:49:39,873:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, system=True)
2025-02-28 13:49:39,873:INFO:Checking exceptions
2025-02-28 13:49:39,875:INFO:Preloading libraries
2025-02-28 13:49:39,926:INFO:Copying training dataset
2025-02-28 13:49:39,926:INFO:Plot type: auc
2025-02-28 13:49:40,042:INFO:Fitting Model
2025-02-28 13:49:40,043:INFO:Scoring test/hold-out set
2025-02-28 13:49:40,187:INFO:Visual Rendered Successfully
2025-02-28 13:49:40,265:INFO:plot_model() successfully completed......................................
2025-02-28 13:49:40,277:INFO:Initializing plot_model()
2025-02-28 13:49:40,277:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028C09413A90>, system=True)
2025-02-28 13:49:40,277:INFO:Checking exceptions
2025-02-28 13:49:40,280:INFO:Preloading libraries
2025-02-28 13:49:40,324:INFO:Copying training dataset
2025-02-28 13:49:40,325:INFO:Plot type: feature
2025-02-28 13:49:40,325:WARNING:No coef_ found. Trying feature_importances_
2025-02-28 13:49:40,445:INFO:Visual Rendered Successfully
2025-02-28 13:49:40,525:INFO:plot_model() successfully completed......................................
2025-02-28 13:49:40,546:INFO:PyCaret RegressionExperiment
2025-02-28 13:49:40,546:INFO:Logging name: reg-default-name
2025-02-28 13:49:40,546:INFO:ML Usecase: MLUsecase.REGRESSION
2025-02-28 13:49:40,547:INFO:version 3.3.2
2025-02-28 13:49:40,547:INFO:Initializing setup()
2025-02-28 13:49:40,547:INFO:self.USI: 72e4
2025-02-28 13:49:40,547:INFO:self._variable_keys: {'X', 'gpu_n_jobs_param', 'transform_target_param', 'seed', 'USI', 'y_train', 'X_test', '_available_plots', 'X_train', 'logging_param', 'exp_name_log', 'memory', 'target_param', 'pipeline', 'n_jobs_param', 'log_plots_param', 'y_test', 'fold_shuffle_param', 'exp_id', '_ml_usecase', 'gpu_param', 'idx', 'fold_groups_param', 'fold_generator', 'y', 'html_param', 'data'}
2025-02-28 13:49:40,547:INFO:Checking environment
2025-02-28 13:49:40,547:INFO:python_version: 3.10.16
2025-02-28 13:49:40,547:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-02-28 13:49:40,547:INFO:machine: AMD64
2025-02-28 13:49:40,547:INFO:platform: Windows-10-10.0.26100-SP0
2025-02-28 13:49:40,547:INFO:Memory: svmem(total=34200334336, available=10663497728, percent=68.8, used=23536836608, free=10663497728)
2025-02-28 13:49:40,547:INFO:Physical Core: 24
2025-02-28 13:49:40,547:INFO:Logical Core: 32
2025-02-28 13:49:40,547:INFO:Checking libraries
2025-02-28 13:49:40,547:INFO:System:
2025-02-28 13:49:40,547:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-02-28 13:49:40,547:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-02-28 13:49:40,547:INFO:   machine: Windows-10-10.0.26100-SP0
2025-02-28 13:49:40,547:INFO:PyCaret required dependencies:
2025-02-28 13:49:40,547:INFO:                 pip: 25.0
2025-02-28 13:49:40,547:INFO:          setuptools: 75.8.0
2025-02-28 13:49:40,547:INFO:             pycaret: 3.3.2
2025-02-28 13:49:40,547:INFO:             IPython: 8.30.0
2025-02-28 13:49:40,548:INFO:          ipywidgets: 8.1.5
2025-02-28 13:49:40,548:INFO:                tqdm: 4.67.1
2025-02-28 13:49:40,548:INFO:               numpy: 1.26.4
2025-02-28 13:49:40,548:INFO:              pandas: 2.1.4
2025-02-28 13:49:40,548:INFO:              jinja2: 3.1.5
2025-02-28 13:49:40,548:INFO:               scipy: 1.11.4
2025-02-28 13:49:40,548:INFO:              joblib: 1.3.2
2025-02-28 13:49:40,548:INFO:             sklearn: 1.4.2
2025-02-28 13:49:40,548:INFO:                pyod: 2.0.3
2025-02-28 13:49:40,548:INFO:            imblearn: 0.13.0
2025-02-28 13:49:40,548:INFO:   category_encoders: 2.7.0
2025-02-28 13:49:40,548:INFO:            lightgbm: 4.5.0
2025-02-28 13:49:40,549:INFO:               numba: 0.61.0
2025-02-28 13:49:40,549:INFO:            requests: 2.32.3
2025-02-28 13:49:40,549:INFO:          matplotlib: 3.7.5
2025-02-28 13:49:40,549:INFO:          scikitplot: 0.3.7
2025-02-28 13:49:40,549:INFO:         yellowbrick: 1.5
2025-02-28 13:49:40,549:INFO:              plotly: 5.24.1
2025-02-28 13:49:40,549:INFO:    plotly-resampler: Not installed
2025-02-28 13:49:40,549:INFO:             kaleido: 0.2.1
2025-02-28 13:49:40,549:INFO:           schemdraw: 0.15
2025-02-28 13:49:40,549:INFO:         statsmodels: 0.14.4
2025-02-28 13:49:40,549:INFO:              sktime: 0.26.0
2025-02-28 13:49:40,549:INFO:               tbats: 1.1.3
2025-02-28 13:49:40,549:INFO:            pmdarima: 2.0.4
2025-02-28 13:49:40,549:INFO:              psutil: 5.9.0
2025-02-28 13:49:40,550:INFO:          markupsafe: 2.1.5
2025-02-28 13:49:40,550:INFO:             pickle5: Not installed
2025-02-28 13:49:40,550:INFO:         cloudpickle: 3.1.1
2025-02-28 13:49:40,550:INFO:         deprecation: 2.1.0
2025-02-28 13:49:40,550:INFO:              xxhash: 3.5.0
2025-02-28 13:49:40,550:INFO:           wurlitzer: Not installed
2025-02-28 13:49:40,550:INFO:PyCaret optional dependencies:
2025-02-28 13:49:40,550:INFO:                shap: 0.44.1
2025-02-28 13:49:40,550:INFO:           interpret: 0.6.9
2025-02-28 13:49:40,550:INFO:                umap: 0.5.7
2025-02-28 13:49:40,550:INFO:     ydata_profiling: 4.12.2
2025-02-28 13:49:40,550:INFO:  explainerdashboard: 0.4.8
2025-02-28 13:49:40,550:INFO:             autoviz: Not installed
2025-02-28 13:49:40,550:INFO:           fairlearn: 0.7.0
2025-02-28 13:49:40,550:INFO:          deepchecks: Not installed
2025-02-28 13:49:40,550:INFO:             xgboost: 2.1.4
2025-02-28 13:49:40,550:INFO:            catboost: 1.2.7
2025-02-28 13:49:40,550:INFO:              kmodes: 0.12.2
2025-02-28 13:49:40,550:INFO:             mlxtend: 0.23.4
2025-02-28 13:49:40,550:INFO:       statsforecast: 1.5.0
2025-02-28 13:49:40,550:INFO:        tune_sklearn: Not installed
2025-02-28 13:49:40,550:INFO:                 ray: Not installed
2025-02-28 13:49:40,550:INFO:            hyperopt: 0.2.7
2025-02-28 13:49:40,550:INFO:              optuna: 4.2.0
2025-02-28 13:49:40,550:INFO:               skopt: 0.10.2
2025-02-28 13:49:40,550:INFO:              mlflow: 2.20.1
2025-02-28 13:49:40,550:INFO:              gradio: 5.15.0
2025-02-28 13:49:40,550:INFO:             fastapi: 0.115.8
2025-02-28 13:49:40,550:INFO:             uvicorn: 0.34.0
2025-02-28 13:49:40,550:INFO:              m2cgen: 0.10.0
2025-02-28 13:49:40,550:INFO:           evidently: 0.4.40
2025-02-28 13:49:40,550:INFO:               fugue: 0.8.7
2025-02-28 13:49:40,550:INFO:           streamlit: Not installed
2025-02-28 13:49:40,550:INFO:             prophet: Not installed
2025-02-28 13:49:40,550:INFO:None
2025-02-28 13:49:40,550:INFO:Set up data.
2025-02-28 13:49:40,555:INFO:Set up folding strategy.
2025-02-28 13:49:40,555:INFO:Set up train/test split.
2025-02-28 13:49:40,557:INFO:Set up index.
2025-02-28 13:49:40,557:INFO:Assigning column types.
2025-02-28 13:49:40,559:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-02-28 13:49:40,559:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,561:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,563:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,590:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,609:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,609:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:40,610:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:40,611:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,613:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,615:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,640:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,660:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,660:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:40,662:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:40,662:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2025-02-28 13:49:40,664:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,667:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,692:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,712:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,712:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:40,713:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:40,716:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,718:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,744:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,764:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,764:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:40,765:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:40,766:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2025-02-28 13:49:40,770:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,795:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,815:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,815:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:40,817:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:40,822:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,847:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,868:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,868:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:40,869:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:40,870:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2025-02-28 13:49:40,899:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,919:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,919:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:40,920:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:40,951:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,971:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-02-28 13:49:40,971:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:40,972:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:40,972:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-02-28 13:49:41,003:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:41,023:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:41,024:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:41,055:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2025-02-28 13:49:41,074:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:41,075:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:41,076:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2025-02-28 13:49:41,128:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:41,129:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:41,180:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:41,182:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:41,183:INFO:Preparing preprocessing pipeline...
2025-02-28 13:49:41,183:INFO:Set up simple imputation.
2025-02-28 13:49:41,185:INFO:Set up encoding of categorical features.
2025-02-28 13:49:41,213:INFO:Finished creating preprocessing pipeline.
2025-02-28 13:49:41,216:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\dagir\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'High_School_GPA',
                                             'SAT_Score', 'University_Ranking',
                                             'University_GPA',
                                             'Internships_Completed',
                                             'Projects_Completed',
                                             'Certifications',
                                             'Soft_Skills_Score',
                                             'Networking_Score'],
                                    transformer=SimpleImputer(a...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('onehot_encoding',
                 TransformerWrapper(exclude=None,
                                    include=['Gender', 'Field_of_Study'],
                                    transformer=OneHotEncoder(cols=['Gender',
                                                                    'Field_of_Study'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0)))],
         verbose=False)
2025-02-28 13:49:41,216:INFO:Creating final display dataframe.
2025-02-28 13:49:41,313:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target   Starting_Salary
2                   Target type        Regression
3           Original data shape        (5000, 13)
4        Transformed data shape        (5000, 21)
5   Transformed train set shape        (3500, 21)
6    Transformed test set shape        (1500, 21)
7              Numeric features                10
8          Categorical features                 2
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              72e4
2025-02-28 13:49:41,366:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:41,367:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:41,417:INFO:Soft dependency imported: xgboost: 2.1.4
2025-02-28 13:49:41,418:INFO:Soft dependency imported: catboost: 1.2.7
2025-02-28 13:49:41,419:INFO:setup() successfully completed in 0.87s...............
2025-02-28 13:49:41,432:INFO:Initializing compare_models()
2025-02-28 13:49:41,432:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2025-02-28 13:49:41,432:INFO:Checking exceptions
2025-02-28 13:49:41,435:INFO:Preparing display monitor
2025-02-28 13:49:41,453:INFO:Initializing Linear Regression
2025-02-28 13:49:41,454:INFO:Total runtime is 1.6673405965169272e-05 minutes
2025-02-28 13:49:41,456:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:41,456:INFO:Initializing create_model()
2025-02-28 13:49:41,456:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:41,456:INFO:Checking exceptions
2025-02-28 13:49:41,456:INFO:Importing libraries
2025-02-28 13:49:41,456:INFO:Copying training dataset
2025-02-28 13:49:41,459:INFO:Defining folds
2025-02-28 13:49:41,459:INFO:Declaring metric variables
2025-02-28 13:49:41,461:INFO:Importing untrained model
2025-02-28 13:49:41,462:INFO:Linear Regression Imported successfully
2025-02-28 13:49:41,467:INFO:Starting cross validation
2025-02-28 13:49:41,468:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:41,551:INFO:Calculating mean and std
2025-02-28 13:49:41,551:INFO:Creating metrics dataframe
2025-02-28 13:49:41,552:INFO:Uploading results into container
2025-02-28 13:49:41,552:INFO:Uploading model into container now
2025-02-28 13:49:41,552:INFO:_master_model_container: 1
2025-02-28 13:49:41,552:INFO:_display_container: 2
2025-02-28 13:49:41,553:INFO:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, positive=False)
2025-02-28 13:49:41,553:INFO:create_model() successfully completed......................................
2025-02-28 13:49:41,632:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:41,632:INFO:Creating metrics dataframe
2025-02-28 13:49:41,635:INFO:Initializing Lasso Regression
2025-02-28 13:49:41,635:INFO:Total runtime is 0.0030422767003377275 minutes
2025-02-28 13:49:41,637:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:41,637:INFO:Initializing create_model()
2025-02-28 13:49:41,637:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:41,637:INFO:Checking exceptions
2025-02-28 13:49:41,637:INFO:Importing libraries
2025-02-28 13:49:41,637:INFO:Copying training dataset
2025-02-28 13:49:41,640:INFO:Defining folds
2025-02-28 13:49:41,640:INFO:Declaring metric variables
2025-02-28 13:49:41,642:INFO:Importing untrained model
2025-02-28 13:49:41,644:INFO:Lasso Regression Imported successfully
2025-02-28 13:49:41,648:INFO:Starting cross validation
2025-02-28 13:49:41,649:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:41,738:INFO:Calculating mean and std
2025-02-28 13:49:41,738:INFO:Creating metrics dataframe
2025-02-28 13:49:41,739:INFO:Uploading results into container
2025-02-28 13:49:41,739:INFO:Uploading model into container now
2025-02-28 13:49:41,740:INFO:_master_model_container: 2
2025-02-28 13:49:41,740:INFO:_display_container: 2
2025-02-28 13:49:41,740:INFO:Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, positive=False,
      precompute=False, random_state=123, selection='cyclic', tol=0.0001,
      warm_start=False)
2025-02-28 13:49:41,740:INFO:create_model() successfully completed......................................
2025-02-28 13:49:41,819:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:41,819:INFO:Creating metrics dataframe
2025-02-28 13:49:41,823:INFO:Initializing Ridge Regression
2025-02-28 13:49:41,823:INFO:Total runtime is 0.006160656611124674 minutes
2025-02-28 13:49:41,826:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:41,826:INFO:Initializing create_model()
2025-02-28 13:49:41,826:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:41,826:INFO:Checking exceptions
2025-02-28 13:49:41,826:INFO:Importing libraries
2025-02-28 13:49:41,826:INFO:Copying training dataset
2025-02-28 13:49:41,828:INFO:Defining folds
2025-02-28 13:49:41,828:INFO:Declaring metric variables
2025-02-28 13:49:41,830:INFO:Importing untrained model
2025-02-28 13:49:41,832:INFO:Ridge Regression Imported successfully
2025-02-28 13:49:41,837:INFO:Starting cross validation
2025-02-28 13:49:41,838:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:41,910:INFO:Calculating mean and std
2025-02-28 13:49:41,910:INFO:Creating metrics dataframe
2025-02-28 13:49:41,911:INFO:Uploading results into container
2025-02-28 13:49:41,911:INFO:Uploading model into container now
2025-02-28 13:49:41,912:INFO:_master_model_container: 3
2025-02-28 13:49:41,912:INFO:_display_container: 2
2025-02-28 13:49:41,912:INFO:Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, positive=False,
      random_state=123, solver='auto', tol=0.0001)
2025-02-28 13:49:41,912:INFO:create_model() successfully completed......................................
2025-02-28 13:49:41,991:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:41,992:INFO:Creating metrics dataframe
2025-02-28 13:49:41,996:INFO:Initializing Elastic Net
2025-02-28 13:49:41,996:INFO:Total runtime is 0.009044301509857177 minutes
2025-02-28 13:49:41,998:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:41,998:INFO:Initializing create_model()
2025-02-28 13:49:41,998:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:41,998:INFO:Checking exceptions
2025-02-28 13:49:41,998:INFO:Importing libraries
2025-02-28 13:49:41,998:INFO:Copying training dataset
2025-02-28 13:49:42,001:INFO:Defining folds
2025-02-28 13:49:42,001:INFO:Declaring metric variables
2025-02-28 13:49:42,003:INFO:Importing untrained model
2025-02-28 13:49:42,005:INFO:Elastic Net Imported successfully
2025-02-28 13:49:42,009:INFO:Starting cross validation
2025-02-28 13:49:42,010:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:42,083:INFO:Calculating mean and std
2025-02-28 13:49:42,083:INFO:Creating metrics dataframe
2025-02-28 13:49:42,084:INFO:Uploading results into container
2025-02-28 13:49:42,084:INFO:Uploading model into container now
2025-02-28 13:49:42,084:INFO:_master_model_container: 4
2025-02-28 13:49:42,085:INFO:_display_container: 2
2025-02-28 13:49:42,085:INFO:ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
           max_iter=1000, positive=False, precompute=False, random_state=123,
           selection='cyclic', tol=0.0001, warm_start=False)
2025-02-28 13:49:42,085:INFO:create_model() successfully completed......................................
2025-02-28 13:49:42,169:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:42,169:INFO:Creating metrics dataframe
2025-02-28 13:49:42,172:INFO:Initializing Least Angle Regression
2025-02-28 13:49:42,173:INFO:Total runtime is 0.01199985345204671 minutes
2025-02-28 13:49:42,175:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:42,175:INFO:Initializing create_model()
2025-02-28 13:49:42,175:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:42,175:INFO:Checking exceptions
2025-02-28 13:49:42,175:INFO:Importing libraries
2025-02-28 13:49:42,175:INFO:Copying training dataset
2025-02-28 13:49:42,178:INFO:Defining folds
2025-02-28 13:49:42,178:INFO:Declaring metric variables
2025-02-28 13:49:42,180:INFO:Importing untrained model
2025-02-28 13:49:42,181:INFO:Least Angle Regression Imported successfully
2025-02-28 13:49:42,185:INFO:Starting cross validation
2025-02-28 13:49:42,186:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:42,257:INFO:Calculating mean and std
2025-02-28 13:49:42,257:INFO:Creating metrics dataframe
2025-02-28 13:49:42,258:INFO:Uploading results into container
2025-02-28 13:49:42,258:INFO:Uploading model into container now
2025-02-28 13:49:42,259:INFO:_master_model_container: 5
2025-02-28 13:49:42,259:INFO:_display_container: 2
2025-02-28 13:49:42,259:INFO:Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,
     jitter=None, n_nonzero_coefs=500, precompute='auto', random_state=123,
     verbose=False)
2025-02-28 13:49:42,259:INFO:create_model() successfully completed......................................
2025-02-28 13:49:42,337:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:42,337:INFO:Creating metrics dataframe
2025-02-28 13:49:42,341:INFO:Initializing Lasso Least Angle Regression
2025-02-28 13:49:42,341:INFO:Total runtime is 0.014793554941813149 minutes
2025-02-28 13:49:42,343:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:42,343:INFO:Initializing create_model()
2025-02-28 13:49:42,343:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:42,343:INFO:Checking exceptions
2025-02-28 13:49:42,343:INFO:Importing libraries
2025-02-28 13:49:42,343:INFO:Copying training dataset
2025-02-28 13:49:42,346:INFO:Defining folds
2025-02-28 13:49:42,346:INFO:Declaring metric variables
2025-02-28 13:49:42,347:INFO:Importing untrained model
2025-02-28 13:49:42,350:INFO:Lasso Least Angle Regression Imported successfully
2025-02-28 13:49:42,355:INFO:Starting cross validation
2025-02-28 13:49:42,356:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:42,428:INFO:Calculating mean and std
2025-02-28 13:49:42,428:INFO:Creating metrics dataframe
2025-02-28 13:49:42,429:INFO:Uploading results into container
2025-02-28 13:49:42,429:INFO:Uploading model into container now
2025-02-28 13:49:42,430:INFO:_master_model_container: 6
2025-02-28 13:49:42,430:INFO:_display_container: 2
2025-02-28 13:49:42,430:INFO:LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,
          fit_path=True, jitter=None, max_iter=500, positive=False,
          precompute='auto', random_state=123, verbose=False)
2025-02-28 13:49:42,430:INFO:create_model() successfully completed......................................
2025-02-28 13:49:42,508:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:42,508:INFO:Creating metrics dataframe
2025-02-28 13:49:42,512:INFO:Initializing Orthogonal Matching Pursuit
2025-02-28 13:49:42,512:INFO:Total runtime is 0.01764927307764689 minutes
2025-02-28 13:49:42,514:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:42,514:INFO:Initializing create_model()
2025-02-28 13:49:42,514:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:42,515:INFO:Checking exceptions
2025-02-28 13:49:42,515:INFO:Importing libraries
2025-02-28 13:49:42,515:INFO:Copying training dataset
2025-02-28 13:49:42,517:INFO:Defining folds
2025-02-28 13:49:42,517:INFO:Declaring metric variables
2025-02-28 13:49:42,519:INFO:Importing untrained model
2025-02-28 13:49:42,521:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 13:49:42,525:INFO:Starting cross validation
2025-02-28 13:49:42,526:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:42,601:INFO:Calculating mean and std
2025-02-28 13:49:42,601:INFO:Creating metrics dataframe
2025-02-28 13:49:42,601:INFO:Uploading results into container
2025-02-28 13:49:42,602:INFO:Uploading model into container now
2025-02-28 13:49:42,602:INFO:_master_model_container: 7
2025-02-28 13:49:42,602:INFO:_display_container: 2
2025-02-28 13:49:42,602:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 13:49:42,602:INFO:create_model() successfully completed......................................
2025-02-28 13:49:42,685:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:42,686:INFO:Creating metrics dataframe
2025-02-28 13:49:42,690:INFO:Initializing Bayesian Ridge
2025-02-28 13:49:42,690:INFO:Total runtime is 0.020617338021596272 minutes
2025-02-28 13:49:42,692:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:42,692:INFO:Initializing create_model()
2025-02-28 13:49:42,692:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:42,692:INFO:Checking exceptions
2025-02-28 13:49:42,692:INFO:Importing libraries
2025-02-28 13:49:42,692:INFO:Copying training dataset
2025-02-28 13:49:42,695:INFO:Defining folds
2025-02-28 13:49:42,695:INFO:Declaring metric variables
2025-02-28 13:49:42,697:INFO:Importing untrained model
2025-02-28 13:49:42,700:INFO:Bayesian Ridge Imported successfully
2025-02-28 13:49:42,705:INFO:Starting cross validation
2025-02-28 13:49:42,706:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:42,790:INFO:Calculating mean and std
2025-02-28 13:49:42,790:INFO:Creating metrics dataframe
2025-02-28 13:49:42,791:INFO:Uploading results into container
2025-02-28 13:49:42,791:INFO:Uploading model into container now
2025-02-28 13:49:42,792:INFO:_master_model_container: 8
2025-02-28 13:49:42,792:INFO:_display_container: 2
2025-02-28 13:49:42,792:INFO:BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,
              compute_score=False, copy_X=True, fit_intercept=True,
              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, max_iter=None,
              n_iter='deprecated', tol=0.001, verbose=False)
2025-02-28 13:49:42,792:INFO:create_model() successfully completed......................................
2025-02-28 13:49:42,867:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:42,867:INFO:Creating metrics dataframe
2025-02-28 13:49:42,872:INFO:Initializing Passive Aggressive Regressor
2025-02-28 13:49:42,872:INFO:Total runtime is 0.023647924264272053 minutes
2025-02-28 13:49:42,874:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:42,875:INFO:Initializing create_model()
2025-02-28 13:49:42,875:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:42,875:INFO:Checking exceptions
2025-02-28 13:49:42,875:INFO:Importing libraries
2025-02-28 13:49:42,875:INFO:Copying training dataset
2025-02-28 13:49:42,877:INFO:Defining folds
2025-02-28 13:49:42,878:INFO:Declaring metric variables
2025-02-28 13:49:42,880:INFO:Importing untrained model
2025-02-28 13:49:42,882:INFO:Passive Aggressive Regressor Imported successfully
2025-02-28 13:49:42,886:INFO:Starting cross validation
2025-02-28 13:49:42,887:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:42,965:INFO:Calculating mean and std
2025-02-28 13:49:42,965:INFO:Creating metrics dataframe
2025-02-28 13:49:42,966:INFO:Uploading results into container
2025-02-28 13:49:42,966:INFO:Uploading model into container now
2025-02-28 13:49:42,966:INFO:_master_model_container: 9
2025-02-28 13:49:42,967:INFO:_display_container: 2
2025-02-28 13:49:42,967:INFO:PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False,
                           epsilon=0.1, fit_intercept=True,
                           loss='epsilon_insensitive', max_iter=1000,
                           n_iter_no_change=5, random_state=123, shuffle=True,
                           tol=0.001, validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-02-28 13:49:42,967:INFO:create_model() successfully completed......................................
2025-02-28 13:49:43,046:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:43,046:INFO:Creating metrics dataframe
2025-02-28 13:49:43,051:INFO:Initializing Huber Regressor
2025-02-28 13:49:43,051:INFO:Total runtime is 0.026635801792144774 minutes
2025-02-28 13:49:43,054:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:43,054:INFO:Initializing create_model()
2025-02-28 13:49:43,054:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:43,054:INFO:Checking exceptions
2025-02-28 13:49:43,054:INFO:Importing libraries
2025-02-28 13:49:43,054:INFO:Copying training dataset
2025-02-28 13:49:43,057:INFO:Defining folds
2025-02-28 13:49:43,057:INFO:Declaring metric variables
2025-02-28 13:49:43,059:INFO:Importing untrained model
2025-02-28 13:49:43,061:INFO:Huber Regressor Imported successfully
2025-02-28 13:49:43,065:INFO:Starting cross validation
2025-02-28 13:49:43,066:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:43,119:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,121:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,129:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,132:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,133:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,133:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,137:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,139:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,150:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,155:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2025-02-28 13:49:43,170:INFO:Calculating mean and std
2025-02-28 13:49:43,170:INFO:Creating metrics dataframe
2025-02-28 13:49:43,172:INFO:Uploading results into container
2025-02-28 13:49:43,172:INFO:Uploading model into container now
2025-02-28 13:49:43,173:INFO:_master_model_container: 10
2025-02-28 13:49:43,173:INFO:_display_container: 2
2025-02-28 13:49:43,173:INFO:HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,
               tol=1e-05, warm_start=False)
2025-02-28 13:49:43,173:INFO:create_model() successfully completed......................................
2025-02-28 13:49:43,250:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:43,250:INFO:Creating metrics dataframe
2025-02-28 13:49:43,255:INFO:Initializing K Neighbors Regressor
2025-02-28 13:49:43,255:INFO:Total runtime is 0.030038229624430337 minutes
2025-02-28 13:49:43,257:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:43,257:INFO:Initializing create_model()
2025-02-28 13:49:43,257:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:43,257:INFO:Checking exceptions
2025-02-28 13:49:43,258:INFO:Importing libraries
2025-02-28 13:49:43,258:INFO:Copying training dataset
2025-02-28 13:49:43,260:INFO:Defining folds
2025-02-28 13:49:43,260:INFO:Declaring metric variables
2025-02-28 13:49:43,263:INFO:Importing untrained model
2025-02-28 13:49:43,265:INFO:K Neighbors Regressor Imported successfully
2025-02-28 13:49:43,269:INFO:Starting cross validation
2025-02-28 13:49:43,270:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:43,356:INFO:Calculating mean and std
2025-02-28 13:49:43,357:INFO:Creating metrics dataframe
2025-02-28 13:49:43,358:INFO:Uploading results into container
2025-02-28 13:49:43,358:INFO:Uploading model into container now
2025-02-28 13:49:43,358:INFO:_master_model_container: 11
2025-02-28 13:49:43,358:INFO:_display_container: 2
2025-02-28 13:49:43,359:INFO:KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
                    metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                    weights='uniform')
2025-02-28 13:49:43,359:INFO:create_model() successfully completed......................................
2025-02-28 13:49:43,436:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:43,436:INFO:Creating metrics dataframe
2025-02-28 13:49:43,441:INFO:Initializing Decision Tree Regressor
2025-02-28 13:49:43,441:INFO:Total runtime is 0.03312759002049764 minutes
2025-02-28 13:49:43,443:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:43,443:INFO:Initializing create_model()
2025-02-28 13:49:43,443:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:43,443:INFO:Checking exceptions
2025-02-28 13:49:43,443:INFO:Importing libraries
2025-02-28 13:49:43,443:INFO:Copying training dataset
2025-02-28 13:49:43,446:INFO:Defining folds
2025-02-28 13:49:43,446:INFO:Declaring metric variables
2025-02-28 13:49:43,447:INFO:Importing untrained model
2025-02-28 13:49:43,451:INFO:Decision Tree Regressor Imported successfully
2025-02-28 13:49:43,455:INFO:Starting cross validation
2025-02-28 13:49:43,456:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:43,557:INFO:Calculating mean and std
2025-02-28 13:49:43,558:INFO:Creating metrics dataframe
2025-02-28 13:49:43,559:INFO:Uploading results into container
2025-02-28 13:49:43,559:INFO:Uploading model into container now
2025-02-28 13:49:43,559:INFO:_master_model_container: 12
2025-02-28 13:49:43,559:INFO:_display_container: 2
2025-02-28 13:49:43,560:INFO:DecisionTreeRegressor(ccp_alpha=0.0, criterion='squared_error', max_depth=None,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      monotonic_cst=None, random_state=123, splitter='best')
2025-02-28 13:49:43,560:INFO:create_model() successfully completed......................................
2025-02-28 13:49:43,638:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:43,638:INFO:Creating metrics dataframe
2025-02-28 13:49:43,642:INFO:Initializing Random Forest Regressor
2025-02-28 13:49:43,642:INFO:Total runtime is 0.03648730119069417 minutes
2025-02-28 13:49:43,645:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:43,645:INFO:Initializing create_model()
2025-02-28 13:49:43,645:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:43,645:INFO:Checking exceptions
2025-02-28 13:49:43,645:INFO:Importing libraries
2025-02-28 13:49:43,645:INFO:Copying training dataset
2025-02-28 13:49:43,647:INFO:Defining folds
2025-02-28 13:49:43,647:INFO:Declaring metric variables
2025-02-28 13:49:43,650:INFO:Importing untrained model
2025-02-28 13:49:43,652:INFO:Random Forest Regressor Imported successfully
2025-02-28 13:49:43,656:INFO:Starting cross validation
2025-02-28 13:49:43,656:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:44,493:INFO:Calculating mean and std
2025-02-28 13:49:44,493:INFO:Creating metrics dataframe
2025-02-28 13:49:44,495:INFO:Uploading results into container
2025-02-28 13:49:44,495:INFO:Uploading model into container now
2025-02-28 13:49:44,495:INFO:_master_model_container: 13
2025-02-28 13:49:44,495:INFO:_display_container: 2
2025-02-28 13:49:44,495:INFO:RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='squared_error',
                      max_depth=None, max_features=1.0, max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, monotonic_cst=None,
                      n_estimators=100, n_jobs=-1, oob_score=False,
                      random_state=123, verbose=0, warm_start=False)
2025-02-28 13:49:44,496:INFO:create_model() successfully completed......................................
2025-02-28 13:49:44,573:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:44,573:INFO:Creating metrics dataframe
2025-02-28 13:49:44,578:INFO:Initializing Extra Trees Regressor
2025-02-28 13:49:44,578:INFO:Total runtime is 0.0520838975906372 minutes
2025-02-28 13:49:44,580:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:44,580:INFO:Initializing create_model()
2025-02-28 13:49:44,580:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:44,580:INFO:Checking exceptions
2025-02-28 13:49:44,580:INFO:Importing libraries
2025-02-28 13:49:44,580:INFO:Copying training dataset
2025-02-28 13:49:44,582:INFO:Defining folds
2025-02-28 13:49:44,582:INFO:Declaring metric variables
2025-02-28 13:49:44,584:INFO:Importing untrained model
2025-02-28 13:49:44,586:INFO:Extra Trees Regressor Imported successfully
2025-02-28 13:49:44,590:INFO:Starting cross validation
2025-02-28 13:49:44,592:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:45,134:INFO:Calculating mean and std
2025-02-28 13:49:45,135:INFO:Creating metrics dataframe
2025-02-28 13:49:45,136:INFO:Uploading results into container
2025-02-28 13:49:45,136:INFO:Uploading model into container now
2025-02-28 13:49:45,136:INFO:_master_model_container: 14
2025-02-28 13:49:45,136:INFO:_display_container: 2
2025-02-28 13:49:45,137:INFO:ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',
                    max_depth=None, max_features=1.0, max_leaf_nodes=None,
                    max_samples=None, min_impurity_decrease=0.0,
                    min_samples_leaf=1, min_samples_split=2,
                    min_weight_fraction_leaf=0.0, monotonic_cst=None,
                    n_estimators=100, n_jobs=-1, oob_score=False,
                    random_state=123, verbose=0, warm_start=False)
2025-02-28 13:49:45,137:INFO:create_model() successfully completed......................................
2025-02-28 13:49:45,218:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:45,218:INFO:Creating metrics dataframe
2025-02-28 13:49:45,224:INFO:Initializing AdaBoost Regressor
2025-02-28 13:49:45,225:INFO:Total runtime is 0.06286602020263671 minutes
2025-02-28 13:49:45,227:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:45,227:INFO:Initializing create_model()
2025-02-28 13:49:45,227:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:45,228:INFO:Checking exceptions
2025-02-28 13:49:45,228:INFO:Importing libraries
2025-02-28 13:49:45,228:INFO:Copying training dataset
2025-02-28 13:49:45,230:INFO:Defining folds
2025-02-28 13:49:45,230:INFO:Declaring metric variables
2025-02-28 13:49:45,233:INFO:Importing untrained model
2025-02-28 13:49:45,235:INFO:AdaBoost Regressor Imported successfully
2025-02-28 13:49:45,240:INFO:Starting cross validation
2025-02-28 13:49:45,240:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:45,413:INFO:Calculating mean and std
2025-02-28 13:49:45,413:INFO:Creating metrics dataframe
2025-02-28 13:49:45,414:INFO:Uploading results into container
2025-02-28 13:49:45,415:INFO:Uploading model into container now
2025-02-28 13:49:45,415:INFO:_master_model_container: 15
2025-02-28 13:49:45,415:INFO:_display_container: 2
2025-02-28 13:49:45,415:INFO:AdaBoostRegressor(estimator=None, learning_rate=1.0, loss='linear',
                  n_estimators=50, random_state=123)
2025-02-28 13:49:45,415:INFO:create_model() successfully completed......................................
2025-02-28 13:49:45,497:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:45,497:INFO:Creating metrics dataframe
2025-02-28 13:49:45,503:INFO:Initializing Gradient Boosting Regressor
2025-02-28 13:49:45,503:INFO:Total runtime is 0.06750136216481525 minutes
2025-02-28 13:49:45,505:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:45,505:INFO:Initializing create_model()
2025-02-28 13:49:45,505:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:45,505:INFO:Checking exceptions
2025-02-28 13:49:45,505:INFO:Importing libraries
2025-02-28 13:49:45,505:INFO:Copying training dataset
2025-02-28 13:49:45,508:INFO:Defining folds
2025-02-28 13:49:45,508:INFO:Declaring metric variables
2025-02-28 13:49:45,510:INFO:Importing untrained model
2025-02-28 13:49:45,512:INFO:Gradient Boosting Regressor Imported successfully
2025-02-28 13:49:45,516:INFO:Starting cross validation
2025-02-28 13:49:45,517:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:45,910:INFO:Calculating mean and std
2025-02-28 13:49:45,910:INFO:Creating metrics dataframe
2025-02-28 13:49:45,911:INFO:Uploading results into container
2025-02-28 13:49:45,911:INFO:Uploading model into container now
2025-02-28 13:49:45,912:INFO:_master_model_container: 16
2025-02-28 13:49:45,912:INFO:_display_container: 2
2025-02-28 13:49:45,912:INFO:GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',
                          init=None, learning_rate=0.1, loss='squared_error',
                          max_depth=3, max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          n_estimators=100, n_iter_no_change=None,
                          random_state=123, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
2025-02-28 13:49:45,912:INFO:create_model() successfully completed......................................
2025-02-28 13:49:45,990:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:45,991:INFO:Creating metrics dataframe
2025-02-28 13:49:45,996:INFO:Initializing Extreme Gradient Boosting
2025-02-28 13:49:45,997:INFO:Total runtime is 0.07572736740112304 minutes
2025-02-28 13:49:45,999:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:45,999:INFO:Initializing create_model()
2025-02-28 13:49:45,999:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:45,999:INFO:Checking exceptions
2025-02-28 13:49:46,000:INFO:Importing libraries
2025-02-28 13:49:46,000:INFO:Copying training dataset
2025-02-28 13:49:46,002:INFO:Defining folds
2025-02-28 13:49:46,002:INFO:Declaring metric variables
2025-02-28 13:49:46,003:INFO:Importing untrained model
2025-02-28 13:49:46,006:INFO:Extreme Gradient Boosting Imported successfully
2025-02-28 13:49:46,010:INFO:Starting cross validation
2025-02-28 13:49:46,010:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:46,547:INFO:Calculating mean and std
2025-02-28 13:49:46,548:INFO:Creating metrics dataframe
2025-02-28 13:49:46,549:INFO:Uploading results into container
2025-02-28 13:49:46,549:INFO:Uploading model into container now
2025-02-28 13:49:46,550:INFO:_master_model_container: 17
2025-02-28 13:49:46,550:INFO:_display_container: 2
2025-02-28 13:49:46,550:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device='cpu', early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, objective='reg:squarederror', ...)
2025-02-28 13:49:46,550:INFO:create_model() successfully completed......................................
2025-02-28 13:49:46,630:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:46,630:INFO:Creating metrics dataframe
2025-02-28 13:49:46,635:INFO:Initializing Light Gradient Boosting Machine
2025-02-28 13:49:46,636:INFO:Total runtime is 0.08639131387074787 minutes
2025-02-28 13:49:46,637:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:46,637:INFO:Initializing create_model()
2025-02-28 13:49:46,637:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:46,637:INFO:Checking exceptions
2025-02-28 13:49:46,637:INFO:Importing libraries
2025-02-28 13:49:46,637:INFO:Copying training dataset
2025-02-28 13:49:46,641:INFO:Defining folds
2025-02-28 13:49:46,641:INFO:Declaring metric variables
2025-02-28 13:49:46,642:INFO:Importing untrained model
2025-02-28 13:49:46,644:INFO:Light Gradient Boosting Machine Imported successfully
2025-02-28 13:49:46,648:INFO:Starting cross validation
2025-02-28 13:49:46,649:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:47,936:INFO:Calculating mean and std
2025-02-28 13:49:47,937:INFO:Creating metrics dataframe
2025-02-28 13:49:47,939:INFO:Uploading results into container
2025-02-28 13:49:47,939:INFO:Uploading model into container now
2025-02-28 13:49:47,939:INFO:_master_model_container: 18
2025-02-28 13:49:47,939:INFO:_display_container: 2
2025-02-28 13:49:47,939:INFO:LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
              importance_type='split', learning_rate=0.1, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
              random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)
2025-02-28 13:49:47,939:INFO:create_model() successfully completed......................................
2025-02-28 13:49:48,032:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:48,032:INFO:Creating metrics dataframe
2025-02-28 13:49:48,038:INFO:Initializing CatBoost Regressor
2025-02-28 13:49:48,038:INFO:Total runtime is 0.10975253184636433 minutes
2025-02-28 13:49:48,041:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:48,041:INFO:Initializing create_model()
2025-02-28 13:49:48,041:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:48,041:INFO:Checking exceptions
2025-02-28 13:49:48,041:INFO:Importing libraries
2025-02-28 13:49:48,041:INFO:Copying training dataset
2025-02-28 13:49:48,043:INFO:Defining folds
2025-02-28 13:49:48,043:INFO:Declaring metric variables
2025-02-28 13:49:48,046:INFO:Importing untrained model
2025-02-28 13:49:48,048:INFO:CatBoost Regressor Imported successfully
2025-02-28 13:49:48,052:INFO:Starting cross validation
2025-02-28 13:49:48,053:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:50,668:INFO:Calculating mean and std
2025-02-28 13:49:50,668:INFO:Creating metrics dataframe
2025-02-28 13:49:50,670:INFO:Uploading results into container
2025-02-28 13:49:50,670:INFO:Uploading model into container now
2025-02-28 13:49:50,670:INFO:_master_model_container: 19
2025-02-28 13:49:50,670:INFO:_display_container: 2
2025-02-28 13:49:50,670:INFO:<catboost.core.CatBoostRegressor object at 0x0000028C1445B820>
2025-02-28 13:49:50,670:INFO:create_model() successfully completed......................................
2025-02-28 13:49:50,749:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:50,749:INFO:Creating metrics dataframe
2025-02-28 13:49:50,755:INFO:Initializing Dummy Regressor
2025-02-28 13:49:50,756:INFO:Total runtime is 0.15504689613978068 minutes
2025-02-28 13:49:50,758:INFO:SubProcess create_model() called ==================================
2025-02-28 13:49:50,758:INFO:Initializing create_model()
2025-02-28 13:49:50,758:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C0AF73A30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:50,758:INFO:Checking exceptions
2025-02-28 13:49:50,758:INFO:Importing libraries
2025-02-28 13:49:50,758:INFO:Copying training dataset
2025-02-28 13:49:50,760:INFO:Defining folds
2025-02-28 13:49:50,760:INFO:Declaring metric variables
2025-02-28 13:49:50,762:INFO:Importing untrained model
2025-02-28 13:49:50,763:INFO:Dummy Regressor Imported successfully
2025-02-28 13:49:50,767:INFO:Starting cross validation
2025-02-28 13:49:50,769:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:50,855:INFO:Calculating mean and std
2025-02-28 13:49:50,855:INFO:Creating metrics dataframe
2025-02-28 13:49:50,857:INFO:Uploading results into container
2025-02-28 13:49:50,857:INFO:Uploading model into container now
2025-02-28 13:49:50,857:INFO:_master_model_container: 20
2025-02-28 13:49:50,857:INFO:_display_container: 2
2025-02-28 13:49:50,857:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 13:49:50,857:INFO:create_model() successfully completed......................................
2025-02-28 13:49:50,937:INFO:SubProcess create_model() end ==================================
2025-02-28 13:49:50,937:INFO:Creating metrics dataframe
2025-02-28 13:49:50,942:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-02-28 13:49:50,947:INFO:Initializing create_model()
2025-02-28 13:49:50,948:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=DummyRegressor(constant=None, quantile=None, strategy='mean'), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:50,948:INFO:Checking exceptions
2025-02-28 13:49:50,949:INFO:Importing libraries
2025-02-28 13:49:50,949:INFO:Copying training dataset
2025-02-28 13:49:50,952:INFO:Defining folds
2025-02-28 13:49:50,952:INFO:Declaring metric variables
2025-02-28 13:49:50,952:INFO:Importing untrained model
2025-02-28 13:49:50,952:INFO:Declaring custom model
2025-02-28 13:49:50,952:INFO:Dummy Regressor Imported successfully
2025-02-28 13:49:50,953:INFO:Cross validation set to False
2025-02-28 13:49:50,953:INFO:Fitting Model
2025-02-28 13:49:50,975:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 13:49:50,976:INFO:create_model() successfully completed......................................
2025-02-28 13:49:51,073:INFO:_master_model_container: 20
2025-02-28 13:49:51,073:INFO:_display_container: 2
2025-02-28 13:49:51,073:INFO:DummyRegressor(constant=None, quantile=None, strategy='mean')
2025-02-28 13:49:51,073:INFO:compare_models() successfully completed......................................
2025-02-28 13:49:51,094:INFO:Initializing create_model()
2025-02-28 13:49:51,095:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, estimator=omp, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-02-28 13:49:51,095:INFO:Checking exceptions
2025-02-28 13:49:51,105:INFO:Importing libraries
2025-02-28 13:49:51,105:INFO:Copying training dataset
2025-02-28 13:49:51,108:INFO:Defining folds
2025-02-28 13:49:51,108:INFO:Declaring metric variables
2025-02-28 13:49:51,111:INFO:Importing untrained model
2025-02-28 13:49:51,114:INFO:Orthogonal Matching Pursuit Imported successfully
2025-02-28 13:49:51,120:INFO:Starting cross validation
2025-02-28 13:49:51,121:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-02-28 13:49:51,212:INFO:Calculating mean and std
2025-02-28 13:49:51,212:INFO:Creating metrics dataframe
2025-02-28 13:49:51,215:INFO:Finalizing model
2025-02-28 13:49:51,240:INFO:Uploading results into container
2025-02-28 13:49:51,241:INFO:Uploading model into container now
2025-02-28 13:49:51,245:INFO:_master_model_container: 21
2025-02-28 13:49:51,245:INFO:_display_container: 3
2025-02-28 13:49:51,245:INFO:OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None)
2025-02-28 13:49:51,245:INFO:create_model() successfully completed......................................
2025-02-28 13:49:51,342:INFO:Initializing plot_model()
2025-02-28 13:49:51,342:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, system=True)
2025-02-28 13:49:51,342:INFO:Checking exceptions
2025-02-28 13:49:51,346:INFO:Preloading libraries
2025-02-28 13:49:51,346:INFO:Copying training dataset
2025-02-28 13:49:51,346:INFO:Plot type: residuals
2025-02-28 13:49:51,474:INFO:Fitting Model
2025-02-28 13:49:51,475:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 13:49:51,491:INFO:Scoring test/hold-out set
2025-02-28 13:49:51,786:INFO:Visual Rendered Successfully
2025-02-28 13:49:51,864:INFO:plot_model() successfully completed......................................
2025-02-28 13:49:51,880:INFO:Initializing plot_model()
2025-02-28 13:49:51,880:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, system=True)
2025-02-28 13:49:51,880:INFO:Checking exceptions
2025-02-28 13:49:51,884:INFO:Preloading libraries
2025-02-28 13:49:51,884:INFO:Copying training dataset
2025-02-28 13:49:51,884:INFO:Plot type: error
2025-02-28 13:49:51,980:INFO:Fitting Model
2025-02-28 13:49:51,980:WARNING:c:\Users\dagir\miniconda3\envs\pyca\lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but OrthogonalMatchingPursuit was fitted with feature names
  warnings.warn(

2025-02-28 13:49:51,980:INFO:Scoring test/hold-out set
2025-02-28 13:49:52,092:INFO:Visual Rendered Successfully
2025-02-28 13:49:52,176:INFO:plot_model() successfully completed......................................
2025-02-28 13:49:52,188:INFO:Initializing plot_model()
2025-02-28 13:49:52,188:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=OrthogonalMatchingPursuit(fit_intercept=True, n_nonzero_coefs=None,
                          precompute='auto', tol=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C12A6B010>, system=True)
2025-02-28 13:49:52,188:INFO:Checking exceptions
2025-02-28 13:49:52,190:INFO:Preloading libraries
2025-02-28 13:49:52,190:INFO:Copying training dataset
2025-02-28 13:49:52,190:INFO:Plot type: feature
2025-02-28 13:49:52,312:INFO:Visual Rendered Successfully
2025-02-28 13:49:52,394:INFO:plot_model() successfully completed......................................
2025-02-28 13:49:53,889:WARNING:C:\Users\dagir\AppData\Local\Temp\ipykernel_37416\1216279964.py:849: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  import pkg_resources

2025-03-07 19:29:35,478:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-03-07 19:29:35,479:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-03-07 19:29:35,479:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-03-07 19:29:35,480:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-03-07 19:29:35,581:INFO:PyCaret RegressionExperiment
2025-03-07 19:29:35,581:INFO:Logging name: reg-default-name
2025-03-07 19:29:35,581:INFO:ML Usecase: MLUsecase.REGRESSION
2025-03-07 19:29:35,581:INFO:version 3.3.2
2025-03-07 19:29:35,581:INFO:Initializing setup()
2025-03-07 19:29:35,581:INFO:self.USI: 8235
2025-03-07 19:29:35,581:INFO:self._variable_keys: {'gpu_n_jobs_param', 'X_test', 'pipeline', 'n_jobs_param', 'USI', 'logging_param', 'fold_shuffle_param', 'X_train', 'exp_name_log', 'idx', 'log_plots_param', 'target_param', 'X', 'transform_target_param', 'seed', 'y_train', 'html_param', 'data', 'memory', '_available_plots', '_ml_usecase', 'y', 'fold_generator', 'gpu_param', 'y_test', 'exp_id', 'fold_groups_param'}
2025-03-07 19:29:35,581:INFO:Checking environment
2025-03-07 19:29:35,581:INFO:python_version: 3.10.16
2025-03-07 19:29:35,581:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-03-07 19:29:35,582:INFO:machine: AMD64
2025-03-07 19:29:35,582:INFO:platform: Windows-10-10.0.26100-SP0
2025-03-07 19:29:35,582:INFO:Memory: svmem(total=34200334336, available=17045491712, percent=50.2, used=17154842624, free=17045491712)
2025-03-07 19:29:35,582:INFO:Physical Core: 24
2025-03-07 19:29:35,582:INFO:Logical Core: 32
2025-03-07 19:29:35,582:INFO:Checking libraries
2025-03-07 19:29:35,582:INFO:System:
2025-03-07 19:29:35,582:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-03-07 19:29:35,582:INFO:executable: c:\Users\dagir\miniconda3\envs\pyca\python.exe
2025-03-07 19:29:35,582:INFO:   machine: Windows-10-10.0.26100-SP0
2025-03-07 19:29:35,582:INFO:PyCaret required dependencies:
2025-03-07 19:29:36,241:INFO:                 pip: 25.0
2025-03-07 19:29:36,241:INFO:          setuptools: 75.8.0
2025-03-07 19:29:36,241:INFO:             pycaret: 3.3.2
2025-03-07 19:29:36,241:INFO:             IPython: 8.30.0
2025-03-07 19:29:36,241:INFO:          ipywidgets: 8.1.5
2025-03-07 19:29:36,241:INFO:                tqdm: 4.67.1
2025-03-07 19:29:36,241:INFO:               numpy: 1.26.4
2025-03-07 19:29:36,241:INFO:              pandas: 2.1.4
2025-03-07 19:29:36,241:INFO:              jinja2: 3.1.5
2025-03-07 19:29:36,241:INFO:               scipy: 1.11.4
2025-03-07 19:29:36,241:INFO:              joblib: 1.3.2
2025-03-07 19:29:36,241:INFO:             sklearn: 1.4.2
2025-03-07 19:29:36,241:INFO:                pyod: 2.0.3
2025-03-07 19:29:36,241:INFO:            imblearn: 0.13.0
2025-03-07 19:29:36,241:INFO:   category_encoders: 2.7.0
2025-03-07 19:29:36,241:INFO:            lightgbm: 4.5.0
2025-03-07 19:29:36,241:INFO:               numba: 0.61.0
2025-03-07 19:29:36,241:INFO:            requests: 2.32.3
2025-03-07 19:29:36,241:INFO:          matplotlib: 3.7.5
2025-03-07 19:29:36,241:INFO:          scikitplot: 0.3.7
2025-03-07 19:29:36,241:INFO:         yellowbrick: 1.5
2025-03-07 19:29:36,242:INFO:              plotly: 5.24.1
2025-03-07 19:29:36,242:INFO:    plotly-resampler: Not installed
2025-03-07 19:29:36,242:INFO:             kaleido: 0.2.1
2025-03-07 19:29:36,242:INFO:           schemdraw: 0.15
2025-03-07 19:29:36,242:INFO:         statsmodels: 0.14.4
2025-03-07 19:29:36,242:INFO:              sktime: 0.26.0
2025-03-07 19:29:36,242:INFO:               tbats: 1.1.3
2025-03-07 19:29:36,242:INFO:            pmdarima: 2.0.4
2025-03-07 19:29:36,242:INFO:              psutil: 5.9.0
2025-03-07 19:29:36,242:INFO:          markupsafe: 2.1.5
2025-03-07 19:29:36,242:INFO:             pickle5: Not installed
2025-03-07 19:29:36,242:INFO:         cloudpickle: 3.1.1
2025-03-07 19:29:36,242:INFO:         deprecation: 2.1.0
2025-03-07 19:29:36,242:INFO:              xxhash: 3.5.0
2025-03-07 19:29:36,242:INFO:           wurlitzer: Not installed
2025-03-07 19:29:36,242:INFO:PyCaret optional dependencies:
2025-03-07 19:29:38,326:INFO:                shap: 0.44.1
2025-03-07 19:29:38,326:INFO:           interpret: 0.6.9
2025-03-07 19:29:38,326:INFO:                umap: 0.5.7
2025-03-07 19:29:38,326:INFO:     ydata_profiling: 4.12.2
2025-03-07 19:29:38,326:INFO:  explainerdashboard: 0.4.8
2025-03-07 19:29:38,326:INFO:             autoviz: Not installed
2025-03-07 19:29:38,326:INFO:           fairlearn: 0.7.0
2025-03-07 19:29:38,326:INFO:          deepchecks: Not installed
2025-03-07 19:29:38,326:INFO:             xgboost: 2.1.4
2025-03-07 19:29:38,326:INFO:            catboost: 1.2.7
2025-03-07 19:29:38,326:INFO:              kmodes: 0.12.2
2025-03-07 19:29:38,326:INFO:             mlxtend: 0.23.4
2025-03-07 19:29:38,326:INFO:       statsforecast: 1.5.0
2025-03-07 19:29:38,326:INFO:        tune_sklearn: Not installed
2025-03-07 19:29:38,326:INFO:                 ray: Not installed
2025-03-07 19:29:38,326:INFO:            hyperopt: 0.2.7
2025-03-07 19:29:38,326:INFO:              optuna: 4.2.0
2025-03-07 19:29:38,326:INFO:               skopt: 0.10.2
2025-03-07 19:29:38,326:INFO:              mlflow: 2.20.1
2025-03-07 19:29:38,326:INFO:              gradio: 5.15.0
2025-03-07 19:29:38,326:INFO:             fastapi: 0.115.8
2025-03-07 19:29:38,326:INFO:             uvicorn: 0.34.0
2025-03-07 19:29:38,326:INFO:              m2cgen: 0.10.0
2025-03-07 19:29:38,326:INFO:           evidently: 0.4.40
2025-03-07 19:29:38,326:INFO:               fugue: 0.8.7
2025-03-07 19:29:38,326:INFO:           streamlit: Not installed
2025-03-07 19:29:38,326:INFO:             prophet: Not installed
2025-03-07 19:29:38,326:INFO:None
2025-03-07 19:29:38,326:INFO:Set up data.
